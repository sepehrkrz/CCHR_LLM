{"text": "Historical overview of numerical In general, the public is not aware that our daily weather forecasts start out as initial- value problems on the major national weather services supercomputers. Numerical weather prediction provides the basic guidance for weather forecasting beyond the \ufb01rst few hours. For example, in the USA, computer weather forecasts issued by the National Center for Environmental Prediction (NCEP) in Washington, DC, guide forecasts from the US National Weather Service (NWS). NCEP forecasts are per- formed by running (integrating in time) computer models of the atmosphere that can simulate, given one day\u2019s weather observations, the evolution of the atmosphere in the next few days.1 Because the time integration of an atmospheric model is an initial-value problem, the ability to make a skillful forecast requires both that the computer model be a realistic representation of the atmosphere, and that the initial conditions be known accurately."}
{"text": "NCEP (formerly the National Meteorological Center or NMC) has performed operational computer weather forecasts since the 1950s. From 1955 to 1973, the forecasts included only the Northern Hemisphere; they have been global since 1973."}
{"text": "1 In this book we will provide many examples mostly drawn from the US operational numerical center (NCEP), because of the availability of long records, and because the author\u2019s experience in this center facilitates obtaining such examples. However, these operational NCEP examples are only given for illustration purposes, and are simply representative of the evolution of operational weather forecasting in all major operational centers."}
{"text": "1 Historical overview of numerical weather prediction prediction. The \u201cS1\u201d score (Teweles and Wobus, 1954) measures the relative error in the horizontal gradient of the height of the constant pressure surface of 500 hPa (in the middle of the atmosphere, since the surface pressure is about 1000 hPa) for 36-h forecasts over North America. Empirical experience at NMC indicated that a score of 70% or more corresponds to a useless forecast, and a score of 20% or less corresponds to an essentially perfect forecast. This was found from the fact that 20% was the average S1 score obtained when comparing analyses hand-made by several experienced forecasters \ufb01tting the same observations over the data-rich are close to what was considered essentially \u201cperfect\u201d 40 years ago: the computer forecasts are able to locate generally very well the position and intensity of the large- scale atmospheric waves, major centers of high and low pressure that determine the general evolution of the weather in the 36-h forecast. The sea level pressure forecasts contain smaller-scale atmospheric structures, such as fronts, mesoscale convective systems that dominate summer precipitation, etc., and are still dif\ufb01cult to forecast in detail (although their prediction has also improved very signi\ufb01cantly over the years) so their S1 score is still well above 20% (Fig. 1.1.1(b)). Fig. 1.1.1(a) also shows that the 72-h forecasts of today are as accurate as the 36-h forecasts were 10\u201320 years ago. This doubling (or better) of skill in the forecasts is observed for other forecast variables, such as precipitation. Similarly, 5-day forecasts, which had no useful skill 15 years ago, are now moderately skillful, and during the winter of 1997\u20138, ensemble forecasts for the second week average showed useful skill (de\ufb01ned as anomaly correlation close to 60% or higher)."}
{"text": "the increased power of supercomputers, allowing much \ufb01ner numerical resolution and fewer approximations in the operational atmospheric models; the improved representation of small-scale physical processes (clouds, precipitation, turbulent transfers of heat, moisture, momentum, and radiation) the use of more accurate methods of data assimilation, which result in improved initial conditions for the models; and the increased availability of data, especially satellite and aircraft data over the oceans and the Southern Hemisphere."}
{"text": "In the USA, research on numerical weather prediction takes place in the national laboratories of the National Oceanic and Atmospheric Administration (NOAA), the National Aeronautics and Space Administration (NASA) and the National Center for Atmospheric Research (NCAR), and in universities and centers such as the NCEP operational S1 scores at 36 and 72 hr over North America (500 hPa) NCEP operational models S1 scores:"}
{"text": "Mean Sea Level Pressure over North America (formerly NMC) models over North America (500 hPa). The S1 score measures the relative error in the horizontal pressure gradient, averaged over the region of interest."}
{"text": "Note that the 72-h forecasts are currently as skillful as the 36-h were 10\u201320 years ago (data courtesy C.Vlcek, NCEP). (b) Same as (a) but showing S1 scores for sea level pressure forecasts over North America (data courtesy C.Vlcek, NCEP). It shows results from global (AVN) and regional (LFM, NGM and Eta) forecasts. The LFM model development was \u201cfrozen\u201d in 1986 and the NGM was frozen in 1991."}
{"text": "1 Historical overview of numerical weather prediction Center for Prediction of Storms (CAPS). Internationally, major research takes place in large operational national and international centers (such as the European Center for Medium Range Weather Forecasts (ECMWF), NCEP, and the weather services of the UK, France, Germany, Scandinavian and other European countries, Canada, Japan, Australia, and others). In meteorology there has been a long tradition of sharing both data and research improvements, with the result that progress in the science of forecasting has taken place on many fronts, and all countries have bene\ufb01ted from In this introductory chapter, we give an overview of the major components and milestones in numerical forecasting. They will be discussed in detail in the following Jule G. Charney (1917\u20131981) was one of the giants in the history of numerical weather prediction. In his 1951 paper \u201cDynamical forecasting by numerical process\u201d, he introduced the subject of this book as well as it could be introduced today. We reproduce here parts of the paper (with emphasis added):"}
{"text": "As meteorologists have long known, the atmosphere exhibits no periodicities of the kind that enable one to predict the weather in the same way one predicts the tides. No simple set of causal relationships can be found which relate the state of the atmosphere at one instant of time to its state at another. It was this realization that led V. Bjerknes (1904) to de\ufb01ne the problem of prognosis as nothing less than the integration of the equations of motion of the atmosphere.2 But it remained for Richardson (1922) to suggest the practical means for the solution of this problem."}
{"text": "He proposed to integrate the equations of motion numerically and showed exactly how this might be done. That the actual forecast used to test his method was unsuccessful was in no way a measure of the value of his work. In retrospect it 2 The importance of the Bjerknes (1904) paper is clearly described by Thompson (1990), another pioneer of NWP, and the author of a very inspiring text on NWP (Thompson, 1961a). His paper \u201cCharney and the revival of NWP\u201d contains extremely interesting material on the history of NWP as well as on early computers:"}
{"text": "It was not until 1904 that Vilhelm Bjerknes \u2013 in a remarkable manifesto and testament of deterministic faith \u2013 stated the central problem of NWP. This was the \ufb01rst explicit, coherent recognition that the future state of the atmosphere is, in principle, completely determined by its detailed initial state and known boundary conditions, together with Newton\u2019s equations of motion, the Boyle\u2013Charles\u2013Dalton equation of state, the equation of mass continuity, and the thermodynamic energy equation. Bjerknes went further: he outlined an ambitious, but logical program of observation, graphical analysis of meteorological data and graphical solution of the governing equations. He succeeded in persuading the Norwegians to support an expanded network of surface observation stations, founded the famous Bergen School of synoptic and dynamic meteorology, and ushered in the famous polar front theory of cyclone formation."}
{"text": "Beyond providing a clear goal and a sound physical approach to dynamical weather prediction, V. Bjerknes instilled his ideas in the minds of his students and their students in Bergen and in Oslo, three of whom were later to write important chapters in the development of NWP in the US (Rossby, Eliassen and Fj\u00a8ortoft)."}
{"text": "becomes obvious that the inadequacies of observation alone would have doomed any attempt, however well conceived, a circumstance of which Richardson was aware. The real value of his work lay in the fact that it crystallized once and for all the essential problems that would have to be faced by future workers in the \ufb01eld and it laid down a thorough groundwork for their solution."}
{"text": "For a long time no one ventured to follow in Richardson\u2019s footsteps. The paucity of the observational network and the enormity of the computational task stood as apparently insurmountable barriers to the realization of his dream that one day it might be possible to advance the computation faster than the weather."}
{"text": "But with the increase in the density and extent of the surface and upper-air observational network on the one hand, and the development of large-capacity high-speed computing machines on the other, interest has revived in Richardson\u2019s problem, and attempts have been made to attack it anew."}
{"text": "These efforts have been characterized by a devotion to objectives more limited than Richardson\u2019s. Instead of attempting to deal with the atmosphere in all its complexity, one tries to be satis\ufb01ed with simpli\ufb01ed models approximating the actual motions to a greater or lesser degree. By starting with models incorporating only what it is thought to be the most important of the atmospheric in\ufb02uences, and by gradually bringing in others, one is able to proceed inductively and thereby to avoid the pitfalls inevitably encountered when a great many poorly understood factors are introduced all at once."}
{"text": "A necessary condition for the success of this stepwise method is, of course, that the \ufb01rst approximations bear a recognizable resemblance to the actual motions. Fortunately, the science of meteorology has progressed to the point where one feels that at least the main factors governing the large-scale atmospheric motions are well known. Thus integrations of even the linearized barotropic and thermally inactive baroclinic equations have yielded solutions bearing a marked resemblance to reality. At any rate, it seems clear that the models embodying the collective experience and the positive skill of the forecast cannot fail utterly. This conviction has served as the guiding principle in the work of the meteorology project at The Institute for Advanced Study [at Princeton University] with which the writer has been connected."}
{"text": "As indicated by Charney, Richardson performed a remarkably comprehensive numerical integration of the full primitive equations of motion (Chapter 2). He used a horizontal grid of about 200 km, and four vertical layers of approximately 200 hPa, centered over Germany. Using the observations at 7 UTC (Universal Coordinate Time) on 20 May 1910, he computed the time derivative of the pressure in central Germany between 4 and 10 UTC. The predicted 6-h change was 146 hPa, whereas in reality there was essentially no change observed in the surface pressure. This huge error was discouraging, but it was due mostly to the fact that the initial conditions were not balanced, and therefore included fast-moving gravity waves which masked the initial rate of change of the meteorological signal in the forecast (Fig. 1.2.1)."}
{"text": "Moreover, if the integration had been continued, it would have suffered \u201ccomputa- tionalblow-up\u201dduetotheviolationoftheCourant\u2013Friedricks\u2013Lewy(CFL)condition 1 Historical overview of numerical weather prediction and superimposed high-frequency gravity waves. Note that even though the forecast of the slow waves is essentially unaffected by the presence of gravity waves, the initial time derivative is much larger in magnitude, as obtained in the Richardson (Chapter 3) which requires that the time step should be smaller than the grid size divided by the speed of the fastest traveling signal (in this case horizontally moving sound waves, traveling at about 300 m/s)."}
{"text": "Charney (1948, 1949) and Eliassen (1949) solved both of these problems by deriving \u201c\ufb01ltered\u201d equations of motion, based on quasi-geostrophic (slowly varying) balance, which \ufb01ltered out (i.e., did not include) gravity and sound waves, and were based on pressure \ufb01elds alone. Charney points out that this approach was justi\ufb01ed by the fact that forecasters\u2019 experience was that they were able to predict tomorrow\u2019s weather from pressure charts alone:"}
{"text": "In the selection of a suitable \ufb01rst approximation, Richardson\u2019s discovery that the horizontal divergence was an unmeasurable quantity had to be taken into account. Here a consideration of forecasting practice gave rise to the belief that this dif\ufb01culty could be surmounted: forecasts were made by means of geostrophic reasoning from the pressure \ufb01eld alone \u2013 forecasts in which the concept of horizontal divergence played no role."}
{"text": "On another3 occasion when our conversations had turned closer to scienti\ufb01c matters, Jule was talking again about the early days of NWP. For a proper 3 The previous occasion was a story about an invitation Charney received to appear on the \u201cToday\u201d show, to talk about how computers were going to forecast the weather. Since the show was at 7 am, Charney, a late riser, had never watched it. \u201cHe told us that he felt that he ought to see the show at least once before agreeing to appear on it, and so, one morning, he managed to pull himself out of bed and turn on the TV set, and the \ufb01rst person he saw was a chimpanzee."}
{"text": "perspective, we should recall that at the time when Charney was a student, pressure was king. The centers of weather activity were acknowledged to be the highs and lows. A good prognostic chart was one that had the isobars in the right locations."}
{"text": "Naturally, then, the thing that was responsible for the weather changes was the thing that made the pressure change. This was readily shown to be the divergence of the wind \ufb01eld. The divergence could not be very accurately measured, and a corollary deduced by some meteorologists, including some of Charney\u2019s advisors, was that the dynamic equations could not be used to forecast the weather."}
{"text": "Such reasoning simply did not make sense to Jule. The idea that the wind \ufb01eld might serve instead of the pressure \ufb01eld as a basis for dynamical forecasting, proposed by Rossby, gave Jule a route to follow.4 He told us, however, that what really inspired him to develop the equations that later became the basis for NWP was a determination to prove, to those who had assured him that the task was impossible, that they were wrong."}
{"text": "Charney, R. Fj\u00f8rtoft, and J. von Neuman (1950) computed a historic \ufb01rst one-day weather forecast using a barotropic (one-layer) \ufb01ltered model. The work took place in 1948\u20139. They used one of the \ufb01rst electronic computers (the Electronic Numerical Integrator and Computer, ENIAC), housed at the Aberdeen Proving Grounds of the US Army in Maryland. It incorporated von Neuman\u2019s idea of \u201cstored programming\u201d (i.e., the ability to perform arithmetic operations over different operands (loops) without having to repeat the code). The results of the \ufb01rst forecasts were quite encouraging: Fig. 1.2.2, reproduced from Charney (1951) shows the 24-h forecast and veri\ufb01cation for 30 January 1949. Unlike Richardson\u2019s results, the forecast remains meteorological, and there is a pattern correlation between the predicted and the observed pressure \ufb01eld 24-h changes."}
{"text": "It is remarkable that in his 1951 paper, just after the triumph of performing the \ufb01rst successful forecasts with \ufb01ltered models, Charney already saw that much more progress would come from the use of the primitive (un\ufb01ltered) equations of motion as Richardson had originally attempted:"}
{"text": "The discussion so far has dealt exclusively with the quasi-geostrophic equations as the basis for numerical forecasting. Yet there has been no intention to exclude the possibility that the primitive Eulerian equations can also be used for this purpose. The outlook for numerical forecasting would be indeed dismal if the quasi-geostrophic approximation represented the upper limit of attainable accuracy, for it is known that it applies only indifferently, if at all, to many of the small-scale but meteorologically signi\ufb01cant motions. We have merely indicated two obstacles that stand in the way of the applications of the primitive equations:"}
{"text": "He decided he could never compete with a chimpanzee for the public\u2019s favor, and so he gracefully declined to appear, much to the dismay of the computer company that had engineered the invitation in the \ufb01rst place\u201d (Lorenz, 1990)."}
{"text": "4 The development of the \u201cRossby waves\u201d phase speed equation c = U \u2212\u03b2L2/\u03c0 2 based on the linearized, non-divergent vorticity equation (Rossby et al., 1939, Rossby, 1940), and its success in predicting the motion of the large-scale atmospheric waves, was an essential stimulus to Charney\u2019s development of the \ufb01ltered equations (Phillips, 1990b, 1998)."}
{"text": "1 Historical overview of numerical weather prediction and \u03b6 + f at t = 0; (b) observed z and \u03b6 + f at t = 24 h; (c) observed (continuous lines) and computed (broken lines) 24-h height change; (d) computed z and \u03b6 + f at t = 24 h. The height unit is 100 ft and the unit of vorticity is 1/3 \u00d7 10\u22124 s\u20131."}
{"text": "(Reproduced from the Compendium of Meteorology, with permission of the American Meteorological Society.) First, there is the dif\ufb01culty raised by Richardson that the horizontal divergence cannot be measured with suf\ufb01cient accuracy. Moreover, the horizontal divergence is only one of a class of meteorological unobservables which also includes the horizontal acceleration. And second, if the primitive Eulerian equations are employed, a stringent and seemingly arti\ufb01cial bound is imposed on the size of the time interval for the \ufb01nite difference equations. The \ufb01rst obstacle is the most formidable, for the second only means that the integration must proceed in steps of the order of \ufb01fteen minutes rather than two hours. Yet the \ufb01rst does not seem insurmountable, as the following considerations will indicate."}
{"text": "He proceeded to describe an unpublished study in which he and J.C. Freeman integrated barotropic primitive equations (i.e., shallow water equations, Chapter 2) which include not only the slowly varying quasi-geostrophic solution, but also fast gravity waves. They initialized the forecast assuming zero initial divergence, and compared the result with a barotropic forecast (with gravity waves \ufb01ltered out)."}
{"text": "The results were similar to those shown schematically in Fig. 1.2.1: they observed that over a day or so the gravity waves subsided (through a process that we call geostrophic adjustment) and did not otherwise affect the forecast of the slow waves."}
{"text": "From this result Charney concluded that numerical forecasting could indeed use the full primitive equations (as eventually happened in operational practice). He listed in the paper the complete primitive equations in pressure coordinates, essentially as they are used in current operational weather prediction, but without heating (nonadiabatic) and frictional terms, which he expected to have minor effects in one- or two-day forecasts. Charney concluded this remarkable paper with the following discussion, which includes a list of the physical processes that take place at scales too small to be resolved, and are incorporated in present models through \u201cparameterizations of the subgrid-scale physics\u201d (condensation, radiation, and turbulent \ufb02uxes of heat, momentum and moisture, Chapter 4):"}
{"text": "Nonadiabatic and frictional terms have been ignored in the body of the discussion because it was thought that one should \ufb01rst seek to determine how much of the motion could be explained without them. Ultimately they will have to be taken into account, particularly if the forecast period is to be extended to Condensational phenomena appear to be the simplest to introduce: one has only to add the equation of continuity for water vapor and to replace the dry by the moist adiabatic equation. Long-wave radiational effects can also be provided for, since our knowledge of the absorptive properties of water vapor and carbon dioxide has progressed to a point where quantitative estimates of radiational cooling can be made, although the presence of clouds will complicate the The most dif\ufb01cult phenomena to include have to do with the turbulent transfer of momentum and heat. A great deal of research remains to be done before enough is known about these effects to permit the assignment of even rough values to the eddy coef\ufb01cients of viscosity and heat conduction. Owing to their statistically indeterminate nature, the turbulent properties of the atmosphere place an upper limit to the accuracy obtainable by dynamical methods of forecasting, beyond which we shall have to rely upon statistical methods. But it seems certain that much progress can be made before these limits can be reached."}
{"text": "This paper, which although written in 1951 has not become dated, predicted with almost supernatural vision the path that numerical weather forecasting was to follow over the next \ufb01ve decades. It described the need for objective analysis of meteorological data in order to replace the laborious hand analyses. We now refer to this process as data assimilation (Chapter 5), which uses both observations and short forecasts to estimate initial conditions. Note that at a time at which only one-day forecasts had ever been attempted, Charney already had the intuition that there was an upper limit to weather predictability, which Lorenz (1965) later estimated to be about two weeks. However, Charney attributed the expected limit to model de\ufb01ciencies (such as the parameterization of turbulent processes), rather than to the chaotic nature of the atmosphere, which imposes a limit of predictability even if the model is perfect 1 Historical overview of numerical weather prediction (Lorenz, 1963b; Chapter 6). Charney was right in assuming that in practice model de\ufb01ciencies, as well as errors in the initial conditions, would limit predictability. At the present time, however, the state of the art in numerical forecasting has advanced enough that, when the atmosphere is highly predictable, the theoretically estimated limit for weather forecasting (about two weeks) is occasionally reached and even exceeded through techniques such as ensemble forecasting (Chapter 6)."}
{"text": "Following the success of Charney et al. (1950), Rossby moved back to Sweden, and was able to direct a group that reproduced similar experiments on a powerful Swedish computer known as BESK. As a result, the \ufb01rst operational (real time) numerical weather forecasts started in Sweden in September 1954, six months before the start-up of the US operational forecasts5 (D\u00a8o\u00a8os and Eaton, 1957, Wiin-Nielsen, Primitive equations, global and regional models, and nonhydrostatic models As envisioned by Charney (1951, 1962) the \ufb01ltered (quasi-geostrophic) equations, although very useful for understanding of the large-scale extratropical dynamics of the atmosphere, were not accurate enough to allow continued progress in NWP, and were eventually replaced by primitive equation models (Chapter 2). The primitive equations are conservation laws applied to individual parcels of air: conservation of the three-dimensional momentum (equations of motion), conservation of energy (\ufb01rst law of thermodynamics), conservation of dry air mass (continuity equation), and equations for the conservation of moisture in all its phases, as well as the equation of state for perfect gases. They include in their solution fast gravity and sound waves, and therefore in their space and time discretization they require the use of smaller time steps, or alternative techniques that slow them down (Chapter 3). For models with a horizontal grid size larger than 10 km, it is customary to replace the vertical component of the equation of motion with its hydrostatic approximation, in which the vertical acceleration is considered negligible compared with the gravitational acceleration (buoyancy). With this approximation, it is convenient to use atmospheric pressure, instead of height, as a vertical coordinate."}
{"text": "The continuous equations of motions are solved by discretization in space and in time using, for example, \ufb01nite differences (Chapter 3). It has been found that the ac- curacy of a model is very strongly in\ufb02uenced by the spatial resolution: in general, the higher the resolution, the more accurate the model. Increasing resolution, however, is extremely costly. For example, doubling the resolution in the three space dimensions also requires halving the time step in order to satisfy conditions for computational 5 Anders Persson (1999 personal communication) kindly provided the notes on the historical development of NWP in the USA and Sweden reproduced in Appendix A."}
{"text": "stability. Therefore, the computational cost of doubling the resolution is a factor of 24 (three space and one time dimensions). Modern methods of discretization attempt to make the increase in accuracy less onerous by the use of semi-implicit and semi- Lagrangian time schemes. These schemes (pioneered by Canadian scientists under the leadership of Andre Robert) have less stringent stability conditions on the time step, and more accurate space discretization. Nevertheless, there is a constant need for higher resolution in order to improve forecasts, and as a result running atmospheric models has always been a major application of the fastest supercomputers available."}
{"text": "When the \u201cconservation\u201d equations are discretized over a given grid size (typi- cally from a few to several hundred kilometers) it is necessary to add \u201csources and sinks\u201d terms due to small-scale physical processes that occur at scales that cannot be explicitly resolved by the models. As an example, the equation for water vapor conservation on pressure coordinates is typically written as where q is the ratio between water vapor and dry air mass, x and y are horizontal coordinates with appropriate map projections, p is pressure, t is time, u and v are the horizontal air velocity (wind) components, \u03c9 = dp/dt is the vertical velocity in pressure coordinates, and the product of primed variables represents turbulent transports of moisture on scales unresolved by the grid used in the discretization, with the overbar indicating a spatial average over the grid of the model. It is customary to call the left-hand side of the equation, the \u201cdynamics\u201d of the model, which is computed explicitly (Chapter 3)."}
{"text": "The right-hand side represents the so-called \u201cphysics\u201d of the model. For the mois- ture equation, it includes the effects of physical processes such as evaporation and condensation E \u2212C, and turbulent transfers of moisture which take place at small scales that cannot be explicitly resolved by the \u201cdynamics\u201d. These subgrid-scale physical processes, which are sources and sinks for the equations, are then \u201cparame- terized\u201d in terms of the variables explicitly represented in the atmospheric dynamics Two types of models are in use for NWP: global and regional models (Chapter 5)."}
{"text": "Global models are generally used for guidance in medium-range forecasts (more than 2 d), and for climate simulations. At NCEP, for example, the global models are run through 16 d every day. Because the horizontal domain of global models is the whole earth, they usually cannot be run at high resolution. For more detailed forecasts it is necessary to increase the resolution, and this can only be done over limited regions Regional models are used for shorter-range forecasts (typically 1\u20133 d), and are run with a resolution two or more times higher than global models. For example, the NCEP global model in 1997 was run with 28 vertical levels, and a horizontal resolution of 100 km for the \ufb01rst week, and 200 km for the second week. The regional 1 Historical overview of numerical weather prediction (Eta) model was run with a horizontal resolution of 48 km and 38 levels, and later in the day with 29 km and 50 levels. Because of their higher resolution, regional models have the advantage of higher accuracy and the ability to reproduce smaller-scale phenomena such as fronts, squall lines, and much better orographic forcing than global models. On the other hand, regional models have the disadvantage that, unlike global models, they are not \u201cself-contained\u201d because they require lateral boundary conditions at the borders of the horizontal domain. These boundary conditions must be as accurate as possible, because otherwise the interior solution of the regional models quickly deteriorates. Therefore it is customary to \u201cnest\u201d the regional models within another model with coarser resolution, whose forecast provides the boundary conditions. For this reason, regional models are used only for short-range forecasts."}
{"text": "After a certain period, which is proportional to the size of the model, the information contained in the high-resolution initial conditions is \u201cswept away\u201d by the in\ufb02uence of the boundary conditions, and the regional model becomes merely a \u201cmagnifying glass\u201d for the coarser model forecast in the regional domain. This can still be useful, for example, in climate simulations performed for long periods (seasons to multiyears), and which therefore tend to be run at coarser resolution. A \u201cregional climate model\u201d can provide a more detailed version of the coarse climate simulation in a region of interest. Several other major NWP centers in Europe (United Kingdom (http://www.met-of\ufb01ce.gov.uk/), France (http://www.meteo.fr/), Germany (http://www.dwd.de/)), Japan (http://www.kishou.go.jp/), Australia (http://www."}
{"text": "bom.gov.au/nmoc/ab nmc op.shtml), and Canada (http://www.ec.gc.ca/) also have similar global and regional models, whose details can be obtained at their web More recently the resolution of some regional models has been increased to just a few kilometers in order to resolve better storm-scale phenomena. Storm-resolving models such as the Advanced Regional Prediction System (ARPS) cannot use the hydrostatic approximation which ceases to be accurate for horizontal scales of the order of 10 km or smaller. Several major nonhydrostatic models have been devel- oped and are routinely used for mesoscale forecasting. In the USA the most widely used are the ARPS, the MM5 (Penn State/NCAR Mesoscale Model, Version 5), the RSM (NCEP Regional Spectral Model) and the COAMPS (US Navy\u2019s Coupled Ocean/Atmosphere Mesoscale Prediction System). There is a tendency towards the use of nonhydrostatic models that can be used globally as well."}
{"text": "Data assimilation: determination of the initial conditions for the computer forecasts As indicated previously, NWP is an initial-value problem: given an estimate of the present state of the atmosphere, the model simulates (forecasts) its evolution."}
{"text": "The problem of determination of the initial conditions for a forecast model is very 1.4 Data assimilation: determination of the initial conditions important and complex, and has become a science in itself (Daley, 1991). In this section we introduce methods that have been used for this purpose (successive cor- rections method or SCM, optimal interpolation or OI, variational methods in three and four dimensions, 3D-Var and 4D-Var, and Kalman \ufb01ltering or KF). We discuss this subject in more detail in Chapter 5, and refer the reader to Daley (1991) as a much more comprehensive text on atmospheric data analysis."}
{"text": "In the early experiments, Richardson (1922) and Charney et al. (1950) performed hand interpolations of the available observations to grid points, and these \ufb01elds of initial conditions were manually digitized, which was a very time consuming procedure. The need for an automatic \u201cobjective analysis\u201d quickly became apparent (Charney, 1951), and interpolation methods \ufb01tting data to grids were developed (e.g., Panofsky, 1949, Gilchrist and Cressman, 1954, Barnes, 1964, 1978). However, there is an even more important problem than spatial interpolation of observations to gridded \ufb01elds: the data available are not enough to initialize current models. Modern primitive equations models have a number of degrees of freedom of the order of 107. For example, a latitude\u2013longitude model with a typical resolution of 1\u25e6and 20 vertical levels would have 360 \u00d7 180 \u00d7 20 = 1.3 \u00d7 106 grid points. At each grid point we have to carry the values of at least four prognostic variables (two horizontal wind components, temperature, moisture), and the surface pressure for each column, giving over 5 million variables that need to be given an initial value. For any given time window of \u00b13 hours, there are typically 10\u2013100 thousand observations of the atmosphere, two orders of magnitude less than the number of degrees of freedom of the model. Moreover, their distribution in space and time is very nonuniform (Fig. 1.4.1), with regions like North America and Eurasia which are relatively data- rich, while others much more poorly observed."}
{"text": "For this reason, it became obvious rather early that it was necessary to use addi- tional information (denoted background, \ufb01rst guess or prior information) to prepare initial conditions for the forecasts (Bergthorsson and D\u00a8o\u00a8os, 1955). Initially clima- tology was used as a \ufb01rst guess (e.g., Gandin, 1963), but as the forecasts became better, a short-range forecast was chosen as the \ufb01rst guess in the operational data assimilation systems or \u201canalysis cycles\u201d. The intermittent data assimilation cycle shown schematically in Fig. 1.4.2 is continued in present-day operational systems, which typically use a 6-h cycle performed four times a day."}
{"text": "In the 6-h data assimilation cycle for a global model, the background \ufb01eld is a model 6-h forecast xb (a three-dimensional array). To obtain the background or \ufb01rst guess \u201cobservations\u201d, the model forecast is interpolated to the observation location, and if they are different, converted from model variables to observed variables yo (such as satellite radiances or radar re\ufb02ectivities). The \ufb01rst guess of the observations is therefore H(xb), where H is the observation operator that performs the neces- sary interpolation and transformation from model variables to observation space."}
{"text": "The difference between the observations and the model \ufb01rst guess yo \u2212H(xb) is denoted \u201cobservational increments\u201d or \u201cinnovations\u201d. The analysis xa is obtained by 1 Historical overview of numerical weather prediction DATA DISTRIBUTION 01 SEP9700Z\u201301SEP9700Z Global analysis (statistical interpolation) and balancing Background or first guess 1.4 Data assimilation: determination of the initial conditions adding the innovations to the model forecast (\ufb01rst guess) with weights W that are determined based on the estimated statistical error covariances of the forecast and Different analysis schemes (SCM, OI, 3D-Var, and KF) are based on (1.4.1) but differ by the approach taken to combine the background and the observations to pro- duce the analysis. Earlier methods such as the SCM (Bergthorsson and D\u00a8o\u00a8os, 1955, Cressman, 1959, Barnes, 1964) were of a form similar to (1.4.1), with weights deter- mined empirically. The weights are a function of the distance between the observation and the grid point, and the analysis is iterated several times. In OI (Gandin, 1963) the matrix of weights W is determined from the minimization of the analysis errors at each grid point. In the 3D-Var approach one de\ufb01nes a cost function proportional to the square of the distance between the analysis and both the background and the observations (Sasaki, 1970). The cost function is minimized directly to obtain the analysis. Lorenc (1986) showed that OI and the 3D-Var approach are equivalent if the cost function is de\ufb01ned as:"}
{"text": "2{[yo \u2212H(x)]T R\u22121[yo \u2212H(x)] + (x \u2212xb)T B\u22121(x \u2212xb)} Thecostfunction J in(1.4.2)measuresthedistanceofa\ufb01eld x totheobservations(the \ufb01rst term in the cost function) and the distance to the \ufb01rst guess or background xb (the second term in the cost function). The distances are scaled by the observation error covariance R and by the background error covariance B respectively. The minimum of the cost function is obtained for x = xa, which is de\ufb01ned as the \u201canalysis\u201d. The analysis obtained in (1.4.1) and (1.4.2) is the same if the weight matrix in (1.4.1) is The difference between OI (1.4.1) and the 3D-Var approach (1.3) is in the method of solution: in OI, the weights W are obtained for each grid point or grid volume, using suitable simpli\ufb01cations. In 3D-Var, the minimization of (1.4.2) is performed directly, allowing for additional \ufb02exibility and a simultaneous global use of the data More recently, the variational approach has been extended to four dimensions, by including within the cost function the distance to observations over a time interval (assimilation window). A \ufb01rst version of this considerably more expensive method was implemented at ECMWF at the end of 1997 (Bouttier and Rabier, 1997). Re- search on the even more advanced and computationally expensive KF (e.g., Ghil et al., 1981), and ensemble KF (Evensen, 1994, Houtekamer and Mitchell, 1998) is dis- cussed in Chapter 5. That chapter also includes a discussion about the problem of enforcing a balance in the analysis so that the presence of gravity waves does not 1 Historical overview of numerical weather prediction mask the meteorological signal, as happened to Richardson (1922) (Fig. 1.2.1). The method used for many years to solve this \u201cinitialization\u201d problem was \u201cnonlinear normal mode initialization\u201d (Machenhauer, 1977, Baer and Tribbia, 1977). The bal- ance in the initial conditions is usually obtained by either adding a constraint to the cost function (1.4.2) (Parrish and Derber, 1992), or through the use of a digital \ufb01lter (Lynch and Huang, 1992, Chapter 5)."}
{"text": "In the analysis cycle, no matter which analysis scheme is employed, the use of the model forecast is essential in achieving \u201cfour-dimensional data assimilation\u201d (4DDA). This means that the data assimilation cycle is like a long model integration, in which the model is \u201cnudged\u201d by the observational increments in such a way that it remains close to the real atmosphere. The importance of the model cannot be overemphasized: it transports information from data-rich to data-poor regions, and it provides a complete estimation of the four-dimensional state of the atmosphere."}
{"text": "guess) and the rawinsonde observations from 1978 to the present (in other words, the rms of the observational increments for 500-hPa heights). It should be noted that the rms differences are not necessarily forecast errors, since the observations also contain errors. In the Northern Hemisphere the rms differences have been halved from about 30 m in the late 1970s, to about 13 m in 2000, equivalent to a mean temperature error of about 0.65 K, similar to rawinsonde observational errors. In the Southern Hemisphere the improvements are even larger, with the differences decreasing from about 47 m to about 12 m. The improvements in these short-range forecasts are a rawinsonde observations) for 500-hPa heights (data courtesy of Steve Lilly, NCEP)."}
{"text": "1.5 Operational NWP and the evolution of forecast skill re\ufb02ection of improvements in the model, the analysis scheme used to assimilate the data, and the quality and quality control of the data (Chapter 5)."}
{"text": "Operational NWP and the evolution of forecast skill Major milestones of operational numerical weather forecasting include the paper by Charney et al. (1950) with the \ufb01rst successful forecast based on the primitive equations, and the \ufb01rst operational forecasts performed in Sweden in September 1954, followed 6 months later by the \ufb01rst operational (real time) forecasts in the USA. We describe in what follows the evolution of NWP at NCEP, but as mentioned before, similar developments took place at several major operational NWP centers:"}
{"text": "in the UK, France, Germany, Japan, Australia and Canada. The history of operational NWP at the NMC (now NCEP) has been reviewed by Shuman (1989) and Kalnay et al. (1998). It started with the organization of the Joint Numerical Weather Prediction Unit (JNWPU) on 1 July 1954, staffed by members of the US Weather Bureau (later the National Weather Service, NWS), the Air Weather Service of the US Air Force, and the Naval Weather Service.6 Shuman pointed out that in the \ufb01rst few years, numerical predictions could not compete with those produced manually. They had several serious \ufb02aws, among them overprediction of cyclone development. Far too many cyclones were predicted to deepen into storms. With time, and with the joint work of modelers and practising synopticians, major sources of model errors were identi\ufb01ed, and operational NWP became the central guidance for operational weather forecasts."}
{"text": "Shuman (1989) included a chart with the evolution of the S1 score (Teweles and Wobus, 1954), the \ufb01rst measure of error in a forecast weather chart which, according to Shuman (1989), was designed, tested, and modi\ufb01ed to correlate well with expert forecasters\u2019 opinions on the quality of a forecast. The S1 score measures the average relative error in the pressure gradient (compared to a verifying analysis chart). Experiments comparing two independent subjective analyses of the same data-rich North American region made by two experienced analysts suggested that a \u201cperfect\u201d forecast would have an S1 score of about 20%. It was also found empirically that forecasts with an S1 score of 70% or more were useless as synoptic guidance."}
{"text": "Shuman pointed out some of the major system improvements that enabled NWP forecasts to overtake and surpass subjective forecasts. The \ufb01rst major improvement took place in 1958 with the implementation of a barotropic (one-level) model, which was actually a reduction from the three-level model \ufb01rst tried, but which included bet- ter \ufb01nite differences and initial conditions derived from an objective analysis scheme (Bergthorsson and D\u00a8o\u00a8os, 1955, Cressman, 1959). It also extended the domain of the 6 In 1960 the JNWPU reverted to three separate organizations: the National Meteorological Center (National Weather Service), the Global Weather Central (US Air Force) and the Fleet Numerical Oceanography Center (US Navy)."}
{"text": "These changes resulted in numerical forecasts that for the \ufb01rst time were competitive with subjective forecasts, but in order to implement them JNWPU had to wait for the acquisition of a more powerful supercomputer, an IBM 704, to replace the previous IBM 701. This pattern of forecast improvements which depend on a combination of the better use of the data and better models, and would require more powerful super- computers in order to be executed in a timely manner has been repeated throughout the history of operational NWP. Table 1.5.1 (adapted from Shuman (1989)) summa- rizes the major improvements in the \ufb01rst 30 years of operational numerical forecasts at the NWS. The \ufb01rst primitive equations model (Shuman and Hovermale, 1968) was implemented in 1966. The \ufb01rst regional system (Limited Fine Mesh or LFM model, Howcroft, 1971) was implemented in 1971. It was remarkable because it remained in use for over 20 years, and it was the basis for Model Output Statistics (MOS)."}
{"text": "Its development was frozen in 1986. A more advanced model and data assimilation system, the Regional Analysis and Forecasting System (RAFS) was implemented as the main guidance for North America in 1982. The RAFS was based on the multiple Nested Grid Model (NGM, Phillips, 1979) and on a regional OI scheme (DiMego, 1988). The global spectral model (Sela, 1980) was implemented in 1980."}
{"text": "2000) summarizes the major improvements implemented in the global system starting NMC between 1955 and 1985 (adapted from Shuman, 1989) Princeton three-level quasi-geostrophic model (Charney, 1954). Not used by the forecasters Barotropic model with improved numerics, objective analysis initial conditions, and octagonal domain."}
{"text": "Three-level quasi-geostrophic model with improved Six-layer primitive equations model (Shuman and LFM model (Howcroft, 1971) (\ufb01rst regional model at Hough functions analysis (Flattery, 1971) Seven-layer primitive equation model (hemispheric) Global spectral model, R30/12 layers (Sela, 1980) Regional Analysis and Forecast System based on the NGM (Phillips, 1979) and OI (DiMego, 1988) 1.5 Operational NWP and the evolution of forecast skill system since 1985 (adapted from Kalnay et al. 1998 and P. Caplan, pers. comm., GFDL physics implemented on the global spectral model with silhouette orography, R40/18 layers New OI code with new statistics Increased resolution to T80/18 layers, Penman\u2013Montieth evapotranspiration and other improved physics (Caplan and White, 1989, Pan, Implementation of hydrostatic complex quality control (CQC) (Gandin, 1988) Increased resolution to T126 L18 and improved physics, mean orography. (Kanamitsu et al., New 3D-Var (Parrish and Derber, 1992, Derber Addition of increments, horizontal and vertical OI checks to the CQC (Collins and Gandin, 1990) First ensemble system: one pair of bred forecasts at 00Z to 10 days, extension of AVN to 10 days (Toth and Kalnay, 1993, Tracton and Kalnay, Simpli\ufb01ed Arakawa\u2013Schubert cumulus convection (Pan and Wu, 1995). Resolution T126/28 layers Second ensemble system: \ufb01ve pairs of bred forecasts at 00Z, two pairs at 12Z, extension of AVN, a total of 17 global forecasts every day to New soil hydrology (Pan and Mahrt, 1987), radiation, clouds, improved data assimilation."}
{"text": "Direct assimilation of TOVS cloud-cleared radiances (Derber and Wu, 1998). New planetary boundary layer (PBL) based on nonlocal diffusion (Hong and Pan, 1996). Improved CQC 1 Historical overview of numerical weather prediction New observational error statistics. Changes to assimilation of TOVS radiances and addition of Assimilation of noncloud-cleared radiances (Derber et al., pers.comm.). Improved physics."}
{"text": "Resolution increased to T170/40 layers (to 3.5 days). Improved physics. 3D ozone data assimilation and forecast. Nonlinear increments in 3D-Var. Resolution reduced to T62/28levels on Oct. 1998 and upgraded back in Jan. 2000 Ensemble resolution increased to T126 for the Tropical cyclones relocated to observed position in 1985 with the implementation of the \ufb01rst comprehensive package of physical pa- rameterizations from GFDL (Geophysical Fluid Dynamics Laboratory). Other major improvements in the physical parameterizations were made in 1991, 1993, and 1995."}
{"text": "The most important changes in the data assimilation were an improved OI formu- lation in 1986, the \ufb01rst operational 3D-Var in 1991, the replacement of the satellite retrievals of temperature with the direct assimilation of cloud-cleared radiances in 1995, and the use of \u201craw\u201d (not cloud-cleared) radiances in 1998. The model resolu- tion was increased in 1987, 1991, and 1998. The \ufb01rst operational ensemble system was implemented in 1992 and enlarged in 1994. The resolution of the ensembles was forecasts (up to 48 h). The RAFS (triple nested NGM and OI) were implemented in 1985. The Eta model, designed with advanced \ufb01nite differences, step-mountain coordinates, and physical parameterizations, was implemented in 1993, with the same 80-km horizontal resolution as the NGM. It was denoted \u201cearly\u201d because of a short data cut-off. The resolution was increased to 48 km, and a \ufb01rst \u201cmesoscale\u201d version with 29 km and reduced coverage was implemented in 1995. A cloud prognostic scheme was implemented in 1995, and a new land-surface parameterization in 1996."}
{"text": "The OI data assimilation was replaced by a 3D-Var in 1998, and at this time the early and meso-Eta models were uni\ufb01ed into a 32-km/45-level version. Many other less signi\ufb01cant changes were also introduced into the global and regional operational systems and are not listed here for the sake of brevity. The Rapid Update Cycle (RUC), which provides frequent updates of the analysis and very-short-range forecasts over 1.5 Operational NWP and the evolution of forecast skill assimilation since 1985 (from compilations by Fedor Mesinger and Geoffrey DiMego, pers. comm., 1998) RAFS based on triply NGM (Phillips, 1979) and OI (DiMego, 1988). Resolution: 80 km/16 RAFS upgraded for the last time: NGM run with only two grids with inner grid domain doubled in size. Implemented Regional Data Assimilation System (RDAS) with three-hourly updates using an improved OI analysis using all off-time data including Pro\ufb01ler and Aircraft Communication Addressing and Reporting System (ACARS) wind reports (DiMego et al., 1992) and CQC procedures (Gandin et al., First operational implementation of the Eta model in the 00Z & 12Z early run for North America at 80-km and 38-layer resolution (Mesinger et al., 1988, Janjic, 1994, Black et al., 1993) The RUC (Benjamin et al., 1996) was implemented for CONUS domain with three-hourly OI updates at 60-km resolution on 25 hybrid (sigma-theta) vertical levels."}
{"text": "Early Eta analysis upgrades (Rogers et al., 1995) A mesoscale version of the Eta model (Black, 1994) was implemented at 03Z and 15Z for an extended CONUS domain, with 29-km and 50-layer resolution and with NMC\u2019s \ufb01rst predictive cloud scheme (Zhao and Black, 1994) and new coupled land-surface\u2013 atmosphere package (two-layer soil)."}
{"text": "Major upgrade of early Eta runs: 48-km resolution, cloud scheme and Eta Data Assimilation System (EDAS) using three-hourly OI updates (Rogers et al., 1996) New coupled land-surface\u2013atmosphere scheme put into early Eta runs (Chen et al., 1997, Nested capability demonstrated with twice-daily support runs for Atlanta Olympic Games with 10-km 60-layer version of Meso Eta."}
{"text": "1 Historical overview of numerical weather prediction Upgrade package implemented in the early and Early Eta runs upgraded to 32 km and 45 levels with four soil layers. OI analysis replaced by 3D-Var with new data sources. EDAS now partially cycled (soil moisture, soil temperature, cloud water/ice & turbulent kinetic RUC (three-hourly) replaced by hourly RUC II system with extended CONUS domain, 40-km and 40-level resolution, additional data sources and extensive physics upgrades."}
{"text": "Meso runs connected to early runs as a single 4/day system for North American domain at 32-km and 45-level resolution, 15Z run moved to 18Z, added new snow analysis. All runs connected with EDAS, which is fully cycled for continental USA (CONUS), developed at NOAA\u2019s Forecast System Labora- tory, was implemented in 1994 and upgraded in 1998 (Benjamin et al., 1996)."}
{"text": "The 36-h S1 forecast veri\ufb01cation scores constitute the longest record of forecast veri\ufb01cation available anywhere. They were started in the late 1940s for subjective surface forecasts, before operational computer forecast guidance, and for 500 hPa in 1954, with the \ufb01rst numerical forecasts. Figure 1.1.1(a) includes the forecast scores for 500 hPa from 1954 until the present, as well as the scores for the 72-h forecasts. It is clear that the forecast skill has improved substantially over the years, and that the current 36-h 500-hPa forecasts are close to a level that in the 1950s would have been considered \u201cperfect\u201d (Shuman, 1989). The 72-h forecasts have also improved, and are now as accurate as the 36-h forecasts were about 15 years ago. This doubling of the skill over 10\u201320 years can be observed in other types of forecasts veri\ufb01cations as well."}
{"text": "As indicated at the beginning of this chapter, the 36-h forecasts of 500 hPa showing the position and intensity of the large-scale atmospheric waves and centers of high and low pressure are generally excellent, as suggested by the nearly \u201cperfect\u201d S1 score."}
{"text": "However, sea level pressure maps are more affected by mesoscale structures, such as fronts and convective systems which are still dif\ufb01cult to forecast in detail, and hence they have a poorer S1 score (Fig. 1.1.1(b)). The solid line with circles starts in 1947 with scores from subjectively made surface forecasts, then barotropic and baroclinic quasi-geostrophic models (Table 1.5.1), the LFM model and since 1983, the global 1.5 Operational NWP and the evolution of forecast skill annual average) of human forecasters at NCEP (data courtesy of J. Hoke)."}
{"text": "spectral model (denoted Aviation or AVN). Other model forecasts are also presented separately on Fig. 1.1.1(b). Note that the AVN model and the Eta model, which con- tinue to be developed, show the most improvement. The development of the LFM was \u201cfrozen\u201d in 1986, and that of the NGM in 1991, when more advanced systems were implemented, and therefore their forecasts show no further improvement with time (except for the effect of improved global forecasts used as a \ufb01rst guess for the LFM)."}
{"text": "Fig. 1.5.1 shows threat scores for precipitation predictions made by expert fore- casters from the NCEP Hydrometeorological Prediction Center (HPC, the Meteoro- logical Operations Division of the former NMC). The threat score (TS) is de\ufb01ned as the intersection of the predicted area of precipitation exceeding a particular threshold (P), in this case 0.5 inches in 24 h, and the observed area (O), divided by the union of the two areas: TS = (P \u2229O)/(P \u222aO). The bias (not shown) is de\ufb01ned by P/O."}
{"text": "The TS, also known as critical success index (CSI) is a particularly useful score for quantities that are relatively rare. Fig. 1.4.2 indicates that the forecasters skill in pre- dicting accumulated precipitation has been increasing with time, and that the current average skill in the 2-d forecast is as good as the 1-d forecasts were in the 1970s."}
{"text": "Beyond the \ufb01rst 6\u201312 h, the forecasts are based mostly on numerical guidance, so that the improvement re\ufb02ects to a large extent improvements of the numerical forecasts, which the human forecasters in turn improve upon based on their knowledge and expertise. The forecasters also have access to several model forecasts, and they use their judgment in assessing which one is more accurate in each case. This constitutes a major source of the \u201cvalue-added\u201d by the human forecasters."}
{"text": "It is the \ufb01rst operational score maintained for the \u201cmedium-range\u201d (beyond the \ufb01rst two days of the forecasts). The score used by Hughes was a standardized anomaly correlation (SAC), which accounted for the larger variability of sea level pressure at higher latitudes compared to lower latitudes. Unfortunately the SAC is not directly comparable to other scores such as the anomaly correlation (discussed in the next section). The fact that until 1976 the 3-day forecast scores from the model were essentially constant is an indication that their rather low skill was more based on synoptic experience than on model guidance. The forecast skill started to improve after 1977 for the 3-day forecast, and after 1980 for the 5-day forecast. Note that the human forecasts are on the average signi\ufb01cantly more skillful than the numerical guidance, but it is the improvement in NWP forecasts that drives the improvements in the subjective forecasts."}
{"text": "Nonhydrostatic mesoscale models The hydrostatic approximation involves neglecting vertical accelerations in the ver- tical equation of motion, compared to gravitational acceleration. This is a very good approximation, even in strati\ufb01ed \ufb02uids, as long as horizontal scales of motion are larger than the vertical scales. The main advantage of the hydrostatic equation (Chap- ter 2) is that it \ufb01lters sound waves (except those propagating horizontally, or Lamb 1.7 Weather predictability waves). Because of the problem of computational instability, the absence of sound waves allows the use of larger time steps (the Lamb waves are handled generally with semi-implicit time schemes, discussed in Section 3.3)."}
{"text": "The hydrostatic approximation is very accurate if the horizontal scales are much larger than the vertical scales. For atmospheric models with horizontal grid sizes of the order of 100 km, the hydrostatic equation is very accurate and convenient. Fur- thermore, for quasi-geostrophic (slow) motion, the hydrostatic equation is accurate even if the horizontal scales are of the same order as the vertical scales, i.e., the hydrostatic approximation can be used even in mesoscale models with grid sizes of the order of 10 km or larger without introducing large errors."}
{"text": "However, in order to represent smaller-scale phenomena such as storms or con- vective clouds which have vertical accelerations that are not negligible compared to buoyancy forces, it is necessary to use the equations of motion without the hy- drostatic approximation. In the last decade a number of nonhydrostatic models have been developed in order to simulate mesoscale phenomena in North America. They include the Penn State/NCAR Mesoscale Model (e.g., Dudhia, 1993), the CAPS Advanced Regional Prediction System (Xue et al., 1995), NCEP\u2019s Regional Spectral Model (Juang et al., 1997), the Mesoscale Compressible Community (MCC) model (Laprise et al., 1997), the CSU RAMS (Tripoli and Cotton 1980), the US Navy COAMPS (Hodur, 1997). In Europe and Japan several other nonhydrostatic models have been developed as well."}
{"text": "Sound waves, which are generally of no consequence for atmospheric \ufb02ow but would require the use of very small steps, require a special approach in nonhydrostatic models in order to maintain a reasonable computational ef\ufb01ciency. Sound waves depend on compressibility (three-dimensional divergence) for their propagation. For this reason, some nonhydrostatic models use the quasi-Boussinesq or \u201canelastic\u201d equations, where the atmosphere is assumed to be separated into a hydrostatic basic state and perturbations, and where the density perturbations are neglected everywhere except in the buoyancy terms (Ogura and Phillips, 1962, Klemp and Wilhelmson, 1978). Other approaches are the use of arti\ufb01cial \u201cdivergence damping\u201d in the pressure gradient terms (e.g., Xue et al., 1995, Skamarock and Klemp, 1992), and the use of implicit time schemes for the terms affecting sound waves that are unconditionally stable (Durran and Klemp, 1983, Laprise et al., 1997)."}
{"text": "Nonhydrostatic models with an ef\ufb01cient (e.g., semi-implicit) treatment of sound waves are computationally competitive with hydrostatic models, and future genera- tions of models may become nonhydrostatic even in the global domain."}
{"text": "Weather predictability, ensemble forecasting, and seasonal to interannual prediction In a series of remarkable papers, Lorenz (1963a,b, 1965, 1968) made the fundamental discovery that even with perfect models and perfect observations, the chaotic nature 1 Historical overview of numerical weather prediction of the atmosphere would impose a \ufb01nite limit of about two weeks to the predictability of the weather. He proved this by running a simple atmospheric model, introducing (by mistake) exceedingly small perturbations in the initial conditions, and running the model again. With time, the small difference between the two forecasts became larger and larger, until after about two weeks, the forecasts were as different as two randomly chosen states of the model. In the 1960s Lorenz\u2019s discovery, which started the theory of chaos, was \u201conly of academic interest\u201d and not relevant to operational weather forecasting, since at that time the skill of even two-day operational forecasts was low."}
{"text": "Since then, however, computer-based forecasts have improved so much that Lorenz\u2019s limit of predictability is starting to become attainable in practice, especially with ensemble forecasting. Furthermore, skillful prediction of longer lasting phenomena such as El Ni\u02dcno is becoming feasible (Chapter 6)."}
{"text": "Because the skill of the forecasts decreases with time, Epstein (1969) and Leith (1974) suggested that instead of performing \u201cdeterministic\u201d forecasts, stochastic forecasts providing an estimate of the skill of the prediction should be made. The only computationally feasible approach in order to achieve this goal is through \u201cen- semble forecasting\u201d in which several model forecasts are performed by introducing perturbations in the initial conditions or in the models themselves."}
{"text": "After considerable research on how to most effectively perturb the initial con- ditions, ensemble forecasting was implemented operationally in December 1992 at both NCEP and ECMWF (Tracton and Kalnay, 1993, Toth and Kalnay, 1993, Palmer et al., 1993, Molteni et al., 1996, Toth and Kalnay, 1997). Since 1994 NCEP has been running 17 global forecasts per day, each out to 16 days, with initial pertur- bations obtained using the method of breeding growing perturbations. This ensures that the initial perturbations contain naturally growing dynamical perturbations in the atmosphere, which are also present in the analysis errors. The length of the fore- casts allows the generation of \u201coutlooks\u201d for the second week. The NCEP ensemble forecasts can be accessed through the world-wide web at the EMC home page (nic.fb4.noaa.gov:8000), and linking to the ensemble home page. At ECMWF, the perturbation method is based on the use of singular vectors, which grow even faster than the bred or Lyapunov vector perturbations. The ECMWF ensemble contains Ensemble forecasting has accomplished two main goals: the \ufb01rst one is to provide an ensemble average forecast that beyond the \ufb01rst few days is more accurate than individual forecasts, because the components of the forecast that are most uncertain tend to be averaged out. The second and more important goal is to provide forecasters with an estimation of the reliability of the forecast, which because of changes in atmospheric predictability, varies from day to day and from region to region."}
{"text": "The \ufb01rst goal is illustrated in Fig. 1.7.1, prepared at the Climate Prediction Center (CPC, the Climate Analysis Center of the former NMC) for the veri\ufb01cation of the NCEP ensemble during the winter of 1997\u20138. This was an El Ni\u02dcno winter with major anomalies in the atmospheric circulation, and the operational forecasts had excellent 1.7 Weather predictability (controls, T126 and T62, and ten perturbed ensemble forecasts). (Data courtesy Jae skill. The control \u201cdeterministic\u201d forecast (circles) had an \u201canomaly correlation\u201d (AC, pattern correlation between predicted and analyzed anomalies) in the 5-day forecast of 80%, which is quite good. The ten perturbed ensemble members have individually a poorer veri\ufb01cation with an average AC of about 73% at 5 days. This is because, in the initial conditions, the control starts from the best estimate of the state of the atmosphere (the analysis), but growing perturbations are added to this analysis for each additional ensemble member. However, the ensemble average forecast tends to average out uncertain components, and as a result, it has better skill than the control forecast starting at day 5. Note that the ensemble extends by one day the length of the useful forecast (de\ufb01ned as an AC greater than 60%) from about 7 days in the control to about 8 days in the ensemble average."}
{"text": "The second goal of the ensemble forecasting, to provide guidance on the uncer- tainty of each forecast, is accomplished best by the use of two types of plots. The \u201cspaghetti\u201d plots show a single contour line for all 17 forecasts, and the probabilistic plots show, for example, what percentage of the ensemble predicts 24-h accumulated precipitation of more than 1 inch at each grid point (for probabilistic Quantitative Precipitation Forecasts or pQPF). Both of them provide guidance on the reliability of the forecasts in an easy-to-understand way. The use of the ensembles has provided the US NWS forecasters with the con\ufb01dence to issue storm forecasts 5\u20137 days in ad- vance when the spaghetti plots indicate good agreement in the ensemble. Conversely, the spaghetti plots also indicate when a short-range development may be particularly 1.7 Weather predictability dif\ufb01cult to predict, so that the users should be made aware of the uncertainty of the forecast. Fig. 1.7.2(a) shows an example of the 5-day forecast for 15 November 1995, the \ufb01rst East Coast winter storm of 1995\u20136: the fact that the ensemble showed good agreement provided the forecasters with the con\ufb01dence to issue a storm forecast these many days in advance. By contrast, Fig. 1.7.2(b) shows a 2.5-day forecast for a storm with veri\ufb01cation time 21 October 1995, and it is clear that even at this shorter range, the atmosphere is much less predictable and there is much more uncertainty about the location of the storm."}
{"text": "The use of ensembles has also led to another major development, the possibility of an adaptive or targeted observing system. As an example, consider a case in which the lack of agreement among the ensemble members indicates that a 3-day forecast in a certain region is exceedingly uncertain, as in Fig. 1.7.2(b). Several new techniques have been developed to trace such a region of uncertainty backward in time, for example 2 days. These techniques will point to a region or regions where additional observations would be especially useful. The additional observations could be dropwinsondes launched from a reconnaissance or a pilotless airplane, additional rawinsondes, or especially intensive use of satellite data such as a Doppler Wind Lidar. If additional observations are available 24 h after the start of the originally critically uncertain 3-day forecast, they can increase substantially the usefulness of the 2-day forecast. Similarly, a few additional rawinsondes could be launched where short-range ensemble forecasts (12\u201324 h) indicate that they are most needed."}
{"text": "Preliminary tests of this approach of targeted observations have been successfully performed within an international Fronts and Storm Track Experiment (FASTEX) in the North Atlantic during January and February 1997, and in the North Paci\ufb01c Experiment (NORPEX) in January and February 1998 (Szunyogh et al., 2000)."}
{"text": "Ensemble forecasting also provides the basic tool to extend forecasts beyond Lorenz\u2019s 2-week limit of weather predictability (Chapter 6). Slowly varying surface forcing, especially from the tropical ocean and from land-surface anomalies, can produce atmospheric anomalies that are longer lasting and more predictable than individual weather patterns. The most notable of these is the El Ni\u02dcno\u2013Southern Os- cillation (ENSO) produced by unstable oscillations of the coupled ocean\u2013atmosphere system, with a frequency of 3\u20137 years. Because of their long time scale, the ENSO oscillations should be predictable a year or more in advance (in agreement with the chaos theory). The \ufb01rst successful experiments in this area were made by Cane et al."}
{"text": "(1986) with a simple coupled atmosphere\u2013ocean model. The warm phases of ENSO (ElNi\u02dcnoepisodes)areassociatedwithwarmseasurfacetemperature(SST)anomalies Caption for Figure 1.7.2: (a) Spaghetti plot for the 5-day forecast for 15 Nov 1995, a case of a very predictable storm over eastern USA. (Figure courtesy of R. Wobus, NCEP.) (b) Spaghetti plot for the 2.5-day forecast for 21 Oct 1995, the case of a very unpredictable storm over the USA. (Courtesy of R. Wobus, NCEP.) Dashes indicate 1 Historical overview of numerical weather prediction intheequatorialcentralandeasternPaci\ufb01cOcean,andcoldphases(LaNi\u02dcnaepisodes) with cold anomalies. NCEP started performing multiseasonal predictions with cou- pled comprehensive atmosphere\u2013ocean models in 1995, and ECMWF did so in 1997."}
{"text": "A single atmospheric forecast forced with the SST anomalies would not be use- ful beyond the \ufb01rst week or so, when unpredictable weather variability would mask the forced atmospheric anomalies. Ensemble averaging many forecasts made with atmospheric models forced by SST anomalies (and by other slowly varying anoma- lies over land such as soil moisture and snow cover) allows the \ufb01ltering out of the unpredictable components of the forecast, and the retention of more of the forced predictable components. This \ufb01ltering is re\ufb02ected in the fact that the ensemble aver- age for the second week of the forecasts for the winter of 1997\u20138 (Fig. 1.7.1) had a high AC of 57%, much higher than previously obtained. Researchers at the Japanese Meteorological Agency have performed forecasts for the 28-day average and also found that ensemble averaging substantially increased the information on the second week and the last 2 weeks of the forecast. The very successful operational forecasts of the ENSO episode of 1997\u20138 performed at both NCEP and ECMWF have been substantially based on the use of ensembles to extract the useful information on the impact of El Ni\u02dcno from the \u201cweather noise\u201d."}
{"text": "The last decades have seen the expectations of Charney (1951) ful\ufb01lled, and an amazing improvement in the quality of the forecasts based on NWP guidance. From the active research taking place, one can envision that the next decade will continue to bring improvements, especially in the following areas:"}
{"text": "detailed short-range forecasts, using storm-scale models able to provide skillful predictions of severe weather; more sophisticated methods of data assimilation able to extract the maximum possible information from observing systems, especially remote sensors such as satellites and radars; development of adaptive observing systems, in which additional observations are placed where ensembles indicate that there is rapid error growth (low improvement in the usefulness of medium-range forecasts, especially through the use of ensemble forecasting; fully coupled atmospheric\u2013hydrological systems, where the atmospheric model precipitation is appropriately downscaled and used to extend the length more use of detailed atmosphere\u2013ocean\u2013land coupled models, in which long-lasting coupled anomalies such as SST and soil moisture anomalies lead to more skillful predictions of anomalies in weather patterns beyond the limit of weather predictability (about two weeks); more guidance to governments and the public on subjects such as air pollution, ultraviolet radiation and transport of contaminants, which affect an explosive growth of systems with emphasis on commercial applications of NWP, from guidance on the state of highways to air pollution, \ufb02ood prediction, guidance to agriculture, construction, etc."}
{"text": "Newton\u2019s second law or conservation of momentum (three equations for the three velocity components); the continuity equation or conservation of mass; the equation of state for ideal gases; the \ufb01rst law of thermodynamics or conservation of energy; a conservation equation for water mass."}
{"text": "Newton\u2019s second law or conservation of momentum: On an inertial frame of reference, the absolute acceleration of a parcel of air in three On a rotating frame of reference centered at the center of the earth, the absolute velocity va is given by the sum of the relative velocity v plus the velocity due to the rotation with angular velocity \u2126:"}
{"text": "where r is the position vector of the parcel. This is a particular case (for A = r) of the general formula relating the total time derivative of any vector on a rotating frame dA/dt to its total derivative in an inertial frame daA/dt:"}
{"text": "We can also apply this formula to A = va, giving Substituting (2.1.2) into (2.1.4) we obtain that the accelerations in an inertial (abso- lute) and a rotating frame of reference are related by This equation indicates that on a rotating frame of reference there are two apparent forces per unit mass: the Coriolis force (second term on the right-hand side) and the centrifugal force (third term)."}
{"text": "The left-hand side of (2.1.5) represents the real forces acting on a parcel of air, i.e., the pressure gradient force \u2212\u03b1\u2207p, the gravitational acceleration ge = \u2212\u2207\u03c6e, and the frictional force F. Therefore in a rotating frame of reference moving with the earth, the apparent acceleration is given by Here \u03b1 = 1/\u03c1 is the speci\ufb01c volume (the inverse of the density \u03c1), p is the pressure, \u03c6e is the Newtonian gravitational potential of the earth, and, as indicated before, the last two terms are the apparent accelerations, denoted the Coriolis force and centrifugal force respectively. We have not included the tidal potential, whose effects are negligible below about 100 km."}
{"text": "We can now combine the centrifugal force with the gravitational force, since \u2212\u2126\u00d7 (\u2126\u00d7 r) = \u21262l = \u2207(\u00152l2/2),where l is the position vector from the axis of rotationtotheparcel.Thereforewecande\ufb01neasthe\u201cgeopotential\u201d\u03c6 = \u03c6e \u2212\u00152l2/2, and the apparent gravity is given by We de\ufb01ne the geographic latitude \u03d5 to be perpendicular to the geopotential \u03c6. At the surface of the earth, the geographic latitude and the geocentric latitude differ by less than 10 minutes of a degree of latitude. Therefore, Newton\u2019s law on the rotating 2 The continuous equations frame of the earth is written as Continuity equation or equation of conservation of mass This can be derived as follows: Consider the mass of a parcel of air of density \u03c1 If we follow the parcel in time, it conserves its mass, i.e., the total time derivative (also called the substantial, individual or Lagrangian time derivative) is equal to zero:"}
{"text": "and similarly for the other directions y, z. Now, the total derivative of any function f (x, y, z, t), following a parcel, can be Equation (1.11) indicates that the total (or Lagrangian or individual) time derivative of a property is given by the local (partial, Eulerian) time derivative (at a \ufb01xed point) plus the changes due to advection. If we expand d\u03c1/dt in (2.1.10) using (2.1.11) we obtain an alternative form of the continuity equation, usually referred to as \u201cin \ufb02ux Equation of state for perfect gases The atmosphere can be assumed to be a perfect gas, for which the pressure p, speci\ufb01c volume \u03b1 (or its inverse \u03c1, density), and temperature T are related by where R is the gas constant for air. This equation indicates that given two thermody- namic variables, the others are determined."}
{"text": "Thermodynamic energy equation or conservation of energy equation This equation expresses that if heat is applied to a parcel at a rate of Q per unit mass, this heat can be used to increase the internal energy CvT and/or to produce work of The coef\ufb01cients of speci\ufb01c heat at constant volume Cv and at constant pressure C p are related by C p = Cv + R. We can use the equation of state (2.1.13) to derive another form of the thermodynamic equation:"}
{"text": "The rate of change of the speci\ufb01c entropy s of a parcel is given by ds/dt = Q/T , i.e., the diabatic heating divided by the absolute temperature. We now de\ufb01ne potential temperature by \u03b8 = T (p0/p)R/C p, where p0 is a reference pressure (1000 hPa). With this de\ufb01nition, it is easy to show that the potential temperature and the speci\ufb01c entropy This shows that potential temperature is individually conserved in the absence of Equation for conservation of water vapor mixing ratio q This equation simply indicates that the total amount of water vapor in a parcel is conserved as the parcel moves around, except when there are sources (evaporation E) and sinks (condensation C):"}
{"text": "Conservation equations for other atmospheric constituents can be similarly written in terms of their corresponding sources and sinks. If we multiply (2.1.17) by \u03c1, expand the total derivative dq/dt = \u2202q/\u2202t + v \u00b7 \u2207q, and add the continuity equation (2.1.12) multiplied by q, we can write the conservation of water in an alternative \u201c\ufb02ux The \ufb02ux form of the time derivative is very useful in the construction of models."}
{"text": "The \ufb01rst term of the right-hand side of (2.1.18) is the convergence of the \ufb02ux of q. Note that we can include similar conservation equations for additional tracers such as liquid water, ozone, etc., as long as we also include their corresponding sources 2 The continuous equations We now have seven equations with seven unknowns: v = (u, v, w), T, p, \u03c1 or \u03b1, and q. For convenience we repeat the governing equations, which (when written without friction F) are sometimes referred to as \u201cthe Euler equations\u201d:"}
{"text": "Atmospheric equations of motion Since the earth is nearly spherical, it is natural to use spherical coordinates. Near the earth, gravity is almost constant, and the ellipticity of the earth is very small, so that one can accurately approximate scale factors by those appropriate for true spher- ical coordinates (Phillips, 1966, 1973, 1990a). The three velocity components are u = zonal (positive eastward) = r cos \u03d5 d\u03bb v = meridional (positive northward) = r d\u03d5 w = vertical (positive up) = dr Note that v = ui + vj + wk, where i, j, k are the unit vectors in the three orthogonal spherical coordinates. When the acceleration (total derivative of the velocity vector) is calculated, the rate of change of the unit vectors has to be included. For example, geometrical considerations show that Exercise 2.2.1: Use spherical geometry to derive this equation, and r cos \u03d5 ( j sin \u03d5 \u2212k cos \u03d5) r cos \u03d5 (\u2212i sin \u03d5 \u2212k cos \u03d5) When we include these time derivatives, take into account that \u2126= \u0015 sin \u03d5k + \u0015 cos \u03d5j, and expand the momentum equation (2.1.19) into its three components, we 2.3 Basic wave oscillations in the atmosphere The terms proportional to u/r cos \u03d5 are known as \u201cmetric terms\u201d."}
{"text": "A \u201ctraditional approximation\u201d (Phillips, 1966) has been routinely made in NWP, since most of the atmospheric mass is con\ufb01ned to a few tens of kilometers. This suggests that in considering the distance of a point to the center of the earthr = a + z, one can neglect z and replace r by the radius of the earth a = 6371 km, replace \u2202/\u2202r by \u2202/\u2202z, and neglect the metric and Coriolis terms proportional to cos \u03d5. Then the equations of motion in spherical coordinates become which possess the angular momentum conservation principle dt [(u + \u0015a cos \u03d5)a cos \u03d5] = a cos \u03d5 With the \u201ctraditional approximation\u201d the total time derivative operator in spherical and the three-dimensional divergence that appears in the continuity equation by Basic wave oscillations in the atmosphere In order to understand the problems in Richardson\u2019s result in 1922 (Fig. 1.2.1) and the effect of the \ufb01ltering approximations introduced by Charney et al. (1950), we need to have a basic understanding of the characteristics of the different types of waves present in the atmosphere. The characteristics of these waves, (sound, gravity, 2 The continuous equations and slower weather waves) have also profound implications for the present use of hydrostatic and nonhydrostatic models. The three types of waves are present in the solutions of the governing equations, and different approximations such as the hydrostatic, the quasi-geostrophic, and the anelastic approximations are designed to To simplify the analysis we make a tangent plane or \u201c f -plane\u201d approximation."}
{"text": "We consider motions with horizontal scales L smaller than the radius of the earth. On this tangent plane we can approximate the spherical coordinates (Section 2.2) by and ignore the metric terms, since u/(a tan \u03d5) is small compared with \u0015."}
{"text": "Consider a basic state at rest u0 = v0 = w0 = 0. From (2.3.1a) and (2.3.1b), we see that p0 does not depend on x,y, p0 = p0(z). From (2.3.1c), \u03c10 and therefore the other basic state thermodynamic variables also depend on z only."}
{"text": "Assume that the motion is adiabatic and frictionless, Q = 0, F = 0. Consider small perturbations p = p0 + p\u2032, etc. so that we can linearize the equations (ne- glect terms which are products of perturbations). For convenience, we de\ufb01ne u\u2217= \u03c10u\u2032; v\u2217= \u03c10v\u2032; w\u2217= \u03c10w\u2032; s\u2217= \u03c10s\u2032. The perturbation equations are then 2.3 Basic wave oscillations in the atmosphere T (p0/p)R/C p, C p = R + Cv, \u03b3 = C p/Cv = 1.4, and c2 the square of the speed of sound."}
{"text": "Pure types of plane wave solutions We \ufb01rst consider special cases with pure wave type solutions. They exist in their pure form only under very simpli\ufb01ed assumptions. However, if we understand their basic characteristics, we will understand their role in the full nonlinear models, and the methodology used for \ufb01ltering some of the waves out. We will be assuming plane wave solutions aligning the x-axis along the horizontal direction of propagation:"}
{"text": "Here k = 2\u03c0/Lx and m = 2\u03c0/Lz are horizontal and vertical wavenumbers, re- spectively, \u03bd = 2\u03c0/T is the frequency, and U, V, W, and P are constant am- plitudes. We will aim to derive the frequency dispersion relationship (FDR) \u03bd = f (k, m, parameters) for each type of wave by substituting the plane wave for- mulation (2.3.3) into the linear equation, and eliminating variables. The FDR gives us not only the frequency, but also the phase speed components (\u03bd/k, \u03bd/m) as well as the group velocity components (\u2202\u03bd/\u2202k,\u2202\u03bd/\u2202m). The phase speed is the speed of individual wave crests and valleys, and the group velocity is the speed at which wave energy propagates in the horizontal and vertical directions. A pure type of wave occurs under idealized conditions, such as no rotation, no strati\ufb01cation for sound waves, but its basic characteristics are retained even if the ideal conditions are not valid (sound waves are still present but slightly modi\ufb01ed in the presence of rotation We neglect rotation, strati\ufb01cation and gravity: f = 0 , g = 0, ds0/dz = 0. From (2.3.2e), we have s\u2217= 0 (recall that s\u2217is a perturbation, and if it was constant, we would have included its value into the basic state s0). Therefore p\u2032 = c2 2 The continuous equations These show that sound waves occur through adiabatic expansion and contraction (three-dimensional divergence), and that the pressure perturbation is proportional to the density perturbation."}
{"text": "Assuming plane wave solutions (2.3.3), with the x-axis along the horizontal di- rection of the waves, and substituting into (2.3.4), we get From (2.3.5b) V = 0, and substituting U and W from (2.3.5a) and (2.3.5c) into (2.3.5d), we get the FDR:"}
{"text": "These are sound waves that propagate through air compression or three-dimensional divergence. The components of the phase velocity are (\u03bd/k, \u03bd/m) and the total phase 2.3.1.2 Lamb waves (horizontally propagating sound waves) We now neglect rotation and assume that there is only horizontal propagation (no vertical velocity), but we allow for the \ufb02uid to be gravitationally strati\ufb01ed. With f = 0 and w\u2217= 0, we again have s\u2217= 0, and from (2.3.2f) p\u2032 = c2 (2.3.2c) the \ufb02ow is now hydrostatic: \u2202p\u2032/\u2202z = \u2212\u03c1\u2032g. If we insert the same type of plane wave solutions (2.3.3) into (2.3.2), we \ufb01nd that p\u2032 = Pe\u2212(g/c2 the vertical wavenumber is imaginary m = ig/c2 s, and the phase speed is \u03bd2/k2 = c2 Since the vertical wavenumber is imaginary, there is no vertical propagation, and the Therefore, a Lamb wave is a type of external horizontal sound wave, which is present in the solutions of models even when the hydrostatic approximation is made."}
{"text": "2.3 Basic wave oscillations in the atmosphere This is very important because it means primitive equation models (which make the hydrostatic approximation) contain these fast moving horizontal sound waves. We will see that Lamb waves are also equivalent to the gravity waves in a shallow water model. Note also that the FDR is such that \u03bd/k = \u00b1c2 s, so that the phase speed does not depend on the wavenumber. This implies that the group velocity \u2202\u03bd/\u2202k = \u00b1c2 is also independent of the wavenumber, and as a result Lamb waves without rotation are nondispersive, so that a package of waves will move together and not disperse."}
{"text": "2.3.1.3 Vertical gravitational oscillations Now we neglect rotation and pressure perturbations, f = p\u2032 = 0, so that there is no horizontal motion, but allow for vertical strati\ufb01cation. Equations (2.3.2) become From these two equations we get and from the continuity equation we obtain Substituting the plane wave solution (2.3.3) into (2.3.8) we obtain \u03bd2 = N 2, where N 2 = gd ln \u03b80/dz is the square of the Brunt\u2013Va\u00a8\u0131s\u00a8al\u00a8a frequency. A typical value of N for the atmosphere is N \u223c10\u22122 s\u22121. A parcel displaced in a stable atmosphere will oscillate vertically with frequency N. Equations (2.3.7b) and (2.3.9) show that the amplitude of w\u2217will decrease with height as e\u2212(d ln \u03b8/dz)z."}
{"text": "2.3.1.4 Inertia oscillations Inertia oscillations are horizontal and are due to the basic rotation. We now assume that p\u2032 = 0, ds0/dz = 0, and there are no pressure perturbations and no strati\ufb01cation."}
{"text": "Then s\u2217= 0, and, therefore, \u03c1\u2032 = 0 and the horizontal equations of motion become As indicated by (2.3.10), the frequency of inertia oscillations is \u03bd = \u00b1 f , with the acceleration perpendicular to the wind, corresponding to a circular wind oscillation."}
{"text": "In the presence of a basic \ufb02ow, there is also a translation, and the trajectories look 2 The continuous equations an inertial oscillation in the presence of a basic \ufb02ow to 2.3.1.5 Lamb waves in the presence of rotation and geostrophic modes:"}
{"text": "We now consider the same case as in Section 2.3.1.2 of horizontally propagating Lamb waves, but without neglecting rotation, i.e., f \u0338= 0, but the vertical velocity is still zero. From w\u2217= 0 and (2.3.2c) we have again p\u2032 = c2 balance in (2.3.2g) then implies \u2202p\u2032/\u2202z = \u2212\u03b3 RT0. Therefore the three-dimensional The system of equations (2.3.2) becomes Thissystemiscompletelyanalogoustothelinearizedshallowwaterequations(SWE) which are widely used in NWP as the simplest primitive equations model:"}
{"text": "Note that this FDR contains two types of solution: one type is \u03bd2 = f 2 + c2 Lamb waves modi\ufb01ed by inertia (rotation), or inertia Lamb waves. In the SWE analog, these are inertia-gravity waves (external gravity waves modi\ufb01ed by inertia), \u03bd2 = f 2 + \u0007k2. Note that in the presence of rotation the phase speed and group velocity depend on the wavenumber: rotation makes Lamb waves dispersive (and this helps with the problem of getting rid of noise in the initial conditions as in 2.3 Basic wave oscillations in the atmosphere The second type of solution (and for us the more important!) is the steady state so- lution \u03bd = 0. This means that \u2202()/\u2202t = \u2212i\u03bd() = 0 for all variables. Without the pres- ence of rotation, this steady state solution would be trivial: u\u2217= v\u2217= w\u2217= p\u2032 = 0."}
{"text": "But with rotation, an examination of (2.3.13) or (2.3.12) shows that this is the geostrophic mode: U = 0, \u2207\u00b7 v\u2217= \u2202U/\u2202x = 0, but V = ikP/f , i.e., This is a steady state, but nontrivial, geostrophic solution. If we add a dependence of f on latitude, the geostrophic solution becomes the Rossby waves solution, which is not steady state, but is still much slower than gravity waves or sound waves."}
{"text": "General wave solution of the perturbation equations in a resting, isothermal atmosphere So far we have been making drastic approximations to obtain \u201cpure\u201d elementary waves (sound, inertia and gravity oscillations). We now consider a more general case, including all waves simultaneously. We consider again the equations for small perturbations (2.3.2), and assume a resting, isothermal basic state in the atmosphere:"}
{"text": "T0(z) = T00, a constant. Then where \u03ba = R/C p = 0.4. Since the basic state is hydrostatic, These equations show that for an isothermal atmosphere, both N 2 and the scale height We continue considering an f -plane, a reasonable approximation for horizontal scales L small compared to the radius of the earth: L << a. If L were not small compared with the radius of the earth, we would have to take into account the vari- ation of the Coriolis parameter with latitude, and spherical geometry. With some manipulation, assuming that the waves propagate along the x-axis, and there is no y-dependence, the perturbation equations (2.3.2) become 2 The continuous equations In these equations we have introduced two constants \u03b1 and \u03b2 as markers for the hydrostatic and the quasi-Boussinesq approximations respectively. They can take the value 1 or 0. If we make \u03b1 = 0, it indicates that we are making the hydrostatic ap- proximation, i.e., neglecting the vertical acceleration in (2.3.17c). If we make \u03b2 = 0, it indicates that we are making the anelastic or quasi-Boussinesq approximation, i.e., assuming that the mass weighted three-dimensional divergence is zero. Otherwise the markers take the value 1. These markers will be used in the next section, where we discuss \ufb01ltering approximations."}
{"text": "We now try plane wave solutions, where the basic state is a function of z of the Instead of assuming a z-dependence of the form ei(mz), we will determine it explicitly. If the horizontal scale is not small compared with the radius of the earth, L \u223ca, then the solutions are of the form (u\u2217, v\u2217, w\u2217, p\u2032, \u03c1\u2032) = (U(z), V (z), W(z), P(z), R(z))A(\u03d5)ei(s\u03bb\u2212\u03bdt), and the equation obtained for A(\u03d5) is the Laplace tidal equation."}
{"text": "Substituting the assumed form of the solution (2.3.18) into (2.3.17) we get From (2.3.19a) and (2.3.19b) From (2.3.19d) and (2.3.19f) From (2.3.19c) and (2.3.19e) 2.3 Basic wave oscillations in the atmosphere From (2.3.19e) and (2.3.19g) From (2.3.19h) and (2.3.19i) or a similar equation for P. This last equation is of the form In order to eliminate the \ufb01rst derivative, we try a substitution of the form W = e\u03b4z\u0015, and obtain d2\u0015/dz2 + C\u0015 = 0. This requires that we choose \u03b4 = \u2212A/2, and in that From (2.3.20), the variable substitution, and additional sweat, we \ufb01nally obtain This is the frequency dispersion relationship for waves in an atmosphere with an isothermal basic state. Given a horizontal structure of the wave (k), and its frequency (\u03bd), (2.3.22) determines the vertical structure (n) of \u0015 (and W), and vice versa."}
{"text": "If n2 < 0, the vertical wavenumber n is imaginary, n = im. The solution of (2.3.21) is then \u0015 = Aemz + Be\u2212mz, or, going back to the vertical velocity, w\u2217(x, z, t) = ei(kx\u2212\u03bdt)e\u22121 These are external waves (the waves do not oscillate in the vertical, and therefore do not propagate vertically). If the boundary condition at the ground is that the vertical velocity is zero, then \u0015 = Aemz + Be\u2212mz = 0 at z = 0, so that A + B = 0, and w\u2217(x, z, t) = ei(kx\u2212\u03bdt)e\u22121 2 The continuous equations density weighted internal which has an exponential behavior in z. Since sinh(mz) cannot be zero above the ground, an upper boundary condition of a rigid top can only be satis\ufb01ed if A = 0."}
{"text": "If n2 > 0, the vertical wavenumber n is real: w\u2217(x, z, t) = ei(kx\u2212\u03bdt)(Aeinz + Be\u2212inz)e\u22121 A, B are determined from the boundary conditions. Now there is both vertical and horizontal propagation. For example, if there is a rigid bottom, we have again A + B = 0, and the solution becomes ei(kx+nz\u2212\u03bdt) \u2212ei(kx\u2212nz\u2212\u03bdt) The shape of internal waves in the vertical is shown schematically in Fig. 2.3.2."}
{"text": "Analysis of the FDR of wave solutions in a resting, We will now plot the general FDR equation (2.3.22). We assume T00 = 250 K and f = 2\u0015 sin 450 \u224810\u22124 s\u22121. Then, the speed of sound is c2 or cs \u2248320 m/s, the scale height is H = RT/g = 7.3 km = 7300 m, and the Brunt\u2013 Va\u00a8\u0131s\u00a8al\u00a8a frequency is N 2 = gd(ln \u03b80)/dz = g\u03ba/H for the isothermal atmosphere, or about 4 \u00d7 10\u22124 s\u22122. Note that the frequency associated with inertial oscillations is much lower than the frequency associated with gravitational oscillations."}
{"text": "f \u223c10\u22124 s\u22121 << N \u223c10\u22122 s\u22121 We \ufb01rst plot in Fig. 2.3.3 the FDR (2.3.22), with \u03b1 = \u03b2 = 1, i.e., without making either the hydrostatic or the quasi-Boussinesq approximations. Note that this equation contains four solutions for the frequency \u03bd, plus an additional solution \u03bd = 0, the geostrophic mode that satis\ufb01es nontrivially (2.3.19)."}
{"text": "2.4 Filtering approximations Horiz. wavenumber k (m-1) 2k 2; n 2=\u22121/4(N 2/g \u2212 g/ cs Internal gravity waves (n 2 > 0) resting atmosphere as a function of k, the horizontal wavenumber (the horizontal scale is its inverse), and the vertical wavenumber n. Shaded regions represent n2 < 0, When we neglect the time derivative of one of the equations of motion, we convert it from a prognostic equation into a diagnostic equation, and eliminate with it one type of solution. Physically, we eliminate a restoring force that supports a certain type of wave. We call this a \u201c\ufb01ltering approximation\u201d. Use of the quasi-geostrophic \ufb01ltering approximation that eliminates both sound and gravity waves made possible the successful forecast of Charney et al. (1950). Currently most global models and some regional models use the hydrostatic approximation, which \ufb01lters sound waves."}
{"text": "In this section we explore the effect of the \ufb01ltering approximations. Quasi-geostrophic approximation As we have already seen, without rotation, if we assume a steady state, the solution of (2.3.19) would be a trivial solution: all perturbations would be equal to zero."}
{"text": "However, with rotation, if we assume steady state solutions, and neglect all time derivatives \u03bd = 0, we obtain from the perturbed equations (2.3.17), the geostrophic 2 The continuous equations mode, a nontrivial solution:"}
{"text": "For the continuous perturbation equations (2.3.17), this means: (geostrophically balanced \ufb02ow) (hydrostatically balanced \ufb02ow) (horizontal, nondivergent \ufb02ow) (pressure perturbations are propor- tional to density perturbations multi- plied by the speed of sound squared, which is true whenever the hydrostatic This is the \u201cultimate\u201d \ufb01ltering approximation: it \ufb01lters out sound waves, inertia and For large horizontal scales we have to include the effects of varying rotation, and the f -plane becomes a \u03b2-plane: f = f0 + \u03b2y. When horizontal advection by the basic \ufb02ow is included, the stationary geostrophic \ufb02ow solution becomes quasi- stationary (slowly varying). The waves corresponding to the geostrophic mode are Rossby-type waves with a frequency small compared with the Coriolis or inertial frequency \u03bd \u2248Uk \u2212\u03b2/k \u223c10\u22125 \u221210\u22126 s\u22121. Rossby waves are quasi-geostrophic (\u03bd2 << f 2), hydrostatically balanced, and the \ufb02ow is quasi-horizontal (w\u2217/H << U \u2217/L), and therefore quasi-nondivergent (\u2207\u00b7 vh \u22480)."}
{"text": "Note that this type of quasi-geostrophic solution, fundamental for NWP, is still present in the general equations of motion, and survives as a solution when we make either the anelastic or the hydrostatic approximation in order to \ufb01lter out sound 2.4 Filtering approximations Quasi-Boussinesq or anelastic approximation (Ogura and Phillips, 1962) We now substitute \u03b2 = 0 in (2.3.19d). This means that we neglected the time derivative \u2202\u03c1\u2032/\u2202t compared with \u2207H \u00b7 v\u2217, \u2202w\u2217/\u2202z in the continuity equation. With this approximation, the equations become \u201canelastic\u201d, i.e., they do not allow the presence of sound waves, which require three-dimensional divergence and con- vergence for their propagation. Consider the terms that are neglected in the FDR sk2, i.e., the frequency of retained solutions is much smaller than that of sound waves, therefore this also \ufb01lters out the Lamb waves, i.e., horizontally propagating sound waves."}
{"text": "s. This approximation is justi\ufb01ed if In other words, the deep anelastic approximation is justi\ufb01ed for a model for which the potential temperature does not change too much within the depth \u03b3 RT/g \u223c10 km. This is a reasonable approximation for the standard troposphere (not for deep \ufb02ow into the stratosphere), since for the troposphere: \u0016\u03b80/\u03b80 \u223c30 K/300 K \u223c0.1."}
{"text": "For models that are so shallow that not only \u0016\u03b80/\u03b80 << 1, but also \u0016T0/T0 << 1, we can also neglect \u2202\u03c10/\u2202z in the continuity equation, and assume \u22073 \u00b7 v\u2032 = 0, not just \u22073 \u00b7 v\u2217= 0. In this case we treat the atmosphere as if it was an incompressible \ufb02uid. This approximation is only accurate for very shallow atmospheric models (less than 1 km depth), but is very appropriate for ocean models, since water is well approximated as an incompressible \ufb02uid."}
{"text": "Fig 2.4.1 schematically shows the FDR when we make the anelastic approxima- tion. From (2.3.22), and letting \u03b2 = 0 (with \u03b1 = 1), we can derive the frequency of inertia-gravity waves with the anelastic approximation:"}
{"text": "where p is like the inverse of a vertical wavelength From (2.4.3) we see that, for internal (n2 > 0) inertia-gravity waves, f 2 < \u03bd2 < N 2, the frequency \u03bd is between the Coriolis and Brunt\u2013Va\u00a8\u0131s\u00a8al\u00a8a frequencies. Note from Fig. 2.4.1 that for these waves, \u2202\u03bd2/\u2202k2 > 0, but \u2202\u03bd2/\u2202n2 < 0. This implies (since we can assume without loss of generality that k > 0) that the horizontal group velocity for gravity waves \u2202\u03bd/\u2202k has the same sign as the phase velocity (the energy of gravity 2 The continuous equations Internal gravity waves (n2 > 0) Horiz. wavenumber k (m-1) resting atmosphere when the quasi-Boussinesq or anelastic approximation is made waves moves in the same direction as the phase speed in the horizontal). In the vertical the opposite is true: if the group velocity is upwards, which happens for example when gravity waves are generated by mountain forcing, the phase velocity Because the anelastic equation \ufb01lters out acoustic internal waves (as well as the Lamb wave) it is widely used for problems in which the hydrostatic approximation cannot be made, as is the case for convection. For example, the ARPS model is based on deep anelastic equations. The FDR with the quasi-Boussinesq approximation is shown schematically in Fig. 2.4.1."}
{"text": "Hydrostatic approximation If we neglect the vertical acceleration \u2202w\u2217/\u2202t in the vertical momentum equation (2.3.17c), letting \u03b1 = 0 (with \u03b2 = 1), we get the FDR 2.4 Filtering approximations This FDR has two solutions: the horizontally propagating external sound wave (Lamb wave) solution, which unfortunately is retained:"}
{"text": "and inertia-gravity waves. From (2.4.4) we can derive the following relationship for inertia-gravity waves: using N 2 = \u03bag/H, H = RT/g, c2 Fig. 2.4.2 shows the relationship between frequency and horizontal and vertical wavenumbers with the hydrostatic equation."}
{"text": "Exercise 2.4.1: Derive (2.4.6) from (2.4.5). When are we justi\ufb01ed in using the hydrostatic equation? By taking \u03b1 = 0, we ne- glected the time derivative of the vertical velocity compared to \u03c1\u2032/\u03c10g. Note that it is not enough to \ufb01nd dw/dt << g to make the hydrostatic approximation: the vertical acceleration is small compared to gravity even for strong vertical motions, as in a Horiz. wavenumber k (m-1) Internal gravity waves (n 2 > 0) resting atmosphere when the hydrostatic approximation is made (\u03b1 = 0)."}
{"text": "2 The continuous equations cumulus cloud. The hydrostatic approximation requires that the vertical acceleration be small compared with the buoyancy (\u03c1\u2032/\u03c10)g, or gravitational acceleration within the \ufb02uid. It can be shown by scale analysis that the hydrostatic approximation is valid as long as we are dealing with shallow \ufb02ow (H/L << 1). For quasi-geostrophic \ufb02ow, the condition for hydrostatic balance is valid even if H/L \u223c1. This implies that the hydrostatic approximation is very accurate for models with grid sizes of the order of 100 km or larger, and still quite acceptable for quasi-geostrophic \ufb02ow, even when the horizontal grid size of the model approaches 10 km. However, the hydrostatic equation is not valid for models with grid sizes of the order of 10 km that attempt to resolve explicitly cumulus convection. Fig. 2.4.2 shows that for high frequencies \u03bd \u223cN or larger, or small horizontal scales the hydrostatic approximation distorts the original FDR (compare with Fig. 2.3.3)."}
{"text": "We now summarize in Table 2.4.1 the characteristics of the different types of waves and the approximations that can be used to \ufb01lter them out. For more details about Rossby waves and the \ufb01ltering of inertia gravity waves, see Section 2.5, where these topics are discussed in the context of the SWEs."}
{"text": "(1) In normal mode analysis of large-scale (hydrostatic) motion, or of atmospheric models, it is customary to \ufb01nd a horizontal structure equation and a vertical structure equation, associated by a separation constant h, where h is denoted as \u201cequivalent depth\u201d (e.g., Williamson and Temperton, 1981)."}
{"text": "The reason h is called the equivalent depth is that internal modes are governed by equations similar to the SWEs with depth h. However, h is not a constant but a function of vertical wavenumber, and therefore the analogy is 2.5 Shallow water equations (adapted from Zhang, personal communication, 1996)."}
{"text": "(2) With the hydrostatic approximation, the geopotential energy gz and the internal energy CvT of an air column are related to each other, zs \u03c1RT dz. Here the subscript s represents the earth\u2019s surface, and limz\u2192\u221epz = 0 is assumed. So, when zs = 0, the ratio of the potential to the internal energy of a column is equal to R/Cv = 0.4. When zs is not constant, the total potential energy (Lorenz, 1955) is given by Shallow water equations, quasi-geostrophic \ufb01ltering, and \ufb01ltering of inertia-gravity waves Consider now the SWEs (Fig. 2.5.1), valid for an incompressible hydrostatic motion of a \ufb02uid with a free surface h(x, y, t). \u201cShallow\u201d means that the vertical depth is much smaller than the typical horizontal depth, which justi\ufb01es the hydrostatic 2 The continuous equations \ufb02uid with a rigid bottom hs(x, y), a free surface h(x, y, t), and horizontal scales L much larger than the mean vertical scale H."}
{"text": "approximation. These equations are not only appropriate for representing a shallow mass of water (e.g., river \ufb02ow, storm surges), but they are prototypical of the primitive equations based on the hydrostatic approximation and are frequently used to test numerical schemes. The shallow water horizontal momentum equations are The continuity equation is which can also be written as Here \u03c6s = ghs(x, y) and hs is the bottom topography."}
{"text": "Exercise 2.5.1: Derive the SWE from the primitive equations assuming hydrostatic, incompressible motion, and that the horizontal velocity is uniform in height. Is the vertical velocity uniform in height as well?"}
{"text": "We now derive the equation of conservation of potential vorticity: expanding the total derivative of the momentum equation and making use of the relationship where \u03c2 = k \u00b7 \u2207\u00d7 vH we obtain or (since d f/dt = v \u00b7 \u2207f ) which indicates that the absolute vorticity ( f + \u03c2) of a parcel of \u201cwater\u201d increases with its convergence (or vertical stretching)."}
{"text": "Exercise 2.5.2: Give a physical interpretation of the equation of conservation of The conservation of potential vorticity is an extremely powerful dynamical con- straint. In a multilevel primitive equation model, the isentropic potential vorticity (the absolute vorticity divided by the distance between two surfaces of constant po- tential temperature) is also individually conserved. If the initial potential vorticity distribution is accurately represented in a numerical model, and the model is able to transport potential vorticity accurately, then the forecast will also be accurate."}
{"text": "(note that (2.5.7) and (2.5.8) are the same equations as in Section 2.4.5 on horizontal sound (Lamb) waves, with gH = c2 Assume solutions of the form (u\u2032, v\u2032, \u03c6\u2032)ei(kx\u2212\u03bdt). Then the FDR is with three solutions for \u03bd:"}
{"text": "the frequency of inertia-gravity waves, analogous to the inertia-Lamb wave, and \u03bd = 0, the geostrophic mode. As before, this is a geostrophic, nondivergent steady Following Arakawa (1997), we can now compare the FDR of inertia-gravity waves in the SWE with the FDR of a three-dimensional isothermal system using the hydrostatic approximation (2.4.4)\u2013(2.4.6). We see that (2.5.10) is analogous to internal inertia-gravity waves for an isothermal hydrostatic atmosphere (2.3.26) if 2 The continuous equations we de\ufb01ne an equivalent depth such that \u0007 = gheq:"}
{"text": "and is analogous to the (external) inertia Lamb waves if we de\ufb01ne the equivalent Quasi-geostrophic scaling for the SWE If we want to \ufb01lter the inertia-gravity waves, as Charney did in the \ufb01rst successful numerical weather forecasting experiment (Chapter 1), we can develop a quasi- geostrophic version of the SWE. We can do it \ufb01rst for an f -plane ( f = f0)."}
{"text": "Assumethattheatmosphereisinquasi-geostrophicbalance:v = vg + vag = vg + \u03b5v\u2032 where we assume that the typical size of the ageostrophic wind is much smaller (order \u03b5 = U/f L, the Rossby number) than the geostrophic wind \u03b5v\u2032 << vg, and that the same is true for their time derivatives \u03b5\u2202v\u2032/\u2202t << \u2202vg/\u2202t. The geostrophic Plugging these into the perturbation equations (2.5.7) and (2.5.8) we obtain In this equation, the dominant terms (pressure gradient and Coriolis force on the geostrophic \ufb02ow) cancel each other (geostrophic balance), so that the smaller effect of the Coriolis force acting on the ageostrophic \ufb02ow is left to balance the time derivative. From (2.5.8), Here the geostrophic wind is nondivergent, so that the time derivative of the pressure is given by the divergence of the smaller ageostrophic wind."}
{"text": "From (2.5.14) and (2.5.15) we can conclude that \u2202vg/\u2202t and \u2202\u03c6/\u2202t are of order \u03b5, i.e., the geostrophic \ufb02ow changes slowly (it is almost stationary compared with other types of motion), and that \u2202vag/\u2202t = \u03b5\u2202v\u2032/\u2202t, which is smaller than \u2202vg/\u2202t, is of order \u03b52. With quasi-geostrophic scaling we neglect terms of O(\u03b52) and we obtain the linearized quasi-geostrophic SWE:"}
{"text": "2.5 Shallow water equations Note that in (2.5.16) there is only one independent time derivative because of the geostrophic relationship (we lost the other two time derivatives when we neglected the term \u2202vag/\u2202t). Physically, this means that we only allow divergent motion to exist as required to maintain the quasi-geostrophic balance, and eliminate the degrees of freedom necessary for the propagation of gravity waves."}
{"text": "where the last term in (2.5.18) appears if we are on a \u03b2-plane: f = f0 + \u03b2y. Then we can eliminate the (ageostrophic) divergence between (2.5.18) and (2.5.17c) and obtain the linear quasi-geostrophic potential vorticity equation on a \u03b2-plane:"}
{"text": "Note that there is a single independent variable (\u03c6) so that there is a single solution for the frequency. If we neglect the \u03b2-term (i.e., assume an f -plane) and allow for plane-wave-type solutions \u03c6 = Fei(kx\u2212\u03bdt), the only solution of the FDR in (2.5.20) is \u03bd = 0, the geostrophic mode. This con\ufb01rms that by eliminating the time derivative of the ageostrophic (divergent) wind vag, we have eliminated the inertia-gravity wave solution. If we assume a \u03b2-plane, i.e., keep the \u03b2 term in (2.5.20), the quasi- 2 The continuous equations The Rossby waves are the essential \u201cweather waves\u201d, and as shown in Table 2.4.1, have rather large amplitudes (up to 50 hPa). The ageostrophic \ufb02ow associated with these waves is responsible for the upward motion that produces precipitation ahead In a multilevel model, the FDR (2.5.21) can be used with the equivalent depths (2.5.11), (2.5.12) applied to the baroclinic (internal) and barotropic Rossby waves, respectively. With these de\ufb01nitions, we can say that the waves in the atmosphere are analogous to the SWE waves. However, because heq appears as a separation constant in the de\ufb01nition of the normal modes of the atmosphere, the equivalent depth depends on the vertical wavenumber, and on the type of wave considered (Lamb or inertia- Exercise 2.5.3: Show that the quasi-geostrophic PVE (potential vorticity equation) using similar scaling arguments."}
{"text": "Exercise 2.5.4: Allow for a basic \ufb02ow ug(total) = U + ug; vg(total) = vg, in (2.5.22). Exercise 2.5.5: Estimate the initial time derivative for typical values of the horizontal wavenumber, the external (barotropic) vertical mode for Rossby waves and inertia- gravity waves that Richardson would have observed."}
{"text": "Exercise 2.5.6: Derive the formula for group velocity in the x-direction for Rossby Exercise 2.5.7: Using typical values of long and short synoptic waves (e.g., horizon- tal wavelengths of 8000 km and 2000 km respectively), calculate the phase speed and the group velocity of Rossby waves for the barotropic mode and the \ufb01rst baroclinic mode (H \u223c10 km and 1 km respectively)."}
{"text": "Inertia-gravity waves in the presence of a basic \ufb02ow As we just saw, the SWEs are a simple version of the primitive equations, and are widely used to understand numerical and dynamical processes in primitive equations."}
{"text": "As we noted in Chapter 1, \ufb01ltered quasi-geostrophic models have been substituted by primitive equation models for NWP, because the quasi-geostrophic \ufb01ltering is not an accurate approximation (it assumes that the Rossby number U/fL is much smaller than 1). Recall that quasi-geostrophic \ufb01ltering was introduced by Charney et al."}
{"text": "2.5 Shallow water equations (1950) in order to eliminate the problem of gravity waves (which requires a small time step) whose high frequencies produced a huge time derivative in Richardson\u2019s computation, masking the time derivative of the actual weather signal."}
{"text": "An alternative way to deal with the presence of fast gravity waves without resorting to quasi-geostrophic \ufb01ltering is the use of semi-implicit time schemes (to be discussed in Chapter 3). Consider small perturbations in the SWE including a basic \ufb02ow U in the x-direction. Then the total linearized time derivative becomes In that case, when we assume solutions of the form Aei(kx\u2212\u03bdt), d/dt = i(\u2212\u03bd + kU)."}
{"text": "Therefore the FDR remains the same except that \u03bd is replaced by \u03bd \u2212kU. The FDR for small perturbations in the SWE with a basic \ufb02ow U is therefore As noted before, this has three solutions, quasi-geostrophic \ufb02ow (which is steady state, except for the uniform translation with speed U) and two solutions for the inertia-gravity waves, modi\ufb01ed by the basic \ufb02ow translation:"}
{"text": "(\u03bdG \u2212kU) = 0 (geostrophic mode) [(\u03bdIGW \u2212kU)2 \u2212f 2 \u2212\u0007k2] = 0 (inertia gravity waves, modi\ufb01ed The phase speed of the inertia-gravity wave is given by Finally, we note that for the Lamb wave (as well as for the external gravity wave), the phase speed of the inertia-gravity wave is dominated by the term \u0007 \u2248\u221ag \u00d7 10 km \u2248300 m/s. As we will see in Section 3.2.5, it is possible to avoid using costly small time steps by means of a semi-implicit time scheme. An implicit time scheme has no constraint on the time step. Therefore, in a semi-implicit scheme, the terms that give rise to the fast gravity waves, namely the horizontal di- vergence and the horizontal pressure gradient are written implicitly, while the rest of the SWE terms can be written explicitly. The terms generating the gravity wave are underlined in the following nonlinear SWE:"}
{"text": "2 The continuous equations Primitive equations and vertical coordinates As Charney (1951) foresaw, most NWP modelers went back to using the primitive equations, with the hydrostatic approximation, but without quasi-geostrophic \ufb01lter- ing. Quasi-geostrophic models are now reserved for simple problems where the main motivation is the understanding of atmospheric or ocean dynamics."}
{"text": "Exercise 2.6.1: Give two or more reasons why using the primitive equations, with the hydrostatic approximation but without quasi-geostrophic \ufb01ltering was a desirable So far we have used z as the vertical coordinate. When we make the hydrostatic approximation, as in the primitive equations, the use of pressure vertical coordinates becomes very advantageous. We can also use any arbitrary variable \u03b6(x, y, z, t) as the vertical coordinate as long as it is a monotonic function of z (Kasahara, 1974)."}
{"text": "The most commonly used vertical coordinates are height z, pressure p, a normalized pressure \u03c3 (Phillips, 1957), potential temperature \u03b8 (Eliassen, 1949), and several kinds of hybrid coordinates (e.g., Simmons and Burridge, 1981, Johnson et al., 1993, Purser, pers. comm., Bleck and Benjamin 1993)."}
{"text": "General vertical coordinates When we transform the vertical coordinate, a variable A(x, y, z, t) becomes A(x, y, \u03b6(x, y, z, t), t). The horizontal coordinates and time remain the same. Let s represent x, y, or t. Then, from Fig. 2.6.1(a) Substituting (2.6.2) in (2.6.1), we get 2.6 Primitive equations and vertical coordinates constant \u03b6 and at constant Z. The points B and D represent values of A on a \u03b6-surface and B and C those on a constant z surface. (b) Schematic of a parcel of air in a hydrostatic system, where \u0016p is proportional to the change in mass per unit area \u0016M."}
{"text": "and for the horizontal divergence of a vector B: The total derivative of A(x, y, \u03b6, t) is given by 2 The continuous equations The horizontal pressure gradient is therefore which becomes, using the hydrostatic equation \u2202p/\u2202\u03c6 = \u2212\u03c1, In summary the horizontal momentum equations become and the hydrostatic equation \u2202p/\u2202z = \u2212\u03c1g becomes The continuity equation can be derived from the conservation of mass for an in\ufb01nites- imal parcel: the hydrostatic equation indicates that the mass of a parcel is proportional to the increase in pressure from the top to the bottom of the parcel (Fig. 2.6.1(b)):"}
{"text": "Now, \u0016p = (\u2202p/\u2202\u03c2)\u0016\u03c2, so that taking a logarithmic total derivative, and noting and the same with the other space variables, we obtain The thermodynamic equation is as before The kinematic lower boundary condition is that the surface of the earth is a material surface: the \ufb02ow can only be parallel to it, not normal. This means that once a parcel touches the surface it is \u201cstuck\u201d to it. This can be expressed as 2.6 Primitive equations and vertical coordinates This kinematic boundary condition is well de\ufb01ned although in practice it may not be accurate, for example, when there is subgrid-scale orography."}
{"text": "Atthetop,unfortunately,theboundaryconditionisnotsowellde\ufb01ned:As z \u2192\u221e, p \u21920, but in general there is no satisfactory way to express this condition for a \ufb01nite vertical resolution model. Most models assume a simple condition of a \u201crigid top\u201d (i.e., making the top surface a material surface) but this is an arti\ufb01cial boundary condition that introduces spurious effects. For ex- ample, Kalnay and Toth (1996) showed that a rigid top introduces arti\ufb01cial \u201cupside- down\u201d baroclinic instabilities in the NCEP global model, and similar observations were made by Hartmann et al. (1997) with the ECMWF model. If the top of the model is suf\ufb01ciently high, and there is enough vertical resolution, the upward mov- ing perturbations are damped in the model (as they are in nature), and the spurious interaction with the arti\ufb01cial top may remain small. Alternatively, radiation condi- tions enforcing the condition that energy can only propagate upwards can be used, but they are not simple to implement."}
{"text": "These coordinates are a natural choice for a hydrostatic atmosphere (Eliassen, 1949). They greatly simplify the equations of motion: the horizontal pressure gra- dient becomes irrotational, and the continuity equation becomes simply zero three- dimensional divergence, a diagnostic linear equation."}
{"text": "As a result the geostrophic wind relationship is also simpler: vg = (1/f )k \u00d7 \u2207\u03c6. For this reason, rawinsonde measurements have been made in pressure coordinates In pressure coordinates, \u2202p/\u2202\u03c2 \u22611, the total derivative operator (2.6.6) is given where the vertical velocity in pressure coordinates is \u03c9 = dp/dt. The primitive 2 The continuous equations and the thermodynamic equation (2.6.13) is unchanged."}
{"text": "The geostrophic and thermal wind relationships are especially simple in pressure On the other hand, the bottom boundary condition is not simple in pressure coordi- nates because the pressure surfaces intersect the surface:"}
{"text": "This requires knowing the rate of change of ps: This complication of the surface boundary condition in pressure coordinates led Phillips (1957) to the invention of sigma coordinates (next subsection)."}
{"text": "Instead of the horizontal momentum equations, we can use the prognostic equa- tions for the vorticity \u03b6 and divergence \u03b4, obtained by applying the operators k \u00b7 \u2207x and \u2207\u00b7 to the momentum equations. In pressure coordinates these equations are Sigma and eta coordinates Because of the complication of the bottom boundary conditions, Phillips (1957) introduced \u201cnormalized pressure\u201d or \u201csigma\u201d coordinates, where \u03c3 = p/ps and ps(x, y, t) is the surface pressure. These are by far the most widely used vertical coordinates. At the surface, \u03c3 = 1, and at p = 0, \u03c3 = 0, so that the top and bottom boundary conditions are \u02d9\u03c3 = 0. More generally, allowing for a rigid top at a \ufb01nite The continuity equation is 2.6 Primitive equations and vertical coordinates The surface pressure tendency equation is:"}
{"text": "where the \ufb01rst term, if sigma surfaces are steep, may not have the information that went into the \ufb01nite difference calculation of the second. To avoid the resulting errors, Mesinger (1984) introduced a step-mountain coordinate denoted \u201ceta\u201d (used in the Eta model at NCEP, e.g., Mesinger et al. (1988), Janjic (1990), Black (1994)):"}
{"text": "The \ufb01rst factor is the standard sigma coordinate, the second is a scaling factor, with po(z) the pressure in the standard atmosphere. Mountains are de\ufb01ned as boxes, whose tops have to coincide with a model eta level (Fig. 2.6.2). As a result of the scaling, the eta surfaces are almost horizontal, and the pressure gradient is computed accurately."}
{"text": "At NCEP, the Eta model has proven to be very skillful especially in predicting storms. 2 The continuous equations The fact that under adiabatic motion, potential temperature is individually conserved suggested long ago that it could be used as a vertical coordinate. The main advantage, whichmakesitanalmostidealcoordinate,isthat\u201cvertical\u201dmotion \u02d9\u03b8 isapproximately zero in these coordinates (except for diabatic heating). This reduces \ufb01nite difference errors in areas such as fronts, where pressure or z-coordinates tend to have large errors associated with poorly resolved vertical motion."}
{"text": "Hydrostatic equation: from the de\ufb01nition of potential temperature, and using the hydrostatic and state equations, we get If we de\ufb01ne the Exner function \u001f = C pT/\u03b8 = C p(p/p0)R/C p, and the Montgomery potential M = C pT + \u03c6, we see from the previous equation that The horizontal pressure gradient becomes very simple, so that for \u03b6 = \u03b8 the mo- The continuity equation is The potential vorticity is conserved for adiabatic, frictionless \ufb02ow (Ertel\u2019s theo- rem). This general property can be posed in its simplest formulation in isentropic where q = ( f + k \u00b7 \u2207\u03b8 \u00d7 v) \u2202\u03b8/\u2202p, and integrating between two isentropic sur- faces, the potential vorticity is which is similar to the SWE potential vorticity."}
{"text": "Although the isentropic coordinates have many advantages, they have also two main disadvantages: The \ufb01rst is that isentropic surfaces intersect the ground (as do other vertical coordinates except for sigma-type coordinates). In practice this implies that it is dif\ufb01cult to enforce strict conservation of mass, and this is important for long (climate)integrations.Forthisreason,hybridsigma\u2013thetacoordinateshavebeenused 2.6 Primitive equations and vertical coordinates (e.g., Johnson et al., 1993). Other approaches have been those of Bleck and Benjamin (1993) for the operational RUC/MAPS model, and that of Arakawa and Konor (1996)."}
{"text": "The second disadvantage is that only statically stable solutions are allowed, since the vertical coordinate has to vary monotonically with height. There are situations, e.g., over hot surfaces, where this is not true even at a grid scale. Moreover, in regions of low static stability, the vertical resolution of isentropic coordinates can be poor."}
{"text": "Exercise 2.6.3: Derive (2.6.31) from (2.6.9) for \u03b6 = \u03b8, and (2.6.32) from the log- where \u0016M is proportional to the mass of a parcel in isentropic coordinates Numerical discretization of the equations Classi\ufb01cation of partial differential equations (PDEs) Second order linear partial differential equations are classi\ufb01ed into three types de- pending on the sign of \u03b22 \u2212\u03b1\u03b3 (e.g., Courant and Hilbert, 1962). Equations are hyperbolic, parabolic or elliptic if the sign is positive, zero, or negative, respectively."}
{"text": "Laplace\u2019s or Poisson\u2019s equations (elliptic). Examples: steady state temperature of a plate, streamfunction/vorticity The behavior of the solutions, the proper initial and/or boundary conditions, and the numerical methods that can be used to \ufb01nd the solutions depend essentially on the type 3.1 Classi\ufb01cation of partial differential equations (PDEs) of PDE that we are dealing with. Although nonlinear multidimensional PDEs cannot in general be reduced to these canonical forms, we need to study these prototypes of the PDEs to develop an understanding of their properties, and then apply similar methods to the more complicated NWP equations."}
{"text": "Advection equation, with solution u(x, t) = u(x \u2212ct, 0). Theadvectionequationisa\ufb01rstorderPDE,butitcanalsobeclassi\ufb01edasahyperbolic, since its solutions satisfy the wave equation (a), and the latter is usually written as or an equivalent transformation Well-posedness, initial and boundary conditions A well-posed initial/boundary condition problem has a unique solution that depends continuously on the initial/boundary conditions. Clearly, the speci\ufb01cation of proper initial conditions and boundary conditions for a PDE is essential in order to have a well-posed problem. If too many initial/boundary conditions are speci\ufb01ed, there will be no solution. If too few are speci\ufb01ed, the solution will not be unique. If the number of initial/boundary conditions is right, but they are speci\ufb01ed at the wrong place or time, the solution will be unique, but it will not depend smoothly on initial/boundary conditions, i.e., small errors in the initial/boundary conditions will produce huge errors in the solution. In any of these cases we have an ill-posed problem. And we can never \ufb01nd a numerical solution of a problem that is ill posed: the computer will show its disgust by \u201cblowing up\u201d."}
{"text": "We brie\ufb02y discuss well-posed initial/boundary conditions: Second order elliptic equations require one boundary condition on each point of the spatial boundary. These are \u201cboundary value\u201d, time-independent problems, and the methods used to solve them are introduced in Section 3.4."}
{"text": "The boundary conditions may be on the value of the function (Dirichlet problem), as when we specify the temperature in the borders of a plate, or on 3 Numerical discretization of the equations of motion its normal derivative (Neumann problem), as when we specify the heat \ufb02ux."}
{"text": "Linear hyperbolic equations require as many initial conditions as the number of characteristics that come out of every point in the surface t = 0, and as many boundary conditions as the number of characteristics that cross a point in the (space) boundary pointing inwards (into the spatial domain). For example: to solve \u2202u/\u2202t = \u2212c\u2202u/\u2202x for x > 0, t > 0; characteristics:"}
{"text": "solutions of dx/dt = c; space boundary: x = 0 (see Fig. 3.1.1(a),(b)). If c > 0, we need the initial condition u(x, 0) = f (x) and the boundary condition u(0, t) = g(t). If c < 0, we need the initial condition u(x, 0) = f (x) but no boundary conditions."}
{"text": "For nonlinear equations, no general statements can be made, but physical insight and local linearization can help to determine proper initial/boundary conditions. For example, in the nonlinear advection equation \u2202u/\u2202t = \u2212u\u2202u/\u2202x, the characteristics are dx/dt = u, and since we don\u2019t know a priori the sign of u at the boundary, and whether the characteristics will point inwards or outwards, we have to estimate the value of u from the nearby solution, and de\ufb01ne the boundary condition accordingly."}
{"text": "the characteristics of the (a) positive and (b) negative initial/boundary conditions 3.1 Classi\ufb01cation of partial differential equations (PDEs) One method of solving simple PDEs is the method of separation of variables, but unfortunately in most cases it is not possible to use it (hence the need for numerical models!). Nevertheless, it is useful to try to solve some simple PDE\u2019s analytically."}
{"text": "Exercise 3.1.1: Solve by the method of separation of variables these prototype PDEs: Boundary condition: u(x, 0) = f (x), u(x, 1) = u(0, y) = u(1, y) = 0 Boundary condition: u(0, t) = u(1, t) = 0 Initial condition: u(x, 0) = f (x) = \u0017\u221e Note that the higher the wavenumber, the faster it goes to zero, i.e., the solution is smoothed as time goes on."}
{"text": "Boundary condition: u(0, t) = u(1, t) = 0 Initial condition: u(x, 0) = f (x) = (4) Same as (3), but now, instead of two initial conditions, we give an initial and a Boundary condition: u(0, t) = u(1, t) = 0 Initial condition: u(x, 0) = f (x); \u201c\ufb01nal condition\u201d u(x, 1) = g(x) In other words, we try to solve a hyperbolic (wave) equation as if it was a boundary value problem. Show that the solution is unique but it does not depend continuously on the boundary conditions, and therefore it is not a well-posed problem."}
{"text": "3 Numerical discretization of the equations of motion Conclusion: Before trying to solve a problem numerically, make sure that it is well posed: it has a unique solution that depends continuously on the data that de\ufb01ne Exercise 3.1.2: Lorenz showed that the atmosphere has a \ufb01nite limit of predictability:"}
{"text": "even if the models and the observations were perfect, \u201cthe \ufb02apping of a butter\ufb02y in Brazil (not taken into account in the model) will result in a completely different forecast over the US after a couple of weeks\u201d. Does this mean that the problem of Initial value problems: numerical solution Hyperbolic and parabolic PDEs are initial value or marching problems: The solution is obtained by using the known initial values and marching or advancing in time."}
{"text": "We take discrete values for x and t: x j = j\u0016x, tn = n\u0016t. The solution of the \ufb01- nite difference equation is also de\ufb01ned at the discrete points ( j\u0016x, n\u0016t): U n U( j\u0016x, n\u0016t). We will use a small u to denote the solution of the PDE (continuous) and capital U to denote the solution of the \ufb01nite difference equation (FDE), a discrete Consider again the advection equation (3.2.1). Suppose that we choose to approx- imate this PDE with the following FDE (called an \u201cupstream scheme\u201d):"}
{"text": "3.2 Initial value problems: numerical solution Truncation errors and consistency We say that the FDE is consistent with the PDE if, in the limit \u0016x \u21920, \u0016t \u21920 the FDE coincides with the PDE. Obviously, this is a \ufb01rst requirement that the FDE should ful\ufb01ll if its solutions are going to be good approximations of the solutions of the PDE. The difference between the PDE and the FDE is the discretization error or local (in space and time) truncation error. Consistency is rather simple to verify: Substitute U by u in the FDE, and evaluate all terms using a Taylor series expansion centered on the point ( j, n), and then subtract the PDE from the FDE. If the difference (or local truncation error \u03c4) goes to zero as \u0016x \u21920, \u0016t \u21920, then the FDE is consistent with the PDE."}
{"text": "Example 3.2.1: We verify the consistency of (3.2.3) with (3.2.1) by a Taylor series Substitute the series (3.2.4) in the FDE (3.2.3) and when we subtract the PDE (3.2.1) we get the (local) truncation error 2 + higher order terms = 0(\u0016t) + 0(\u0016x) so that lim\u0016t\u21920,\u0016x\u21920 \u03c4 \u21920. Therefore the FDE is consistent. Note that both the time and the space truncation errors are of \ufb01rst order, because the \ufb01nite differences are uncentered in both space and time. Truncation errors for centered differences are second order, and therefore centered differences are more accurate than uncentered differences (see Fig. 3.2.1(a) and the leapfrog scheme, based on centered differences in space and in time, later in this section)."}
{"text": "Convergence and stability criteria The second question posed in Section 3.2.1 was whether the solution of the FDE converges to the PDE solution, i.e., whether U( j\u0016x, n\u0016t) \u2192u(x, t) when j\u0016x \u2192x, n\u0016t \u2192t, \u0016x \u21920, \u0016t \u21920. This is of evident practical importance, but can only be answered after considering another problem, that of computational sta- bility. Consider again the PDE (3.2.1), which has the solution u(x, t) = u(x \u2212ct, 0), shown schematically in Fig. 3.2.1(b) (the initial shape of u translates with velocity c)."}
{"text": "3 Numerical discretization of the equations of motion differences estimating the with \u2202u/\u2202t since they all truncation errors are second order. (b) Schematic of the The FDE (3.2.3) can be written as where \u00b5 = c\u0016t/\u0016x is the Courant number. Assume that 0 \u2264\u00b5 = c\u0016t/\u0016x \u22641, as in Fig. 3.2.2(a). Then the FDE solution at the new time level U n+1 j\u22121. In this case the advection scheme works the way it should, because we know the true solution is in between those values. However, if this condition is not satis\ufb01ed, and \u00b5 = c\u0016t/\u0016x > 1 (as in Fig. 3.2.2(b)) or \u00b5 = c\u0016t/\u0016x < 0 (as in Fig. 3.2.2(c)), then the value of Un+1 j\u22121. The problem with extrapolation is that the maximum absolute value of the solution U n j increases with each time step. Taking absolute values of (3.2.7) and letting U n = max j |U n 3.2 Initial value problems: numerical solution interpolation of the solution at time level n + 1 (case (a)), or to extrapolation (cases (b) and (c)) depending on Then U n+1 \u2264U n if and only if 0 \u2264\u00b5 \u22641."}
{"text": "If the condition 0 \u2264\u00b5 \u22641 is not satis\ufb01ed, then the solution is not bounded and it grows with n. If we let \u0016t, \u0016x \u21920 with \u00b5 = const., it only makes things worse, because then n \u2192\u221e. In practice, if the condition 0 \u2264\u00b5 \u22641 is not satis\ufb01ed, the FDE \u201cblows up\u201d in a few time steps, faster for nonlinear problems. We de\ufb01ne now computational stability: we say that an FDE is computationally stable if the solution of the FDE at a \ufb01xed time t = n\u0016t remains bounded as \u0016t \u21920. The condition on the Courant number of being less than 1 in absolute value is usually known as the Courant\u2013Friedrichs\u2013Lewy or CFL condition."}
{"text": "We can now state the fundamental Lax\u2013Richtmyer theorem: Given a properly posed linear initial value problem, and a \ufb01nite difference scheme that satis\ufb01es the consistency condition, then the stability of the FDE is the necessary and suf\ufb01cient condition for convergence."}
{"text": "3 Numerical discretization of the equations of motion The theorem is useful because it allows us to establish convergence by examining separately the easier questions of consistency and stability. We are interested in convergence not because we want to let \u0016t, \u0016x \u21920, but because we want to make sure that if \u0016t, \u0016x are small, then the errors u( j\u0016x, n\u0016t) \u2212U n global truncation errors at a \ufb01nite time) are acceptably small."}
{"text": "To determine the necessary condition for stability of the FDE (3.2.3) we used the \u201ccriterion of the maximum\u201d method. We can also use it to study the stability condition of the following FDE, which approximates the parabolic diffusion equation The veri\ufb01cation of consistency is immediate. Note that, because the differences are centered in space but forward in time, the truncation error is \ufb01rst order in space and second order in time O(\u0016t) + O(\u0016x)2."}
{"text": "where \u00b5 = \u03c3\u0016t/\u0016x2. If we take absolute values, and let U n = max j |U n So we obtain a condition 0 \u2264\u00b5 \u22641/2 to insure that the solution remains bounded as n \u2192\u221e, i.e., as the necessary condition for stability of the FDE."}
{"text": "Exercise 3.2.1: The condition on the wave equation 0 \u2264\u00b5 \u22641 for the upstream FDE is interpreted as \u201cthe time step should be chosen so that a signal cannot travel more than one grid size in one time step.\u201d Give a physical interpretation of the stability condition and the equivalent \u201cCourant number\u201d \u00b5 = \u03c3 (\u0016t/\u0016x2) \u22641/2 for Unfortunately, the criterion of the maximum, which is intuitively very clear, can only be applied in very few cases. In most FDEs some coef\ufb01cients of the equations analogous to (3.2.9) are negative, and the criterion cannot be applied."}
{"text": "Another stability criterion that has much wider application is the von Neu- mann stability criterion: Assume that the boundary conditions allow expansion of the solution of the FDE in an appropriate set of eigenfunctions. For sim- plicity we will assume an expansion into Fourier series (e.g., periodic boundary 3.2 Initial value problems: numerical solution The space variable, x, and the wavenumber k can be multidimensional, e.g., x = (x1, x2, x3), k = (k1, k2, k3). The dependent variable U can also be a vector for a Let x j = j\u0016x (or x j = ( j1\u0016x1, j2\u0016x2, j3\u0016x3)). We de\ufb01ne p as the wavenumber for the \ufb01nite Fourier series: p = k\u0016x or p = (k1\u0016x1, k2\u0016x2, k3\u0016x3). Let tn = n\u0016t."}
{"text": "When we substitute this Fourier expansion into a linear FDE, we obtain a system G is an \u201campli\ufb01cation\u201d matrix that, when applied to the pth Fourier component of the solution at time n\u0016t \u201cadvances\u201d it to the time (n + 1)\u0016t; G depends on p, \u0016t and \u0016x. If we know the initial conditions then the solution of the FDE in (3.2.11) is Therefore, stability, i.e., boundedness of the solution for any permissible initial con- dition at any \ufb01xed time, is guaranteed if the matrix Gn is bounded for all p when \u0016t \u21920 and n \u2192\u221e. So, we must have ||Gn|| < M for all p, as n \u2192\u221e. Here ||A|| is a norm or measure of the \u201csize\u201d of a matrix A. If \u03c3(G) is the spectral radius of G, i.e., \u03c3(G) = maxi |\u03bbi|, where \u03bbi are the eigenvalues of G, then it can be shown that The equal sign is valid if G is normal, i.e., if GG\u22c6= G\u22c6G, where G\u22c6is the transpose- conjugate of G, but in general the ampli\ufb01cation matrices arising from FDEs are not Thus a necessary condition for stability of an FDE, and therefore a necessary condition for convergence, is that \u0016t\u21920,n\u0016t\u2192t[\u03c3(G)]n = \ufb01nite = econst."}
{"text": "The term O(\u0016t) allows bounded growth with time if this growth is \u201clegitimate\u201d, i.e., if it arises from a physical instability present in the PDE. If the exact solution grows with time, then the FDE cannot both satisfy \u03c3(G) \u22641 and be consistent with Suf\ufb01cient conditions are very complicated, and are known only for special cases."}
{"text": "In principle this method can also be used to study the stability of the boundary conditions, if they are appropriately included in the ampli\ufb01cation matrix. In practice this is complicated, and computational stability of the boundary conditions is usually obtained by ensuring well-posedness, and testing the stability experimentally. For simple equations, and without considering the effect of boundary conditions, the von Neumann criterion can be simpli\ufb01ed by assuming solutions with an ampli\ufb01cation factor \u03c1 rather than a matrix. The solution for the ampli\ufb01cation factor \u03c1 then coincides with the eigenvalues of the ampli\ufb01cation matrix, and the von Neumann stability criterion is \u03c1 \u22641 + O(\u0016t)."}
{"text": "We substitute in (3.2.17) and eliminate Aeipj and obtain The ampli\ufb01cation factor \u03c1 is the same as the 1 \u00d7 1 ampli\ufb01cation matrix G, and therefore the same as its spectral radius \u03c3(G), and the stability condition is |\u03c1| \u22641 for all wavenumbers p. We need to estimate the maximum value of the spectral radius (or ampli\ufb01cation factor in this case):"}
{"text": "\u03c1 = 1 \u2212\u00b5(1 \u2212e\u2212ip) = 1 \u2212\u00b5(1 \u2212cos p + i sin p) |\u03c1|2 = (1 \u2212\u00b5(1 \u2212cos p))2 + \u00b52 sin2 p 3.2 Initial value problems: numerical solution We make use of the trigonometrical relationships |\u03c1|2 = 1 \u22124\u00b5(1 \u2212\u00b5) sin2 p Now consider the sin2 p/2 term: The shortest wave that can be present in the \ufb01- nite difference solution is L = 2\u0016x, therefore the maximum value that p = k\u0016x = 2\u03c0\u0016x/L can take is p = \u03c0, and the maximum value of sin2p/2 is therefore 1. The other factor, \u00b5(1 \u2212\u00b5), is a parabola whose maximum value is 0.25 when \u00b5 = 0.5."}
{"text": "So the ampli\ufb01cation factor squared will remain less than or equal to 1 as long as 0 \u2264\u00b5 \u22641. This coincides with the condition we obtain from the criterion of the maximum (and also with the notion that we should not extrapolate but interpolate the new values at time level t = (n + 1)\u0016t, cf. Fig. 3.2.2)."}
{"text": "The upstream scheme decreases the amplitude of all Fourier wave components of the solution, since, if 0 < \u00b5 < 1, \u03c1 < 1. This is therefore a very dissipative FDE: it has strong \u201cnumerical diffusion.\u201d Fig. 3.2.3 shows the decrease in amplitude when using the upstream scheme after one time step and after 100 time steps for each either the \u201cupstream\u201d FDE, and the Matsuno or Euler-backward schemes with \u00b5 = 0.1; L is the wavelength in units of \u0016x\u03c1E B = [1 \u2212\u00b52 sin2 p + \u00b54 sin4 p]1/2 and \u03c1Upxy = [1 \u22124\u00b5(1 \u2212\u00b5) sin2 p]1/2."}
{"text": "3 Numerical discretization of the equations of motion wavenumber p using a Courant number \u00b5 = 0.1, a typical value for advection given the presence of fast gravity waves. Since its truncation errors are large (of \ufb01rst order), theupstreamschemeisingeneralnotrecommendedexceptforspecialsituations(e.g., for out\ufb02ow boundary conditions, or when modi\ufb01ed in such a way that the dissipation rate becomes lower). An alternative, less damping scheme known as the Matsuno or Euler-backward scheme, frequently used in combination with the leapfrog scheme is also shown. Note that a \u201cdownstream\u201d scheme (Fig. 3.2.2(c)) is unstable."}
{"text": "Since the last term in the quadratic equation (3.2.24) is \u20131, and this is the product of the roots, the term inside the root (\u2212\u00b52sin2 p + 1) must be real, since otherwise the roots would be purely imaginary, and one of them would be larger than 1, which violates the stability criterion. In order for (\u2212\u00b52sin2 p + 1) to be real for all p, we must have \u00b52 \u22641. The stability condition for the leapfrog scheme therefore becomes Exercise 3.2.3: Draw a schematic like Fig. 3.2.2, and explain why the sign of the Courant number does not matter for its stability criterion, unlike the sign of the upstream scheme. Why did the term O(\u0016t) not appear in the stability condition?"}
{"text": "3.2 Initial value problems: numerical solution We can actually \ufb01nd the exact solution of the leapfrog FDE (3.2.22), as well as of the PDE. Recall that the PDE \u2202u/\u2202t + c\u2202u/\u2202x = 0 has plane wave solutions of the form Aeik(x\u2212ct) = Aei(kx\u2212\u03c9t), since the exact solution is of the form u(x, t) = u(x \u2212ct, 0)."}
{"text": "The FDR \u03c9 = kc gives the exact frequency of the PDE. By analogy we try to \ufb01nd solutions of the FDE of the form Apei(pj\u2212\u03b8n), where \u03b8 = \u03bd\u0016t represents the computational frequency \u03bd multiplied by \u0016t (the computational frequency \u03bd is in general different than the exact frequency \u03c9). Substituting in the FDE and dividing by ei(pj\u2212\u03b8n), we get (e\u2212i\u03b8 \u2212ei\u03b8) + \u00b5(eip \u2212e\u2212ip) = 0 the FDR for the leapfrog scheme. Because sin \u03b8 = sin(\u03c0 \u2212\u03b8), the two solutions for the \ufb01nite difference FDR are Substituting into the FDR, and assuming that the initial amplitude for the wave- number p is 1, we obtain that the solution of the FDE is a sum of two terms corre- sponding to \u03b81 and \u03b82 respectively:"}
{"text": "j = Apei(pj\u2212\u03b8n) + (1 \u2212Ap)ei(pj+\u03b8n)(\u22121)n where \u03b8 = arcsin(\u00b5 sin p), and ei\u03c0 = \u22121. (This can also be obtained by noting that when we assume solutions of the form ei(pj\u2212\u03b8n), they imply an ampli\ufb01cation fac- tor \u03c1 = e\u2212i\u03b8 = cos \u03b8 \u2212i sin \u03b8 = \u2212i\u00b5 sin p \u00b1 1 \u2212\u00b52 sin2 p, i.e., sin \u03b8 = \u00b5 sin p, with two solutions as indicated above.) Of the two terms in the solution, the \ufb01rst one is the \u201clegitimate\u201d solution, which approximates the PDE solution. Note that the second term changes sign every time step, and it moves in the wrong direction: for this reason this unphysical term is called \u201ccomputational mode\u201d. It arises because the leapfrog scheme has three time levels, rather than two, giving rise to an additional spurious solution. Although the leapfrog scheme is simple and accurate, its three-time level character gives rise to two problems that need to be dealt with."}
{"text": "The \ufb01rst problem is that the leapfrog scheme needs a special initial step to get to the \ufb01rst time level U 1 from the initial conditions U 0, before it can be started (Fig. 3.2.4). This can be done in several simple ways:"}
{"text": "(b) Use for the \ufb01rst time step a forward time scheme. The forward scheme has truncation errors of order O(\u0016t), but since the time step is only used once, its 3 Numerical discretization of the equations of motion contribution to the global error is multiplied by \u0016t, so that the total error is still of O(\u0016t)2. For the same reason, the computational instability is not a signi\ufb01cant problem. Alternatively, use an Euler-backwards (Matsuno) scheme for the \ufb01rst time step (see Table 3.2.1)."}
{"text": "(c) Use half (or a quarter, eighth, etc.) of the initial time step for the forward time step (Fig. 3.2.4), followed by leapfrog time steps. This will halve (or reduce by a quarter, eighth, etc.) the error introduced in the unstable \ufb01rst step."}
{"text": "The second problem is that for nonlinear examples, the leapfrog scheme has a ten- dency to increase the amplitude of the computational mode with time, separating the space dependence in a checkerboard fashion between the even and odd time steps."}
{"text": "This can be solved by restarting every 50 steps or so, or by applying a Robert\u2013Asselin Exercise 3.2.4: Show that a forward time scheme is unstable for hyperbolic equa- Robert\u2013Asselin time \ufb01lter (Robert, 1969, Asselin, 1972) After the leapfrog scheme is used to obtain the solution at t = (n + 1)\u0016t, a slight time smoothing is applied to the solution at time n\u0016t:"}
{"text": "The smoother (3.2.31) is centered in time, and reduces the amplitude of different frequencies \u03bd by a factor (1 \u22124\u03b1 sin2(\u03bd\u0016t/2)). The computational mode, whose period is 2\u0016t, is reduced by (1 \u22124\u03b1) every time step. Because the \ufb01eld at t = (n \u22121)\u0016t is replaced by the already \ufb01ltered value, the \ufb01lter (3.2.30) introduces a slight distortion of the centered \ufb01lter (Asselin, 1972). This \ufb01lter is widely used with the leapfrog scheme, with \u03b1 of the order of 1%."}
{"text": "3.2 Initial value problems: numerical solution (a)\u2013(i)); dU/dt = F1(U) +F2(U) (schemes (j)\u2013(k)) Leapfrog (good for hyperbolic U n = U n + \u03b1(U n+1 \u22122U n + U n\u22121) Leapfrog smoothed with the Robert\u2013Asselin time \ufb01lter; diffusive terms, unstable for Crank\u2013Nicholson or centered Implicit, slightly damping Fully implicit or backward Euler-backward or Matsuno:"}
{"text": "Another predictor\u2013corrector = F(U n+1/2\u2217\u2217) U n+1 \u2212U n Runge\u2013Kutta (fourth order) 3 Numerical discretization of the equations of motion N = multiple of 4; Nth order Boundary conditions: periodic Initial conditions: u(x, 0) = c + A sin(kx) c = 20 m/s, A = 10 m/s, \u0016x = 200 km, k = 2\u03c0/L with L = 10\u0016x (a) Choose two time steps, one of which satis\ufb01es the CFL condition and one which violates it. How long does it take to \u201cblow up\u201d?"}
{"text": "Boundary conditions: periodic Initial conditions: u(x, 0) = A sin(kx) c = 20 m/s, A = 10 m/s, \u0016x = 200 km, k = 2\u03c0/L with L = 10\u0016x Choose two time steps, one of which satis\ufb01es the CFL condition and one which violates it. How long does it take to \u201cblow up\u201d? Compare with the linear equation Compute a nonlinear solution with high resolution, taking it as \u201ctruth,\u201d and then prepare a table summarizing R and RE."}
{"text": "Exercise 3.2.8: Explain physically why the term b\u0016t does not in\ufb02uence the stability In these schemes the advection or diffusion terms are written in terms of the new The factor \u03b1 determines the weight of the \u201cold\u201d time values compared with the \u201cnew\u201d time values in the right-hand side of the FDE. Using the von Neumann method, we j = A\u03c1neipj = Aei(pj\u2212\u03b8n) into (3.2.35)."}
{"text": "3 Numerical discretization of the equations of motion an implicit scheme. The dot represents the value being updated and the stars the Note that with the implicit extrapolation, and allows no This implies that \u03c1 \u22641 if \u03b1 \u22640.5, i.e., if the new values are given at least as much weight as the old values in computing the RHS. In this case there is no restriction on the size that \u0016t can take! This result (absolute stability, independent of the Courant number) is typical of implicit time schemes. In Fig. 3.2.5 we show that in an implicit scheme, a point at the new time level is in\ufb02uenced by all the values at the new level, which avoids extrapolation, and therefore is absolutely stable. Note also that if \u03b1 < 0.5 the implicit time scheme reduces the amplitude of the solution: it is an example of a damping scheme. This property is useful for solving some problems such as spuriously growing mountain waves in semi-Lagrangian schemes."}
{"text": "In summary, if we consider a marching equation explicit methods such as the forward scheme are either conditionally stable (when there is a condition on the Courant number or the equivalent stability number for parabolic equations) or absolutely unstable."}
{"text": "and a centered implicit scheme (Crank\u2013Nicholson) 3.2 Initial value problems: numerical solution are absolutely stable. The latter scheme is attractive because it is centered in time (around tn+1/2), and it can be written with centered space differences, which makes it second order in space and in time. Also, it only has two time levels so it does not have a computational mode. But, like all implicit schemes, it also has a great disadvantage."}
{"text": "If it involves only tridiagonal systems, this is not an obstacle, because there are fast methods to solve them. There are also methods, such as fractional steps (with each spatial direction solved successively), where one space dimension is considered at a time, that allow taking advantage of the large time steps permitted by implicit schemes without paying a large additional computational cost."}
{"text": "Moreover, we will see in the next section that the possibility of using a time step with a Courant number much larger than 1 in an implicit scheme does not imply that we will obtain accurate results economically. The implicit scheme maintains stability by slowing down the solutions, so that the slower waves do satisfy the CFL condition. For this reason implicit schemes are only useful for those modes (such as the Lamb wave or vertical sound waves) that are very fast but of little meteorological importance (semi-implicit schemes, see next section)."}
{"text": "(1) It is easy to check the properties of the time schemes in Table 3.2.1 when applied to hyperbolic equations by testing them with a simple harmonic with solution U(t) = U(0)e\u2212i\u03bdt. After one time step, the exact solution is U((n + 1)\u0016t) = U(n\u0016t)e\u2212i\u03bd\u0016t which indicates that the exact magni\ufb01cation factor is e\u2212i\u03bd\u0016t."}
{"text": "In (3.2.43), v is the computational frequency for a wave equation for a given space discretization. For example, if we were using second order centered differences in space, \u03bd = (sin k\u0016x/\u0016x) c, for a spectral scheme, \u03bd = kc. For the fully implicit time scheme (d), the ampli\ufb01cation factor is Since the exact ampli\ufb01cation factor has an amplitude equal to 1, this shows that the implicit scheme is dissipative; similarly, comparing the imaginary components of the exact and approximate ampli\ufb01cation factors, it is clear that the implicit solution is slowed down by a factor of about 1/[1 + (\u03bd\u0016t)2]."}
{"text": "3 Numerical discretization of the equations of motion Exercise 3.2.9: Show that the Crank\u2013Nicholson scheme signi\ufb01cantly slows down the angular speed of the solution by deriving the magni\ufb01cation factor for this scheme, and comparing it with the exact magni\ufb01cation factor e\u2212i\u03bd\u0016t. Determine the limit of the Crank\u2013Nicholson ampli\ufb01cation factor for the Courant number (2) Equations with damping terms (such as the parabolic equation) can also be simply represented by the equation:"}
{"text": "In (2.46), \u00b5 can be considered as the computational rate of damping. For example, for the diffusion equation, using centered differences in space, Exercise 3.2.10: Show that the leapfrog scheme is unstable for a damping term."}
{"text": "Exercise 3.2.12: Show that for a wave equation the forward time scheme with cen- tered differences in space is absolutely unstable. Note that this scheme shows that the \u201cno extrapolation\u201d rule is a necessary but not a suf\ufb01cient condition for stability Consider the SWEs that we discussed in Section 2.4.1:"}
{"text": "As indicated in that section, the phase speed of the inertia-gravity wave is given by 3.2 Initial value problems: numerical solution and the terms that give rise to the fast gravity waves are underlined. This means that the Courant number \u00b5 = cIGW\u0016t/\u0016x is dominated by the speed of external inertia-gravity waves (equivalent to the Lamb waves, horizontal sound waves), and an explicit scheme would therefore require a time step an order of magnitude smaller than that required for advection. For this reason, Robert (1969) introduced the use of semi-implicit schemes to slow down the gravity waves. We write such a scheme using the compact \ufb01nite difference notation for differences and averages:"}
{"text": "andsimilarlyfordifferencesin y ort.Withthisnotation,assuminguniformresolution, \u03b42x f = \u03b4x f x = fi+1 \u2212fi\u22121 Using this compact \ufb01nite difference notation we can write the leapfrog semi-implicit \u03b42tu + u\u03b42xu + v\u03b42yu = \u2212\u03b42x\u03c62t + f v \u03b42tv + u\u03b42xv + v\u03b42yv = \u2212\u03b42y\u03c62t \u2212f u \u03b42t\u03c6 + u\u03b42x\u03c6 + v\u03b42y\u03c6 = \u2212\u0007(\u03b42xu + \u03b42yv) Everything that does not have a time average involves only terms evaluated explicitly at the nth time step. We can rewrite the FDEs (3.2.50) as = \u2212\u03b42x(\u03c6n+1 + \u03c6n\u22121)/2 + Ru = \u2212\u03b42y(\u03c6n+1 + \u03c6n\u22121)/2 + Rv = \u2212\u0007[\u03b42x(un+1 + un\u22121)/2 + \u03b42y(vn+1 + vn\u22121)/2] + R\u03c6 where the \u201cR\u201d terms are the \u201crest\u201d of the terms evaluated at the center time n\u0016t. For example, Ru = f v \u2212u\u03b42xu \u2212v\u03b42yu, and similarly for Rv and R\u03c6."}
{"text": "From these three equations we can eliminate un+1, vn+1 and obtain an elliptic 3 Numerical discretization of the equations of motion Note that the right-hand side of this elliptic equation is evaluated at t = n\u0016t or (n \u22121)\u0016t, so that it is known. Solving this elliptic equation provides \u03c6n+1, and once this is known, it can be plugged back into the \ufb01rst two equations of (3.2.51), and thus (un+1, vn+1) can be obtained."}
{"text": "The elliptic operator in brackets in the left-hand side of (3.2.52), is a \ufb01nite differ- ence equivalent to (\u22072 \u2212\u03bb2), \u03c6i+2, j + \u03c6i\u22122, j + \u03c6i, j+2 + \u03c6i, j\u22122 \u2212 where we have assumed for simplicity that \u0016x = \u0016y = \u0016, and \u00b52 = \u0007\u0016t2/\u00162 is the square of the Courant number for gravity waves. Since \u00b52 = \u0007\u0016t2/\u00162 >> 1, the semi-implicit scheme distorts the gravity wave solution, slowing the gravity wave down until they satisfy the von Neumann criterion. This is an acceptable distortion since we are interested in the slower \u201cweather-like\u201d processes, and since the slower modes satisfy the CFL (von Neumann) stability criterion, and they are written ex- plicitly, they are not slowed down or distorted in a signi\ufb01cant way."}
{"text": "In the same way that the terms giving rise to gravity waves can be written semi- implicitly, the terms giving rise to sound waves can also be written semi-implicitly (Robert, 1982). They are the three-dimensional divergence in the continuity equation (Sections 2.3.2, 2.3.3). This has allowed the use of nonhydrostatic models without the use of the anelastic approximation or the hydrostatic approximation. Andr\u00b4e Robert (1982) created a model that can be considered the \u201cultimate\u201d atmospheric model. It treats the terms generating sound waves (anelastic terms, i.e., three-dimensional di- vergence), and the terms generating gravity waves (pressure gradient and horizontal divergence) semi-implicitly, and it uses a three-dimensional semi-Lagrangian scheme for all advection terms. This model, denoted the \u201cMesoscale Compressible Commu- nity\u201d (MCC) model, is a \u201cuniversal\u201d model designed so that it can tackle accurately atmospheric problems from the planetary scale through mesoscale, convective and smaller (Laprise et al., 1997)."}
{"text": "There is another approach followed by several major nonhydrostatic models (e.g., MM5 and ARPS): the use of fractional steps (see Table 3.2.1, scheme (k)), with the sound-wave terms integrated with small time steps. In addition, the ARPS model uses a semi-implicit scheme for vertically propagating sound waves (Xue et al., 1995)."}
{"text": "Exercise 3.2.13: Consider the diffusion equation \u2202u/\u2202t = \u03c3\u22022u/\u2202x2 with initial conditions u = x for x \u22640.5 and u = 1 \u2212x for x \u22650.5. Compute the \ufb01rst two time steps using an explicit scheme (forward in time, centered in space) with \ufb01ve points between x = 0 and x = 1, and a time step such that r = \u03c3\u0016t/(\u0016x)2 is equal to r = 0.1, 0.5, 1.0. Repeat using Crank\u2013Nicholson\u2019s scheme."}
{"text": "Second and fourth order schemes. It is convenient to separate the truncation errors in a discretized model into space truncation errors and time truncation errors. For explicit \ufb01nite difference models, the errors introduced by space truncation tend to dominate the total forecast errors because for \u201cweather waves\u201d the time step and the Courant number used are much smaller than would be required to physically resolve the frequency. Let\u2019s neglect for the moment time truncation errors and consider the wave equation \u2202U/\u2202t = \u2212c\u2202U/\u2202x discretized only in space."}
{"text": "If we approximate \u2202U/\u2202x using space centered differences, we get If instead of the closest neighboring points j + 1, j \u22121, we use the points j + 2, This is also a second order scheme, but the truncation errors are four times as large."}
{"text": "We can now eliminate from (3.3.1) and (3.3.2) the term A\u0016x2, and obtain 3\u03b44xU j = Ux \u22124B\u0016x4 + \u00b7 \u00b7 \u00b7 Now (3.3.3) is a fourth order approximation of the space derivative. So is a second order FDE and Assume solutions of the form U j(t) = Aeik(x j\u2212c\u2032t) = Aei(kx j\u2212\u03bd\u2032t) where c\u2032 is the computational phase speed, and \u03bd\u2032 the computational frequency, so that dU j/dt = \u2212iv\u2032U j. Making use of \u03b42xU j = i (sin k\u0016x/\u0016x) U j, and replacing 3 Numerical discretization of the equations of motion in (3.3.4) and (3.3.5) we \ufb01nd that for second order differences, and for fourth order differences, Note that (3.3.7) and (3.3.8) imply that the phase speed is always underestimated by space \ufb01nite differences. For the smallest possible wavelength, L = 2\u0016x, k\u0016x = \u03c0, the computational phase speed is zero for both second and fourth order differences:"}
{"text": "the shortest waves don\u2019t move at all (Fig. 3.3.1)! For L = 4\u0016x, k\u0016x = \u03c0/2, a much more accurate approximation is obtained with fourth order than with second order 4 = 0.85c, and the fourth order advantage becomes even better for longer waves: for L = 8\u0016x, c\u2032 We can also compute the computational group velocity \u2202\u03bd\u2032/\u2202k, where for second order differences. Then, for second order differences. Therefore, for the shortest waves, L = 2\u0016x, k\u0016x = \u03c0, with both second and fourth order differences the energy moves in the oppo- site direction to the real group velocity (equal to the phase speed c): c\u2032 and fourth order differences. As a result of the negative group velocity, space cen- tered FDEs of the wave equation tend to leave a trail of short-wave computational noise upstream of where the real perturbation should be. This problem is greatly reduced using more advanced recent schemes such as those of Takacs (1985) and Smolarkiewicz and Grawoski (1990)."}
{"text": "A second type of fourth order \ufb01nite difference scheme, known as the compact or implicit fourth order scheme, can be obtained by again making use of (3.3.1) but re- placing the third derivative in the truncation error for the centered differences by its \ufb01- nite difference approximation Uxxx j \u2248(Ux j+1 \u22122Ux j + Ux j\u22121)/ (\u0016x)2 + O (\u0016x)2."}
{"text": "The new fourth order scheme then becomes Ux j+1 + 4Ux j + Ux j\u22121 = 6U j+1 \u2212U j\u22121 It is called \u201ccompact\u201d because it involves only the point j and its closest neighbors, and \u201cimplicit\u201d because (3.3.11) results in a system of (tridiagonal) equations for the x-derivative, rather than an explicit estimate such as (3.3.4) or (3.3.5)."}
{"text": "With this scheme, the \ufb01nite difference space derivative for a given wavenumber is and the computational phase speed becomes and for L = 4\u0016x, k\u0016x = \u03c0/2, the phase speed is c\u2032 14 = 0.955c, which is consider- ably better even than the regular fourth order differences phase speed."}
{"text": "3 Numerical discretization of the equations of motion The group velocity for this scheme, is already positive for L = 3\u0016x (Fig. 3.3.1). For implicit schemes where one is already solving a tri-diagonal equation (see Section 3.4.2), this compact fourth order scheme, which has an accuracy equivalent to linear \ufb01nite elements, is very accurate and involves little additional computational cost. The compact scheme is similar to Galerkin \ufb01nite element approximation to space derivatives (Durran, 1999)."}
{"text": "Galerkin and spectral space representation The use of spatial \ufb01nite differences, as we saw in the previous section, introduces errors in the space derivatives, resulting in a computational phase speed slower than the true phase speed, especially for short waves."}
{"text": "The Galerkin approach to ameliorate this problem is to perform the space dis- cretization using a sum of basis functions U(x, t) = \u0017K k=1 Ak(t)\u03d5k(x). Then, the residual (error) R(U) = \u2202U/\u2202t + F(U) of the original PDE \u2202u/\u2202t + F(u) = 0 is required to be orthogonal to the basis functions \u03d5(x). The space derivatives are com- puted directly from the known d\u03d5(x)/dx. This procedure leads to a set of ordinary differential equations for the coef\ufb01cients Ak(t). If the basis functions chosen for the discretization are orthogonal and satisfy the boundary conditions, the derivation be- comes simpler. The use of local basis functions (e.g., \u03d5i(x) a piecewise linear function equal to 1 at a grid point i and zero at the neighboring points) gives rise to the \ufb01nite element method, with accuracy similar to that of the compact fourth order scheme."}
{"text": "Another popular type of Galerkin approach is the use of a global spectral expansion for the space discretization, which allows the space derivatives to be computed an- alytically rather than numerically. In one dimension, periodic boundary conditions suggest the use of complex Fourier series as a basis."}
{"text": "Consider a periodic domain of length L, with a number of grid points Jmax = JM, and scale x by 2\u03c0/L. If we use discrete complex Fourier series truncated to include wavenumbers up to K, the spectral representation is:"}
{"text": "k(t), and the star represents the complex conjugate. Alternatively, (3.3.14) can be written using real Fourier series as 3.3 Space discretization methods There are 2K + 1 distinct real coef\ufb01cients that are determined by Here we have used the orthogonality property If JM = 2K + 1, the grid representation (left-hand side of (3.3.14)) and the spectral representation (right-hand side of (3.3.14)) contain the same number of degrees of freedom, and the same information."}
{"text": "If we neglect the time discretization errors, as before, and assume solutions of the form U(x, t) = Aeik(x\u2212c\u2032t), we \ufb01nd that c\u2032 = c, i.e., the computational phase speed is equal to the true speed (Fig. 3.3.1). The space discretization based on a spectral rep- resentation is extremely accurate (the space truncation errors are of \u201cin\ufb01nite\u201d order)."}
{"text": "derivatives are computed ef\ufb01ciently and accurately in spectral space, whereas non- linear products are computed ef\ufb01ciently in physical space. This leads to the so-called transform method used for spectral models: the space derivative is computed in spec- tral space, then U is transformed back into grid space, and the product U j (\u2202U/\u2202x) jis computed locally in grid space. We will see later that in order to avoid nonlinear in- stability introduced by aliasing of wavenumbers beyond K that appear in quadratic terms, the grid representation requires about 3/2 as many points as the minimum number of points required for a linear transform (JM = 2K +1). For this reason the new values of U at time (n + 1)\u0016t are usually stored in their spectral representation, We can use von Neumann\u2019s criterion to determine the maximum time step allowed for stability using, for example, the leapfrog time scheme. The FDE is 3 Numerical discretization of the equations of motion Assuming solutions for the wave equation of the form U n = \u03c1neikx, we obtain that the ampli\ufb01cation factor is \u03c1 = \u2212ikc\u0016t \u00b1 1 \u2212k2c2\u0016t2, and in order to have |\u03c1| \u22641 we need to satisfy the stability condition Since the highest wavenumber present corresponds to L = 2\u0016x, the stability cri- terion for spectral models is therefore c\u0016t/\u0016x \u22641/\u03c0. So, the stability criterion is more restrictive for spectral models than for \ufb01nite difference models, but this is com- pensated by the fact that the accuracy, especially for shorter waves, is much higher, and therefore fewer short waves need to be included (Fig. 3.3.1)."}
{"text": "The basis functions used in spectral methods are usually the eigensolutions of the Laplace equation. In a rectangular domain, they are sines and cosines (e.g., the Regional Spectral Model (RSM), Juang et al., 1997). On a circular plate, one would instead use Bessel functions."}
{"text": "where \u00b5 = sin\u03d5, m is the zonal wavenumber and n is the \u201ctotal\u201d wavenumber in spherical coordinates (as suggested by the Laplace equation (3.3.21)). Pm associated Legendre polynomials in \u00b5 = sin \u03d5 = cos \u03b8, where \u03b8 = \u03c0 \u2212\u03d5 is the co- latitude. For example, the P0 Using \u201ctriangular\u201d truncation the spatial resolution is uniform throughout the sphere. This is a major advantage over \ufb01nite differences based on a latitude\u2013longitude grid, where the convergence of the meridians at the poles requires very small time steps. Although there are solu- tions for this \u201cpole problem\u201d for \ufb01nite differences, the natural approach to solve the pole problem for global models is the use of spherical harmonics. Williamson and Laprise (1998) provide a comprehensive description of numerical methods for global 3.3 Space discretization methods Williamson and Laprise (1998). (a) Depiction of three spherical harmonics with total wavenumber n = 6. Left, zonal wavenumber m = 0; center, m = 3; right, m = 6."}
{"text": "Note that n is associated with the total wavelength (twice the distance between a maximum and a minimum), which is the same for the three \ufb01gures. (b) and (c) Amplitude of Legendre polynomials for different combinations of m and n showing how high zonal wavenumbers are suppressed near the poles, so that the horizontal resolution is uniform when using a spectral representation with triangular truncation."}
{"text": "Fig. 3.3.2(a) shows the shape of three spherical harmonics with total wavenum- ber n = 6, and zonal wavenumber m = 0, 3 and 6. Note that the distance between neighboring maxima and minima is similar for the three harmonics, and is associated with the \u201ctotal\u201d (two-dimensional) wavenumber n. Figures 3.3.2(b) and (c) show that the amplitude of the Legendre polynomials for high zonal wavenumbers are indeed 3 Numerical discretization of the equations of motion suppressed near the poles. This suppression eliminates the need for small time steps due to the convergence of the meridians in the poles, which are not singular points Another numerical method that has become very popular in NWP models is the semi-Lagrangian scheme. The equations of motion, as we have seen, can in general be written as conservation equations where the left-hand side of the equation represents a total time derivative (following an individual parcel) of the vector of dependent variables u. The total time derivative (also known as individual, substantial or Lagrangian time derivative) is conserved for a parcel, except for the changes introduced by the source or sink S."}
{"text": "In a truly Lagrangian scheme, one would follow individual parcels (transporting them with the three-dimensional \ufb02uid velocity), and then add the source term at the right time. This is not practical in general because one has to keep track of many individual parcels, and with time they may \u201cbunch up\u201d in certain areas of the \ufb02uid, and leave others without parcels to track."}
{"text": "The semi-Lagrangian scheme avoids this problem by using a regular grid as in the previous schemes discussed (which are denoted Eulerian, because the partial 3.3 Space discretization methods arrival point AP at the new time level (a point) and the departure point DP at the previous time level. The thick arrow represents the advection from DP to AP. The value of the variables at AP is equal to their value at DP, which is obtained by interpolation between neighboring points. Because there is no extrapolation, the semi-Lagrangian schemes are absolutely stable."}
{"text": "derivative \u2202u/\u2202t is estimated instead of the total derivative). At every new time step we \ufb01nd out where the parcel arriving at a grid point (denoted arrival point or AP) came from in the previous time step (denoted departure point or DP). The value of u at the DP is obtained by interpolating the values of the grid points surrounding the departure point. Figure 3.3.3 suggests that, because there is no extrapolation, the semi-Lagrangian scheme is absolutely stable with respect to advection, which can be con\ufb01rmed by doing a von Neumann criterion check (Bates and McDonald, 1982)."}
{"text": "The semi-Lagrangian scheme can then be written using two or three time levels. In a three-level time scheme, for example, if MP is the middle point between the DP and AP, the scheme can be written as AP = (U n\u22121)DP + 2\u0016tS(U n)M P In a two-time level scheme it could be written as In general, for nonlinear equations \u2202q/\u2202t = \u2212u \u2202q/\u2202x + S(q), so the semi- Lagrangian scheme for the quantity q can be written as However, the DP has to be determined from the trajectory dx/dt = u integrated between the DP and AP, for example as Since u evolves with time, UAP and UDP are not known until the DP has been determined, this is an implicit equation that needs to be solved iteratively. For three- level semi-Lagrangian schemes, the approximation 3 Numerical discretization of the equations of motion also has to be solved iteratively for UM P, but this is simpler than for the two-level The accuracy of the semi-Lagrangian scheme depends on the accuracy of the de- termination of the DP, and on the determination of the value of UDP and the other conserved quantities q by interpolation from the neighboring points. A linear inter- polation between neighboring points results in excessive smoothing, especially for the shortest waves. For this reason cubic interpolation is preferred (Williamson and Laprise, 1998). This is a costly overhead of semi-Lagrangian schemes. Despite the additional costs, in practice this scheme has been found to be accurate and ef\ufb01cient (see the general review of semi-Lagrangian methods by Staniforth and C\u02c6ot\u00b4e (1991))."}
{"text": "A \u201ccascade\u201d method has been proposed that results in a very ef\ufb01cient high order inter- polation between the distorted Lagrangian grid and the regular Eulerian grid (Purser and Leslie, 1991, Leslie and Purser, 1995). This allowed Purser and Leslie to suggest a forward trajectory semi-Lagrangian approach instead of the conventional back- ward trajectory that we have so far described, which has additional advantages. (See Staniforth and C\u02c6ot\u00b4e (1991), Bates et al. (1995), Purser and Leslie (1996), Williamson and Laprise (1998) for further details.) Combining the semi-Lagrangian approach with a semi-implicit treatment of gravity waves (Section 3.2.5), as \ufb01rst suggested by Robert (1982) and Robert et al. (1985), increases its ef\ufb01ciency. Laprise et al. (1997) have documented a \u201cmesoscale compressible community\u201d model, which is nonhy- drostatic, three-level semi-Lagrangian, and uses the semi-implicit approach for both the elastic terms (three-dimensional divergence) and the gravity wave terms. As such, it is a \ufb02exible and accurate model that can be used for a wide range of scales."}
{"text": "Nonlinear computational instability. Quadratically conservative schemes. The Arakawa Jacobian In 1957 Phillips published the \ufb01rst \u201cclimate\u201d or \u201cgeneral circulation\u201d simulation ever made with a numerical model of the atmosphere. He started with a baroclinically unstable zonal \ufb02ow using a two-level quasi-geostrophic model, added small random perturbations, and was able to follow the baroclinic growth of the perturbations, and their nonlinear evolution. He obtained very realistic solutions that contributed signi\ufb01cantly to the understanding of the atmospheric circulation in mid-latitudes."}
{"text": "However, his climate simulation only lasted for about 16 days: the model \u201cblew up\u201d despite the fact that care had been taken to satisfy the von Neumann criterion for linear computational instability. In 1959, Phillips pointed out that this instabil- ity, which he named nonlinear computational instability (NCI), was associated with nonlinear terms in the quasi-geostrophic equations, in which products of short waves create new waves shorter than 2\u0016x. Since these waves cannot be represented in the grid, they are \u201caliased\u201d into longer waves. The shortest wave that can be repre- sented with a grid (with a wavelength 2\u0016x) corresponds to the maximum computa- tional wavenumber pmax = 2\u03c0/Lmin\u0016x = \u03c0. However, quadratic terms with Fourier 3.3 Space discretization methods (solid line) become folded back (dashed line) and are added to the original spectrum, producing a spurious maximum in the energy spectrum at the cut-off wavelength (dotted line). (b) Schematic showing that if we use a grid with 3/2 as many grid points as the original grid, the total spectrum in the Fourier transform of a quadratic product is increased by 3/2 (i.e., pmax = 3\u03c0/2). Then aliasing of wavenumbers between 3\u03c0/2 and 2\u03c0 occurs outside the original spectrum and it is avoided within the range 0 to \u03c0."}
{"text": "components will generate higher wavenumbers: e\u00b1ip1e\u00b1ip2 = e\u00b1i(p1\u00b1p2), doubling the maximum wavenumber. The new shorter waves, with wavenumbers p = \u03c0 + \u03b4, can- not be represented in the grid, and become folded back (aliased) into p\u2032 = \u03c0 \u2212\u03b4, leading to a spurious accumulation of energy at the shortest range (Fig. 3.3.4)."}
{"text": "The effect of NCI can be seen clearly in the following simple example: consider the nonlinear (quasi-linear) PDE \u2202u/\u2202t = \u2212u\u2202u/\u2202x and the corresponding FDE \u2202U j/\u2202t = \u2212U j(U j+1 \u2212U j\u22121)/2\u0016x. Suppose that at a given time t we have U1 = \u2202t = 0, i.e., U2 and U3 will grow without bound and the FDE will blow up. In fact this 3 Numerical discretization of the equations of motion will happen even for a linear model \u2202u/\u2202t = \u2212a(x)\u2202u/\u2202x if a1 = 0, a2 > 0, a3 < 0, a4 = 0. On the other hand, if a(x) is always of the same sign, and we use the same FDE i.e., that the solution will remain bounded. Numerical experiments show that nonlin- earcomputationalinstabilityarisesonlywhentherearechangesinsigninthevelocity."}
{"text": "Phillips (1959) proposed transforming the grid-space solution into Fourier series (with sine and cosine wavenumbers from 0 to \u03c0), and chopping the upper half of the spectrum (wavenumbers above \u03c0/2). Since the maximum wavenumber generated in a quadratic term is twice the original wavenumber, this avoids spurious aliasing, and, indeed, Phillips found that the model could then be run inde\ufb01nitely. However, the procedure is rather inef\ufb01cient, since half of the spectrum is not used."}
{"text": "For grid-point models, complete Fourier \ufb01ltering of the high wavenumbers has been found to be an unnecessarily strong measure to avoid nonlinear computational instability. Some models \ufb01lter high wavenumbers but only enough to maintain com- putational stability. Experience shows that as long as the amplitude of the highest wavenumbers is not allowed to acquire \ufb01nite amplitude, nonlinear computational stability can be avoided. For example, Kalnay-Rivas et al. (1977) combined the use of an energy-conserving fourth order model with a sixteenth order \ufb01lter (similar to the eighth power of the horizontal Laplacian (Shapiro, 1970)). This ef\ufb01ciently \ufb01ltered out the shortest waves (mostly between 2\u0016x and 3\u0016x) without affecting waves of wavelength 4\u0016x or longer, and resulted in an accurate and economic model.1 1 The Shapiro \ufb01lter of order n of a \ufb01eld Ui is a simple and ef\ufb01cient operator given by 2n = [1 \u2212(\u2212D)n]U j, where the \u201cdiffusion\u201d operator DU j = (U j+1 \u22122U j + U j\u22121)/4 is applied to the original \ufb01eld n times. For a Fourier component eip with wavenumber p = 2\u03c0\u0016x/L, the response of the operator is Deipj = \u2212(sin2p/2)eipjso that the second order 4(U j+1 + 2U j + U j\u22121) has a response 2 = (1 \u2212sin2 p/2)U j. This is a strong \ufb01lter that zeroes out the highest wavenumber (L = 2\u0016x), and reduces the amplitude of even the longer waves. A higher order \ufb01lter, for example 2n = 16, however, has the following desirable response: U j which still \ufb01lters out 2\u0016x waves, dampens waves shorter than 4\u0016x, and essentially leaves 3.3 Space discretization methods Spectral models with a wavenumber cut-off of M (i.e., with 2M + 1 degrees of freedom) require at least 2M + 1 grid points to be transformed into equivalent solutions in grid space. Orszag (1971) showed that if they are transformed into 3M + 1 grid points before a quadratic term is computed in physical space, then aliasing is avoided. In other words, it is not necessary to perform the space transform into 4M points. The reason for this is shown schematically in Fig. 3.3.4(b): even if there is aliasing, it only occurs on the part of the spectrum (above p = \u03c0) that is eliminated anyway on the back transformation into spectral space. For this reason, in two horizontal dimensions, spectral models use \u201cquadratic grids\u201d with about (3/2)2 as many grid points as spectral degrees of freedom, and therefore spectral models are \u201calias-free\u201d for quadratic computations. Triple products in spectral models still suffer from aliasing, but this is generally not a serious problem."}
{"text": "Using quadratically conserving schemes Lilly (1965) showed that it is possible to create a spatial \ufb01nite difference scheme that conserves both the mean value and its mean square value when integrated over a closed domain. Quadratic conservation will generally ensure that catastrophic NCI does not take place. Arakawa (1966) created a numerical scheme for the vorticity equation that conserves the mean vorticity, the mean square vorticity (enstrophy), and the kinetic energy. This ensures that the mean wavenumber is also conserved (as it is in the continuous equation), and therefore that even in the absence of diffusion the solution remains realistic. Arakawa and Lamb (1977) showed how an equivalent \u201cArakawa Jacobian\u201d can be written for primitive equation models."}
{"text": "Now, consider any function of G(\u03b1). Multiply the conservation equation by g(\u03b1) = dG/d\u03b1 and integrate over a closed domain (i.e., a domain bounded by walls with zero normal velocities or by periodic boundary conditions). It is easy to show that the mean value of G(\u03b1) will be conserved in time:"}
{"text": "Therefore the mass weighted mean and the mean squared value of \u03b1 (as well as all its higher moments) will be conserved. With \ufb01nite differences, we can only enforce two independent conservation properties (Arakawa and Lamb, 1977). We discuss now how to enforce mean and quadratic conservation, as suggested by Lilly (1965)."}
{"text": "The simplest approach is to write \ufb01rst the FDE continuity equation in \ufb02ux form. This constitutes the backbone of a quadratically conservative scheme, and it is also similar to the simplest \ufb01nite volume schemes (Section 3.3.6)."}
{"text": "Exercise 3.3.3: Consider any function G(\u03b1) and multiply the conservation equation by g(\u03b1) = dG/d\u03b1 and integrate over a closed domain. Show that the mean value of G(\u03b1) will be conserved as in (3.3.32)."}
{"text": "Consider Fig. 3.3.5, which shows a typical grid element with the value of \u03b1 de\ufb01ned in the center, and estimates of the normal mass \ufb02uxes at its boundaries (e.g., (hu)i+1/2, j at the right wall). These estimates are used for casting the continuity FDE and for constructing a quadratically conservative FDE for \u03b1. The continuity FDE in \ufb02ux form + (hu)i+1/2, j \u2212(hu)i\u22121/2, j + (hv)i, j+1/2 \u2212(hv)i, j\u22121/2 It is easy to check that this FDE will conserve total mass (\u2202/\u2202t) \u0017 hi, j\u0016xi, j\u0016yi, j = 0 since the mass \ufb02ux into one grid box will cancel the mass \ufb02ux out of the neighboring element. We now write the FDE for \u03b1 using any consistent estimate of \u03b1 at the normal walls of the grid box:"}
{"text": "+ (hu)i+1/2, j(\u03b1)i+1/2, j \u2212(hu)i\u22121/2, j(\u03b1)i\u22121/2, j +(hv)i, j+1/2(\u03b1)i, j+1/2 \u2212(hv)i, j\u22121/2(\u03b1)i, j\u22121/2 3.3 Space discretization methods grid element with the value of \u03b1 de\ufb01ned in the center, and estimates of the normal These estimates are used for casting the continuity FDE quadratically conservative equation model). (b) Grid (quasi-geostrophic model)."}
{"text": "Again it is easy to check that this FDE will conserve the total (mass weighted) value of \u03b1: (\u2202/\u2202t) #i, jhi, j\u03b1i, j\u0016xi, j\u0016yi, j = 0. This is a general property of FDEs written Finally, we choose to estimate the value of \u03b1 at the walls of the grid-cells as an av- erage between the two contiguous cells (\u03b1)i+1/2, j = (\u03b1i, j + \u03b1i+1, j)/2, and similarly for the other walls. With this particular choice, we obtain:"}
{"text": "+ (hu)i+1/2, j(\u03b1i, j + \u03b1i+1, j) \u2212(hu)i\u22121/2, j(\u03b1i, j + \u03b1i\u22121, j) +(hv)i, j+1/2(\u03b1i, j + \u03b1i, j+1) \u2212(hv)i, j\u22121/2(\u03b1i, j + \u03b1i, j\u22121) We can show that this scheme is quadratically conservative. First note that we can construct a mass weighted quadratic conservation equation for \u03b1 from either the advection of the \ufb02ux form prognostic equation for \u03b1 and the continuity equation (prognostic equation for h):"}
{"text": "The second equality suggests how to test quadratic conservation of \u03b1. Multiply the FDE continuity equation (3.3.33) by \u03b1i, j 2/2 and subtract it from the \ufb02ux form prognostic equation (3.3.35) multiplied by \u03b1i, j. If we do this, we \ufb01nd that (because of 3 Numerical discretization of the equations of motion cancellations of mass weighted \ufb02uxes of \u03b1i, j on the grid-box walls), there is indeed Note that this is true no matter how the FDE for the continuity equation is written. We could choose several \ufb01nite difference formulations, and as long as the \ufb02ux form of the FDE for h\u03b1 is consistent with the continuity equation, and as long as we estimate \u03b1 at the walls by a simple average, we have quadratic conservation and the danger Exercise 3.3.4: Show that the FDE (3.3.33) will conserve total mass."}
{"text": "In this case a simple scheme that conserves the mean vorticity and its mean square (i.e., an enstrophy conserving scheme) can be written following the recipe given above (Kalnay-Rivas and Merkine, 1981). The continuity equation is (cf."}
{"text": "\u2212(v)i, j+1/2 \u2212(v)i, j\u22121/2 where the normal velocity estimates are obtained from \u2248\u2212$i+1/2, j+1/2 \u2212$i+1/2, j\u22121/2 and similarly for the other velocities. Note that this satis\ufb01es the continuity equation automatically. Then we write the forecast equation for the vorticity in a way consistent 3.3 Space discretization methods with the continuity equation, thus ensuring conservation of the mean vorticity and enstrophy (mean square vorticity)."}
{"text": "\u2212(v)i, j+1/2(\u03b6i, j + \u03b6i, j+1) \u2212(v)i, j\u22121/2(\u03b6i, j + \u03b6i, j\u22121) After a new vorticity \ufb01eld is obtained at t = (n + 1)\u0016t using, for example, leapfrog, we have to determine the new streamfunction \u03c8. This is done by solving the elliptic equation \u03b6 = \u22072$, which in \ufb01nite differences can be written as In Section 3.4 we will discuss how to solve this boundary value problem."}
{"text": "Once we obtain \u03c8i, j we can obtain $i+1/2, j+1/2 by averaging the corresponding four surrounding values of \u03c8i, j. This is probably the simplest FDE model of the barotropic atmosphere devoid of nonlinear computational instability that we can Before we discuss the Arakawa Jacobian, let\u2019s note that the continuous vorticity equation conserves total (kinetic) energy as well as enstrophy. Multiply the vorticity equation by the streamfunction:"}
{"text": "Therefore, integrating (3.3.43) over the domain, the mean kinetic energy is conserved. The simple scheme described above conserves vorticity and squared vorticity but not Arakawa (1966) introduced a Jacobian that conserves all three properties: it is based on the FDE corresponding to these three equivalent formulations of the 3 Numerical discretization of the equations of motion The Arakawa Jacobian is the \ufb01nite difference Jacobian corresponding to JA = (J1 + J2 + J3)/3 and it conserves kinetic energy and enstrophy. Arakawa and Lamb (1977) showed how the Arakawa Jacobian could also be approximately constructed for primitive equation models."}
{"text": "Exercise 3.3.8: Derive the \ufb01nite difference equivalent of J1, J2 and J3 The ratio of enstrophy to kinetic energy is proportional to the mean square of the wavenumber, and this quantity is conserved by the continuous frictionless vorticity equation. Therefore, the Arakawa conservation ensures that a long model run will conserve the mean square of the wavenumber, and generally look realistic (i.e., not become dominated by small scale noise) even without horizontal diffusion. In the real atmosphere, however, turbulent dissipation acts as a control on the amplitude of the smallest waves, and leaks their energy out of the system, so that strict conservation is not truly relevant. For this reason, there is no consensus among the community of modelers on whether the use of strictly conserving FDEs is an essential requirement."}
{"text": "On the one hand, some models are based on schemes that are as conservative as pos- sible (remember that the continuous equations conserve all moments of the quantities being advected, whereas the FDEs can only conserve one or two moments). Other modelers prefer to use less conservative but more accurate and simpler schemes."}
{"text": "They include dissipation acting at the highest wavenumbers that mimics the leakage of energy that takes place in reality. Experience shows that an energy-conserving scheme, for example, combined with a small amount of high-order horizontal diffu- sion, in practice also behaves very realistically, approximately conserving enstrophy."}
{"text": "This is because a catastrophic loss of enstrophy occurs only when energy is allowed to accumulate in the shortest waves and they acquire large amplitudes. The dispute as to whether it is more important to have conservative FDEs or accurate (higher order or semi-Lagrangian) FDEs that are not conservative but avoid NCI has thus not been So far all the variables we have used (e.g. h, u, v) have been de\ufb01ned at the same location in a grid cell. This means that in order to compute centered space 3.3 Space discretization methods dimension; (b) example of differences at a point j, for example, we need to go to j + 1, and j \u22121, and the differences are computed over a distance of 2\u0016x (Fig. 3.3.6(a)). If we use instead a staggered grid, certain differences (such as the pressure gradient for the u equa- tion and the horizontal convergence term for the h equation) can be computed over just 1\u0016x, and, for those terms, it is equivalent to doubling the horizontal res- olution. (Fig. 3.3.6(b)). However, the advection terms still have to be computed over 2\u0016x (or 2d, where d is the distance between closest grid points of the same Let\u2019s consider again the SWE in two dimensions:"}
{"text": "The terms in square brackets in (3.3.47) are the dominant terms for the geostrophic and the inertia-gravity wave dynamics. These terms are computed in different ways depending on the type of grid we use. The advective terms are less affected by the choice of alternative (staggered) grids."}
{"text": "In two dimensions there are several possibilities for staggered grids (Arakawa and Lamb, 1977), which are shown schematically in Fig. 3.3.7. Grid A (unstag- gered) has several advantages and disadvantages. The advantages are its simplicity, and, because all variables are available at all the grid points, it is easy to construct a higher order accuracy scheme. Grid A tends to be favored by proponents of the philosophy \u201caccuracy is more important than conservation\u201d. Its main disadvantage is that all differences occur on distances 2d, and that neighboring points are not coupled for the pressure and convergence terms. This can give rise in time to a horizontal uncoupling (checkerboard pattern), which needs to be controlled by using a high order diffusion (e.g., Janjic, 1974, Kalnay-Rivas et al., 1977)."}
{"text": "Grid C has the advantage that the convergence and pressure terms in square brack- etsin(3.3.47)arecomputedoveradistanceofonly1d,whichisequivalenttodoubling the resolution of grid A. For this reason geostrophic adjustment (the dispersion of gravity waves generated when the \ufb01elds are not in geostrophic balance, see Chapter 5) 3 Numerical discretization of the equations of motion Lamb (1977) classi\ufb01cation."}
{"text": "is computed much more accurately (Arakawa, 1997). The Coriolis acceleration terms, on the other hand, require horizontal averaging, making the inertia-gravity waves less accurate. This makes grid C less attractive for situations in which the length of the Rossby radius of deformation Rd = \u221agH/f is not large compared to the grid size d. The equivalent depth, H, is about 10 km for the external mode, so that Rd is 3.3 Space discretization methods about 3000 km (Chapter 6), but H is an order of magnitude smaller for the second vertical mode, and it becomes much smaller for higher vertical modes. Therefore, some atmospheric models use grid B, where the minimum distance for horizontal 2d, rather than 1d as in grid C, but where u and v are available at the same locations. The NCEP Eta model is de\ufb01ned on a grid B rotated by 45\u25e6, denoted grid E by Arakawa and Lamb (1977), see Fig. 3.3.7."}
{"text": "The disadvantages of staggered grids are: (a) the terms in square brackets are hard to implement in higher order schemes, and (b) the staggering introduces considerable complexity in, for example, diagnostic studies and graphical output."}
{"text": "Grid D has no particular merit, but, if also staggered in time (as suggested by Eliassen), it becomes ideal for atmospheric \ufb02ow using the leapfrog scheme (see Fig. 3.3.8). In the Eliassen grid all differences are computed on a distance d (the advection also requires a horizontal average over one grid length, but this is a small drawback). Despite its apparent optimality, this grid has not been adopted in any major model, probably because of the complications of the additional staggering, and because it would require special procedures for starting the leapfrog scheme. Lin and Rood (1997) have adopted a similar idea for a global atmospheric model on the In the vertical direction, most models have adopted a staggered grid, with the vertical velocity de\ufb01ned at the boundary of layers and the prognostic variables in the center of the layer (Fig. 3.3.9). This type of grid, introduced by Lorenz in 1960, allows simple quadratic conservation, and the boundary conditions of no \ufb02ux at the top and the bottom are easily ful\ufb01lled. However, as pointed out by Arakawa and Moorthi (1988), the Lorenz grid allows the development of a spurious com- putational mode, since the geopotential in the hydrostatic equation (and therefore the acceleration of the wind components) is insensitive to temperature oscillations of 2\u0016\u03c3 wavelength. The Lorenz grid is being replaced in some newer models by a vertical grid similar to the one introduced by Charney and Phillips (1953) for a two-level model. In the Charney\u2013Phillips grid, the vertical staggering is more consistent with the hydrostatic equation and therefore it does not have the ad- ditional computational mode (Arakawa, 1997). A nonstaggered vertical grid, al- lowing a simple implementation of higher order differences in the vertical, would 3 Numerical discretization of the equations of motion Unstaggered vertical grid and with walls at which \ufb02uxes are computed in a \ufb01nite volume method."}
{"text": "also be possible, but it would also have more computational modes present in the We present here a brief introduction to the \ufb01nite volume approach, which is discussed in more detailed in texts such as Durran (1999), Fletcher (1988) and Gustaffson et al. (1996). The basic idea of this method is that the governing equations are \ufb01rst written in an integral form for a \ufb01nite volume, and only then are they discretized."}
{"text": "This is in contrast to the methods we have seen so far, in which the equations in differential form are discretized using \ufb01nite differences or spectral methods. The two approaches may or may not lead to similar discretized schemes."}
{"text": "Consider for example the continuity equation and a conservation equation for a shallow water model written in \ufb02ux form, as in (3.3.31), and integrate them over a volume limited by walls AB, BC, C D, and DA (in this two-dimensional case, the volume of integration is the horizontal area, Fig. 3.3.10)."}
{"text": "If we integrate (3.3.31) within the volume ABC D, and apply Green\u2019s theorem, 3.3 Space discretization methods where H is the normal \ufb02ux of h across the walls, and n is the normal vector to the wall. These equations can be discretized, for example, as Here, the overbar indicates a suitable average over the grid volume or area. It is evident that any scheme based on these \ufb01nite volume equations will conserve the average mass and average mass weighted \u03b1. There are a number of choices of how this average can be carried out over this subgrid domain of each grid volume: one can assume that h and \u03b1 are constant within the volume, or that they vary linearly, etc. A simple choice for the estimates of the average values at the center and at the walls leads naturally to the quadratically conservative differences presented above in i+1/2 j = (hi j + hi+1 j)(ui j + ui+1 j)/4 i+1/2 j)(\u03b1i j + \u03b1i+1 j)/2 Although in this case both methods lead to the same discretization, the \ufb01nite volume approach allows additional \ufb02exibility in the choice of discretization. For example, Lin and Rood (1996) developed a combination of semi-Lagrangian and \ufb01nite vol- ume methods, in which the boundaries of the grid volume are transported to the new time step, rather than the centers of the volume as is done in the conventional semi-Lagrangian schemes (Fig. 3.3.11). Although the order of the scheme is for- mally low, the method seems very promising, but it requires considerable care in the detailed formulation in order both to remain conservative and to maintain the shape of the transported tracers. Lin (1997) also developed a rather simple \ufb01nite volume expression to compute the horizontal pressure gradient force that can be applied to any hydrostatic vertical coordinate system. It avoids the problem of having two large terms in the pressure gradient computation that almost cancel each other, which is 3 Numerical discretization of the equations of motion Rood, 1996). It differs from the regular semi-Lagrangian scheme (Fig. 3.3.3) in that the walls of the volume are transported to the \u201carrival walls\u201d. The mass weighted average of the variables at the arrival volume is equal to the value at the departure volume (indicated by the thick segments), ensuring mass conservation. Because there is no extrapolation, the \ufb02ux-form semi-Lagrangian scheme is still absolutely characteristic of the sigma and other vertical coordinate systems. There are several other variants of \ufb01nite volume systems (e.g., Durran, 1999)."}
{"text": "Elliptic equations are boundary value problems, with either a \ufb01xed time, or a steady state solution at long times. Two examples of such problems arising in NWP (a) Finding the new streamfunction from the vorticity after the latter has been updated to time (n + 1)\u0016t. For example, in Section 3.3.4 we introduced the following enstrophy-conserving numerical scheme:"}
{"text": "\u2212(v)i, j+1/2(\u03b6i, j + \u03b6i, j+1) \u2212(v)i, j\u22121/2(\u03b6i, j + \u03b6i, j\u22121) where we used the leapfrog scheme, and the right-hand side is evaluated at time t = n\u0016t. After solving for \u03b6 n+1 i, j , we can obtain the streamfunction by solving the elliptic equation (Laplace) valid at t = (n + 1)\u0016t:"}
{"text": "3.4 Boundary value problems (b) Solving a semi-implicit elliptic equation for the heights also at (n + 1)\u0016t These linear elliptic equations are easily solved with spectral methods in which the basis functions are eigenfunctions of the Laplace equation. For example, if we use spherical harmonics on the globe, and make use of n (\u03bb, \u03d5) simply by writing the Helmholtz linear equation \u22072\u03c6n+1 \u2212 (1/\u0007\u0016t2)\u03c6n+1 = F corresponding to (3.4.3) component by component so that the solution for each spherical harmonic coef\ufb01cient is given by (Note that in (3.4.5) and (3.4.6) we have used p instead of n for the time step to avoid confusion with the total wavenumber n.) The simplicity with which the semi- implicit scheme can be computed is a major advantage of spectral models. For \ufb01nite differences, the solution is much more involved."}
{"text": "The methods of solution for elliptic equations (discretized in space) are basically of two types: direct and iterative. Here we only present some simple examples of both types of methods, and refer the reader to texts such as Golub and van Loan (1996), Ferziger and Peric (2001), Dahlquist and Bj\u00a8ork (1974) and Gustaffson et al. (1996) for more complete discussions of direct and iterative schemes. In the last decade, considerable work has also been done on the solution of nonsymmetric systems."}
{"text": "(1995), Bruaset (1995), Greenbaum (1997), and Meurant (1999). Direct methods for linear systems We saw that for spectral models, the direct solution of the linear elliptic equation arising from the semi-implicit method is trivial. For \ufb01nite differences, however, direct methods involve solving equations like (3.4.2) or (3.4.3), which can be written in using any direct solver. They are related to Gaussian elimination. If the matrix A is \ufb01xed (e.g., independent of the time step) the LU decomposition of A = LU, where the diagonal of L are lii = 1, allows us to perform the decomposition once and then solve LX = F, followed by U\u0007 = X. Here L and U are lower and upper triangular If the matrix is tridiagonal, the direct problem is particularly easy to solve. A tridiagonal problem can be written as:"}
{"text": "a jU j\u22121 + b jU j + c jU j+1 = d j with general boundary conditions An algorithm based on Gaussian elimination is the \u201cdouble sweep\u201d method: Assume Then U j\u22121 = E j\u22121U j + Fj\u22121 which can be substituted into the tridiagonal equation (a j E j\u22121 + b j)U j + c jU j+1 = d j \u2212a j Fj\u22121 So the method of solution is:"}
{"text": "(a) use the lower boundary condition U0 = A1U1 + A2 to determine (b) sweep forward using (3.4.12) to obtain E j, Fj, 3.4 Boundary value problems (c) determine UJ, UJ\u22121 from UJ\u22121 = E J\u22121UJ + FJ\u22121 and the upper boundary condition UJ = B1UJ\u22121 + B2; (d) determine U j, j = J \u22122, . . . , 1 using (3.4.10)."}
{"text": "Iterative methods for solving elliptic equations The system A\u03c6 = F can be solved iteratively by transforming it into another system, choosing an initial guess \u03c60 and then iterating (3.4.13): \u03c6v+1 = M\u03c6v + F. The method converges if the spectral radius \u03c3(M) = max |\u03bbi| < 1, where \u03bbi are the eigen- values of M. The asymptotic convergence rate is de\ufb01ned as We now give an example for a simple elliptic equation to provide an idea of how to attack the problem. The reader is referred to the references cited in subsection 3.4.1 for a more comprehensive discussion."}
{"text": "For a uniform grid with \u0016x = \u0016y = \u0016, an elliptic equation like (3.4.3) can be where the \ufb01nite difference Laplace operator is \u03b42\u03c6i, j = (\u03c6i+1, j + \u03c6i\u22121, j + \u03c6i, j+1 + \u03c6i, j\u22121 \u22124\u03c6i, j) Suppose we are in iteration v. Then i, j is the error in iteration v. If we assume at the point i, j This is the Jacobi simultaneous relaxation method. If we start at the southwest corner, and sweep to the right and up, by the time we reach the point i, j we have already 3 Numerical discretization of the equations of motion updated the neighboring points to the west and the south, so we can use these updated This is the Gauss\u2013Seidel or successive relaxation method."}
{"text": "If instead, we overcorrect by changing the sign of \u2208v+1 i, j rather than making it equal the rate of convergence is further increased. This is the successive overrelaxation (SOR) method. Optimal values for \u03c9 can be obtained analytically for simple geome- tries such as a rectangular domain. For the equation above, the spectral radius of the and JM, KM are the number of intervals in the x and y directions of the problem."}
{"text": "Then the optimum value of the overrelaxation coef\ufb01cient is Since the maximum error is reduced after each Jacobi iteration by the spectral radius \u03bb1 = (1 \u2212\u03b5), we can de\ufb01ne the rate of convergence as \u03b5."}
{"text": "The rates of convergence of the three methods are then: \u03b5 = rate of convergence of the Jacobi iteration; 2\u03b5 = rate of convergence of the Gauss\u2013Seidel iteration; 2\u03b5 = rate of convergence of the SOR iteration with optimum We give only a simple introduction to other methods and refer the reader for further details to the references cited in Section 3.4.1."}
{"text": "Alternating Direction Implicit (ADI) An ef\ufb01cient fractional time steps time scheme (Table 3.2.1) is used to obtain the solution of the elliptic equation as a steady state solution. For example, to solve the 3.4 Boundary value problems Laplace equation we write the parabolic equation The asymptotic long-time solution of (3.4.22) is the solution to the Laplace equation."}
{"text": "Equation (3.4.22) is integrated numerically by separating it into two fractional steps (similar to the time scheme k in Table 3.2.1) Since each fractional step is implicit, large time steps can be used. And since the solution of each fractional step involves only inverting tridiagonal matrices, it can be performed very ef\ufb01ciently (see, e.g., Hageman and Young (1981))."}
{"text": "The speed of convergence for iterative schemes depends on the number of grid points, and is much faster for coarser grids (see expression for \u03bb1 above). Moreover, the errors that take longest to converge correspond to long waves (i.e., they are smooth), whereas the shortest waves are damped fastest. Multigrid methods take advantage of this and use both coarse and \ufb01ne grids (see Briggs (1987), Hackbusch (1985), Barrett et al. (1995)). The procedure is as follows: Several steps of a basic method on the full grid are performed in order to smooth out the error (pre-smoothing). A coarse grid is selected from a subset of the grid points, and the iterative method is used to solve the problem on this coarse grid. The coarse grid solution is then interpolated back to the original grid, and the original method applied again for a few iterations (post-smoothing). In carrying out the solution in the second step, the method can be applied recursively to coarser grids, until the number of grid points is small enough that a direct solution can be obtained."}
{"text": "The method of descending through a sequence of coarser grids and then ascending back to the full grid is known as a V-cycle. A W-cycle results from visiting the coarse grid twice, with some smoothing steps in between. Some multigrid methods have an (almost) optimal number of operations, i.e., almost proportional to the number of There are a number of iterative algorithms for solving the linear problem of (3.4.7) in the Krylov subspace, de\ufb01ned by r0, Ar0, A2r0, . . . , Am\u22121r0 where r0 = F \u2212A\u03c60 is the residual for an arbitrary initial error \u03c60. The approximate solution \u03c6m lies in the space \u03c60 + Km(A,r0). The residual after m steps has to satisfy 3 Numerical discretization of the equations of motion certain conditions, and the choice of the condition gives rise to different types of iterative methods (e.g., Sameh and Sarin, 1999). The requirement that the residual be orthogonal to the Krylov subspace, F \u2212A\u03c6m\u22a5Km(A,r0) leads to the conjugate gra- dient and the Lanczos methods. Methods like GMRES, MINRES and ORTHODIR are obtained by requiring that the residual be minimized over the Krylov subspace."}
{"text": "The bi-conjugate gradient and QMR methods are derived requiring the residual to be orthogonal to Km(AT ,r0). The discussion of these methods applicable to nonsym- metric systems is beyond the scope of this book, but is given in the texts referred in Lateral boundary conditions for regional models The use of regional models for weather prediction has arisen from the desire to reduce the model errors through an increase in horizontal resolution that cannot be afforded in a global model. Operational regional models have been embedded or \u201cnested\u201d into coarser resolution hemispheric or global models since the 1970s. In the USA, the \ufb01rst regional model was the LFM model (Chapter 1). The nesting of regional models requires the use of updated lateral boundary conditions obtained from the We have seen that for pure hyperbolic equations there should be as many boundary conditions imposed at a given boundary as the number of characteristics moving into the domain. Parabolic equations with second order diffusion require one boundary condition at every point in the boundary for each prognostic equation. Second order elliptic equations (such as Laplace, Poisson, and Helmholtz equations) also require one boundary condition. The \ufb01rst forecast experiment of Charney et al. (1950) used the barotropic vorticity equation (conservation of absolute vorticity), and already had to deal with boundary conditions. They solved the hyperbolic equation \u2202\u03b6/\u2202t = \u2212v \u00b7 \u2207(\u03b6 + f ) followed by the Poisson (elliptic) equation \u22072\u2202$/\u2202t = \u2202\u03b6/\u2202t."}
{"text": "Therefore, Charney et al. (1950) had to impose a boundary condition on the stream- function at all the boundary points (needed to solve the Poisson equation) and a boundary condition for the vorticity at the in\ufb02ow points. They used persistence in both cases: for the elliptic equation they used as boundary condition \u2202$/\u2202t = 0 (i.e., the normal wind remains constant), and then speci\ufb01ed that the vorticity also remained constant (\u2202\u22072$/\u2202t = 0) at the in\ufb02ow points and extrapolated the vorticity using upstream differences at the out\ufb02ow points."}
{"text": "For the SWEs, there are three characteristics, one corresponding to a geostrophic solution, moving with the speed of the \ufb02ow U, and the other two corresponding to inertia-gravity waves, moving with speed U \u00b1 f 2k2 + \u0007. At the boundaries, if the speed of inertia-gravity waves is larger than U and the \ufb02ow is inward, we have 3.5 Lateral boundary conditions for regional models to specify two boundary conditions. If the \ufb02ow is outward, we have to specify one boundary condition (corresponding to the inertia-gravity waves moving in). If U is greater than the speed of the inertia-gravity waves, we have to specify all three boundary conditions at the in\ufb02ow points and none at the out\ufb02ow points. For parabolic equations (with horizontal diffusion), each predicted variable has to be speci\ufb01ed as well at all lateral boundaries."}
{"text": "Oliger and Sundstrom (1978) showed that the hydrostatic primitive equations are not purely hyperbolic (because of the loss of the time derivative of the vertical velocity), and that they do not have a well-posed set of boundary conditions. In an excellent review of the lateral boundary condition used in operational regional NWP models, McDonald (1997) pointed out that with the presence of horizontal diffusion in models there is a feeling that we can \u201cover-specify slightly the lateral boundary conditions and not do very much damage\u201d."}
{"text": "In practice, boundary conditions are chosen pragmatically and tested numerically to check their appropriateness. Several methods have been tried over the years, but the most widely used is the boundary relaxation scheme introduced by Davies (1976)."}
{"text": "Davies (1983) has a very illuminating analysis of the impact of the different types of boundary conditions and their generation of spurious re\ufb02ection using simple ex- amples of wave equations and SWEs. He points out that an overspecifying boundary condition scheme is satisfactory if: (a) it transmits incoming waves from the \u201chost\u201d model providing boundary information without appreciable change of phase or am- plitude, and (b) at the out\ufb02ow boundaries, re\ufb02ected waves do not reenter the domain of interest with appreciable amplitude. We follow the Davies (1983) analysis and the review by McDonald (1997) in the rest of this section. Durran (1999), Chapter 8, is also devoted to this subject."}
{"text": "Lateral boundary conditions for one-way The majority of regional models have \u201cone-way\u201d lateral boundary conditions, i.e., the host model, with coarser resolution, provides information about the boundary values to the nested regional model, but it is not affected by the regional model solution. This approach has some advantages: (a) it allows for independent develop- ment of the regional model, and (b) the host model can be run for long integrations without being \u201ctainted\u201d by problems associated with nonuniform resolution or from the regional model. Overall, the regional one-way nesting can be considered to have been successful, in the sense that the boundary information from the host model is able to penetrate the regional model, and the regional model solution is able to leave the domain without appreciable deterioration of the solutions. The success can also be measured by the fact that there have been several attempts to perform long-term integrations of nested regional models. In these long-term integrations, the 3 Numerical discretization of the equations of motion initial regional information is swept out of the domain in the \ufb01rst day or two, and all the additional information comes from the global model integration. This approach is denoted \u201cregional climate modeling\u201d. Takle et al. (1999) discuss the Project to Intercompare Regional Climate Systems (PIRCS). In these extended integrations, the regional model acts as a \u201cmagnifying glass\u201d for the global solution, allowing the large-scale \ufb02ow to interact with smaller scale forcing such as orography, variations in soil moisture and land\u2013sea contrast, and as a result tend to give a more realistic so- lution. The \u201cadded value\u201d over the global solutions empirically indicates the overall success of the one-way boundary conditions used in different models."}
{"text": "Pseudo-radiation boundary conditions Orlanski (1976) proposed a \ufb01nite difference approximation of the \u201cradiation condi- tion\u201d, i.e., specifying well-posed boundary conditions for pure hyperbolic equations."}
{"text": "One assumes that the prognostic equations locally satisfy \u2202u/\u2202t + c\u2202u/\u2202x = 0 and then estimates the phase speed c using a \ufb01nite difference equivalent of at the points immediately inside the boundary (denoted by b \u22121). Miller and Thorpe (1981) used \ufb01rst order upstream approximation as well as higher-order approximations. After estimating c\u2032, if it points into the is speci\ufb01ed. If it points out, the upstream scheme is used: un+1 b\u22121). If c\u2032\u0016t/\u0016x > 1 because the space derivative of u is small, Orlanski (1976) suggested limiting the value of c\u2032 to c\u2032 = \u0016x/\u0016t. Klemp and Lilly (1978) pointed out reasons why the approximate \u201cradiation schemes\u201d are not completely successful in avoiding spurious re\ufb02ection: there can be overspeci\ufb01cation at the boundaries, speci\ufb01cation of the right number of boundary conditions but not their correct values, and errors in the estimation of c\u2032. The radiation condition has been used for research models (e.g., Durran et al. 1993). Klemp and Durran (1983) and Bougeault (1983) used radiation boundary conditions at the top of the model."}
{"text": "Operational models generally do not use radiation boundary conditions and instead impose the condition that the vertical velocity be zero at the top (e.g., \u02d9\u03c3 = 0 at \u03c3 = \u03c3T for sigma coordinates). As a result, the presence of this arti\ufb01cial \u201crigid top\u201d leads to spurious wave re\ufb02ections and even generates instabilities near the top (e.g., Kalnay and Toth, 1996, Hartman et al., 1997)."}
{"text": "3.5 Lateral boundary conditions for regional models Diffusive damping in a boundary zone or \u201csponge layer\u201d (Burridge, 1975, In this method the global (or host) model boundary conditions are speci\ufb01ed for all variables, and horizontal diffusion is added over a boundary zone to dissipate the noisy waves generated by the boundary conditions:"}
{"text": "This would seem to be a natural choice for regional model boundary conditions since by increasing the order of the equation to make it parabolic within a limited boundary zone, it is possible to specify all variables at the boundary without overspecifying."}
{"text": "However, this approach also has clear disadvantages: it damps the incoming waves from the global model (unless they are long compared to the width of the damping zone). It also produces spurious re\ufb02ections of outgoing waves if \u03bd increases abruptly, and if it increases slowly it may not be enough to damp the re\ufb02ected waves. As a result, this method is not very much in use at this time."}
{"text": "Tendency modi\ufb01cation scheme (Perkey and Kreitzberg, 1976) The wave equation is replaced by where u is prescribed from the host model (which is assumed to be correct near the boundary), and \u03b3 is zero in the interior and increases to large values at the boundaries."}
{"text": "Since the host model follows the wave equation we can write an \u201cerror\u201d equation for the difference u\u2032 between the regional and the where c\u2217= c/(1 + \u03b3 ). Therefore the time tendency scheme advects the error and slows it down to almost zero at the boundaries, thus avoiding overspeci\ufb01cation. In practice this scheme is also found to produce spurious re\ufb02ections."}
{"text": "3 Numerical discretization of the equations of motion The \u201cerror\u201d equation is now indicating that the error is advected to or from the boundary and damped. At the in\ufb02ow boundaries only the differences between the regional and the host model are damped. Therefore this scheme mitigates the effects of overspeci\ufb01cation at the out\ufb02ow boundaries without introducing deleterious effects in the in\ufb02ow boundaries."}
{"text": "If K increases abruptly, it can also introduce some spurious re\ufb02ection. For this reason, Kallberg (1977) proposed the use of a smoothly growing function for K. Let\u2019s consider a complete prognostic equation for the regional model near the boundaries In (3.5.9) F includes all the regular \u201cforcing terms\u201d in the interior time derivative (e.g., advection, sources/sinks, etc.). We can discretize it in time, using, for example, the leapfrog scheme for the regular terms and backward implicit scheme for the boundary relaxation term, as Here the overbar represents the host model, un+1 is the updated regional model, and the subscript i indicates the regional model (internal) solution obtained before relaxing towards the host model values un+1:"}
{"text": "From (3.5.10) and (3.5.11) we can now write Here \u03b1 = 2\u0016t K varies from 0 in the interior (K = 0), to 1 at the boundary, where the regional model solution is speci\ufb01ed to coincide with the host model solution."}
{"text": "McDonald (1997) mentioned three functions that have been proposed for \u03b1( j), where we de\ufb01ne j = 0, \u03b1(0) = 1 at the boundary, and assume that the boundary zone has n points so that for j \u2265n, \u03b1( j) = 0. The \ufb01rst function, found to be optimal in minimizing false re\ufb02ection of both Rossby and gravity waves by Kallberg (1977), starts gently in the interior and has the steepest slope at the boundary: \u03b1 = 1 \u2212 tanh( j/2). Jones et al. (1995) used a linear pro\ufb01le \u03b1 = 1 \u2212j/n, and McDonald and Haugen (1992) proposed a cosine pro\ufb01le \u03b1 = [1 + cos( j\u03c0/n)]/2, which has the steepest slope at the center of the boundary zone. Benoit et al. (1997) in the MC2 model used \u03b1( j) = cos2( j\u03c0/2n) and reported good results."}
{"text": "3.5 Lateral boundary conditions for regional models Other examples of lateral boundary conditions Tatsumi (1983, 1986), following an idea of Hovermale, suggested adding an \u201cerror diffusion\u201d at the boundaries as well, which can also help to reduce the boundary errors without affecting the incoming wave:"}
{"text": "This is used in the regional spectral model of the Japan Weather Service (Tatsumi, Juang and Kanamitsu (1994) and Juang et al. (1997) also developed a RSM nested in the NCEP global spectral model, but they cast it as a perturbation model, so that the full RSM solution includes the global model solutions plus the regional perturbations."}
{"text": "They use an \u201cimplicit\u201d variation of the tendency modi\ufb01cation approach with where \u00b5 = \u03b1/T, T is an e-folding time (3 hours), and so that for the perturbation u\u2032 = u \u2212u, the implicit relaxation is given by They found that the orography of the regional model also has to be blended with the global orography in the boundary zone in order to avoid spurious noise."}
{"text": "The Eta model at NCEP (Mesinger et al., 1988, Janjic, 1994, Black, 1994) uses an \u201calmost well-posed\u201d approach. It uses boundary values from the NCEP global model only at the outermost row. When the \ufb02ow is inwards, all the prognostic variables are prescribed from the global model. At the out\ufb02ow points, the tangential velocities are extrapolated from the interior of the integration domain. The variables in the second row are a blend from the outermost and the third row. The \u201cinterior\u201d is de\ufb01ned as the third row inwards, but the Eta model uses an upstream advection scheme for the \ufb01ve outer rows of the domain in order to minimize possible re\ufb02ections at the boundary."}
{"text": "Two-way interactive boundary conditions Finally, we note that some regional models have been developed using two-way interaction in the boundary conditions, i.e., the (presumably more accurate) regional solution, in turn, also affects the global solution. Although, in principle, this would seem a more accurate approach than the one-way boundary condition, care has to be taken that the high-resolution information does not become distorted in the coarser 3 Numerical discretization of the equations of motion resolution regions, which can result in worse overall results, especially at longer time scales. There are basically two types of two-way boundary condition approaches."}
{"text": "The \ufb01rst approach corresponds to a truly nested model, with abrupt changes in the resolution, but with the inner or nested solution also used to modify the global or outer model solution. The \ufb01rst operational example of this type of two-way interaction was the NGM developed by Phillips (1979). Zhang et al. (1986) implemented two-way boundary conditions for the nesting in the MM5 model. See also Kurihara and Bender (1980), and Skamarock (1989)."}
{"text": "It is evident that with this approach, the equations in the regional high-resolution areas do not require special boundary conditions, and that they do in\ufb02uence the solutions in the regions more coarsely resolved, so that they can be considered as two-way interactive nesting. There have been a few methods used to obtain regional high resolution using stretched global coordinates:"}
{"text": "(c) A regular volume (such as a cube) projected on the sphere and then stretched (Rancic et al., 1996, Taylor et al., 1997a). A variant of this approach is the spherical geodesic grids explored somewhat unsuccessfully during the 1960s, and now again in vogue (Williamson, 1968, Sadourny et al., 1968, Masuda and Ohnishi, 1986, Heikes and Randall, 1995). The use of a regular volume to generate the grid avoids the pole problem of the convergence of the meridians in the latitude\u2013longitude grid."}
{"text": "Introduction to the parameterization of subgrid-scale physical processes In Chapter 2 we derived the equations that govern the evolution of the atmosphere, and in Chapter 3 we discussed the numerical discretizations that allow the numerical integration of those equations on a computer. The discretization of the continuous governing equation is limited by the model resolution, i.e., by the size of the smallest resolvable scale. We have seen that in a \ufb01nite difference scheme, the smallest scales of motion that can be (poorly) resolved are those which have a wavelength of two grid sizes. In spectral models, the motion of the smallest wave present in the solution is more accurately computed, but for these and for any type of numerical discretization there is always a minimum resolvable scale. Current climate models typically have a horizontal resolution of the order of several hundred kilometers, global weather forecast models have resolutions of 50\u2013100 km, and regional mesoscale models of 10\u201350 km. Storm-scale models have even higher resolution, with grid sizes of the order of 1\u201310 km. In the vertical direction, model resolution and vertical extent have also been increased substantially, with current models having typically between 10 and 50 vertical levels, and extending from the surface to the stratosphere or even the mesosphere. As computer power continues to increase, so does the resolution of Despite the continued increase of horizontal and vertical resolution, it is obvious that there are many important processes and scales of motion in the atmosphere that cannot be explicitly resolved with present or future models. They include turbulent motions with scales ranging from a few centimeters to the size of the model grid, as well as processes that occur at a molecular scale, like condensation, evaporation, 4 Introduction to the parameterization of subgrid-scale physical processes friction and radiation. We refer to all the processes that cannot be resolved explicitly as \u201csubgrid-scale processes\u201d. An example of an important process that takes place at a subgrid scale is the turbulent mixing in the planetary boundary layer. During the day- time, the solar heating at the earth\u2019s surface not only warms the soil but also causes the plants to transpire and soil moisture to evaporate, thus transporting moisture into the atmosphere. Surface heating leads to turbulent motion that is on the scale of a few me- ters to a few hundred meters. With a horizontal grid size of 10\u2013100 km, models cannot resolve these motions. Yet the transport of the heat and moisture into the boundary layer is very crucial to the development of afternoon thunderstorms and a host of other phenomena that are important to the resolvable atmospheric \ufb01elds. Another no- known to be extremely important to the global energy balance, yet each cloud typi- cally occupies only a few kilometers of space horizontally and vertically (Pan, 1999)."}
{"text": "For example, condensation of water vapor on a subgrid scale occurs if the resolved- scale humidity \ufb01eld is suf\ufb01ciently high, and, in turn, condensation releases latent heat that warms the grid-scale temperature \ufb01eld. For this reason, it is not possible to ignore the effect of the subgrid processes on the resolvable-scale \ufb01elds without degrading the quality of the forecast. To reproduce the interaction of the grid and subgrid- scale processes, the subgrid-scale phenomena are \u201cparameterized\u201d, i.e., their effect is formulated in terms of the resolved \ufb01elds. Fig. 4.1.1, adapted from Arakawa (1997), indicates schematically the resolved processes (usually referred to as the \u201cdynamics of the model\u201d), and the processes that must be parameterized (\u201cthe model physics\u201d), processes in the atmosphere and their interactions. The resolvable scales, in bold, are explicitly computed by (discussed in Chapters 2 and 3). The other subgrid-scale processes are parameterized 4.2 Subgrid-scale processes and Reynolds averaging and their interactions. Arakawa (1997) points out that some subgrid-scale processes can be interpreted as adjustment processes. For example, the atmosphere adjusts to the surface conditions through boundary layer adjustment processes, which are very ef\ufb01cient if the planetary boundary layer is unstable. Radiative \ufb02uxes occur because temperaturetendstoadjusttowardsradiativeequilibrium.Convectiveprocessesoccur in the presence of an unstable strati\ufb01cation and adjust the \ufb01eld towards a more neutrally stable state. Because radiative equilibrium is convectively unstable for the lower troposphere, radiative\u2013convective adjustment is a dominant process controlling the vertical thermal structure of the troposphere."}
{"text": "The details of the parameterizations have a profound effect on the model forecast, especially at longer time scales, and are the subject of very intense research. In this chapter we provide only a very elementary introduction to model parameterizations."}
{"text": "A short but inspiring introduction is presented in Arakawa (1997). An overview of different subgrid processes, and their parameterizations in atmospheric models appears in Haltiner and Williams (1980), and a more recent review, including ocean and land models, is available in Climate system modeling edited by Trenberth (1992)."}
{"text": "Stull(1988)andGarrart(1994)aretextsontheatmosphericboundarylayerprocesses. Emanuel and Raymond (1993) edited a volume including detailed discussions of a number of cumulus parameterizations. Pan (1999) discusses the philosophy that guides modelers in the development of parameterizations. Randall (2000) has edited a book honoring Akio Arakawa on the occasion of his retirement, which includes many review papers on areas related to physical parameterizations (as well as numerical Subgrid-scale processes and Reynolds averaging Consider the prognostic equation for water vapor written in \ufb02ux form in z-coordinates In the real atmosphere, both u and q contain scales that are resolved by the grid of the model, and smaller, subgrid scales. We write then where the overbar represents the spatial average over a grid, and the primes, the subgrid-scale perturbation. We can neglect the subgrid-scale variations of \u03c1. By de\ufb01nition, the grid-box average of all quantities linear in the perturbations is zero, e.g.,q\u2032 = 0, u\u2032q = 0. Also, averaging a grid-average quantity does not change it, e.g., uq = uq. These are the rules for Reynolds averaging, a method originally developed 4 Introduction to the parameterization of subgrid-scale physical processes by Reynolds in 1895 for use in time averages, but that we apply to grid-box averages."}
{"text": "We can substitute (4.2.2) in the moisture equation (4.2.1), take a grid average, and The \ufb01rst three terms of the right-hand side are the grid-scale (resolved) advection terms, whose numerical discretization we have studied in the Chapter 3. They are included in the \u201cdynamical processes\u201d box of Fig. 4.1.1. The next three terms are the divergences of the eddy \ufb02uxes of moisture or turbulent moisture transports. The last two terms (evaporation and condensation) are subgrid-scale processes that occur at a molecular scale and that we still need to parameterize. Both the molecular-scale processes and eddy \ufb02uxes that occur at scales much larger than molecular, but smaller than the grid resolution, are denoted collectively as \u201csubgrid-scale processes\u201d. As indicated in the introduction, the impact of at least some of these physical processes on the larger scales explicitly represented in the model must be included. Without the parameterization of at least the most important subgrid-scale processes, the model integrations cease to be realistic in a very short period, from a day or two for large- scale \ufb02ow, to less than an hour for storm-scale simulations."}
{"text": "There are several choices for the parameterization of the effect of turbulent trans- port terms in terms of the resolved scales. Consider, for example, the vertical turbulent \ufb02ux of moisture (which, because of the strong vertical gradients, especially in the planetary boundary layer, is by far the dominant component of the eddy \ufb02uxes). We (a) Neglect the vertical turbulent \ufb02ux, assuming that, in the boundary layer, the grid-scale \ufb01eld is well mixed:"}
{"text": "This is known as a \u201czeroth order\u201d closure, in which only the average properties are sought. An example is the bulk parameterization of the mixed boundary layer (Deardorff, 1972), in which the potential temperature, water vapor, and wind are assumed to be well mixed, and only the depth of the layer (b) Parameterize the vertical \ufb02ux as a \u201cturbulent diffusion process\u201d in terms of q and the other grid-scale variables (this is a \ufb01rst order closure, and is the most This represents the effect of turbulent mixing due to parcels moving up or down, bringing with them the moisture from their original level, and mixing 4.2 Subgrid-scale processes and Reynolds averaging with the environment at the new level. The main problem in \u201cK-theory\u201d, as this approach is also known, is to \ufb01nd a suitable formulation of the eddy diffusivity K, which also depends on the grid-average \ufb01elds and the stability (c) Obtain a prognostic equation for w\u2032q\u2032 by multiplying the vertical equation of motion by \u03c1q and adding it to (4.2.1) multiplied by w. We obtain an equation We can then take its Reynolds average and subtract it from (4.2.6), and derive a prognostic equation for the turbulent \ufb02uxes \u2202\u03c1w\u2032q\u2032/\u2202t = \u00b7 \u00b7 \u00b7 \u2212\u2202\u03c1w\u2032w\u2032q\u2032/ \u2202z \u00b7 \u00b7 \u00b7. This equation can be included as an additional model equation. Since it contains triple products of turbulent terms, these terms, in turn, have to be parameterized in terms of the double products:"}
{"text": "This is a second order closure. Second order closure models have many additional prognostic equations (for all the products of turbulent variables) but are an alternative to high-resolution models to obtain an estimate of turbulent transports (e.g., Moeng and Wyngaard, 1989). Mellor and Yamada (1974, 1982) show how to construct a hierarchy of closures for vertical \ufb02uxes and provide simplifying assumptions."}
{"text": "If an important physical process that occurs in the real atmosphere on a scale unre- solved by the model is not parameterized, it may still appear in the model integration \u201caliased\u201d into the resolved scales. For example, primitive equation model integrations will be ruined by dry convective instability if it is not parameterized. In the real at- mosphere, if the potential temperature decreases with height, the unstable convective circulation that takes place occurs at very small horizontal scales, of the same order as the depth of the unstable layer, typically 1 km or less. Since this cannot be resolved with horizontal grids of the order of 10\u2013100 km, models with unstable layers develop an unrealistic appearance of \u201cvertical noodles\u201d, with narrow columns moving up and down side by side. In order to handle this problem, Manabe et al. (1965) developed the dry convective adjustment, a simple parameterization of dry convection still used in most present-day models. In this parameterization, when the grid-scale atmosphere lapse rate exceeds the dry adiabatic lapse rate %d = g/C p \u224810 K/km, the unstable atmospheric column is instantaneously adjusted to an adiabatic or very slightly stable pro\ufb01le, while keeping constant the layer total enthalpy. Moist (cumulus) convection that occurs when there is grid-scale saturation and the temperature gradient exceeds the moist adiabatic lapse rate also results in a \u201cwet noodles\u201d circulation. This led to the moist convective adjustment, the \ufb01rst parameterization of cumulus convection (Manabe et al., 1965), adjusting to a moist adiabatic pro\ufb01le. The moist convective 4 Introduction to the parameterization of subgrid-scale physical processes adjustment was not found to be a suf\ufb01ciently realistic cumulus parameterization, and has since been replaced by other convective parameterizations by Kuo, 1974, Arakawa and Schubert, 1974, Betts and Miller, 1986, Kain and Fritsch, 1990. See the volume edited by Emanuel and Raymond (1993) for a detailed review of cumu- lus parameterizations, and some updates in Randall (2000). Cumulus convection is one of the most important parameterizations in determining the characteristics of the model climatology (e.g., Miyakoda and Sirutis, 1977)."}
{"text": "When a process occurs at scales not much smaller than the grid size, it presents an additional dif\ufb01culty: the resolved scales and the unresolved scales to be pa- rameterized are not well separated. An example of a process only marginally re- solved in present-day models, which therefore appears aliased into the shortest waves present in the solution, is the sea-breeze circulation. A model with a grid size of 50\u2013100 km (or more) cannot resolve the real sea-breeze circulation that takes place, for example, over a distance of the order of 1\u201320 km in the Florida penin- sula on summer days. Therefore, in the model, the sea-breeze coastal circulation becomes distorted into a 2\u0016x circulation, and because the scales are not well sep- arated, its effects on the large scales are dif\ufb01cult to parameterize. Similar effects are observed near heated mountain slopes when they are not properly resolved. The same problem of lack of scale separation complicates cumulus convection param- eterization in models with a resolution of the order of 10 km, which is close to the horizontal scale of the convection, but not high enough to resolve convection Overview of model parameterizations In a typical hydrostatic model on pressure coordinates, the governing equations (Chapter 2), including subgrid-scale processes, denoted with a tilde, are written as:"}
{"text": "for the two horizontal equations of motion, including the effect of eddy \ufb02uxes of the hydrostatic equation, 4.3 Overview of model parameterizations the rate of change of the surface pressure, and the \ufb01rst law of thermodynamics, which includes radiative heating and cooling, sensible heat \ufb02uxes and condensation and evaporation, and the conservation equation for water vapor. Condensation takes place when the grid average value oversaturates (stable or grid-scale condensation), or when there is moist convective instability and cumulus convection. The condensed water falls as precipitation, and may evaporate if the layers below are not saturated. Addi- tional conservation equations can be written for cloud and rain water in models with prognostic (rather than diagnostic) clouds, and for other substances such as In these equations the quantities with an overbar are the grid-averaged quantities computed by the model dynamics, and the terms with the tilde represent the terms that are parameterized. In a typical model, the vertical eddy \ufb02ux of momentum \u02dc\u03c4 = \u03c1w\u2032u\u2032 i + \u03c1w\u2032v\u2032 j (also known as eddy stress) of sensible heat \u02dcFT = \u03c1C pw\u2032T \u2032 and of moisture \u02dcFq = \u03c1w\u2032q\u2032 may be represented using K-theory in the boundary layer and neglected in the free atmosphere above the boundary layer (using K = 0 or a very small value). The vertical derivatives of the turbulent \ufb02uxes that appear in the right-hand sides of (4.3.1), (4.3.6), and (4.3.7) introduce a requirement for lower boundary conditions for the surface \ufb02uxes of heat, moisture, and momentum. These surface \ufb02uxes are computed using a bulk parameterization based on the Monin\u2013 Obukhov (1954) similarity theory. This theory concludes that the pro\ufb01les of wind and temperature in the turbulent surface layer can be described by a set of equations that depends only on a few parameters, including the surface roughness length z0. The hypothesis of similarity, based on many observational studies, suggests that the \ufb02uxes of momentum and heat are nearly constant with height in the surface layer (of depth 10\u2013100 m, which is much thinner than the planetary boundary layer). The \ufb02uxes in the surface or constant \ufb02ux layer are usually represented with bulk aerodynamic 4 Introduction to the parameterization of subgrid-scale physical processes Here, v, \u03b8, q are the velocity, potential temperature, and mixing ratio in the surface layer, respectively, and the variables with an S subscript are the corresponding values at the underlying ocean or land surface (vS = 0). CD, CH, and CE are transfer coef- \ufb01cients (CD is known as \u201cdrag coef\ufb01cient\u201d) and they depend on the stability of the surface layer (measured by the bulk Richardson number RiB = gz the height z and the surface roughness length). They are nondimensional and have typical values of the order of 10\u22123 for stable conditions and 10\u22122 for unstable con- ditions (Louis, 1979). \u03b2 is a coef\ufb01cient representing the degree of saturation of the underlying surface (1 for oceans, 0\u20131 for land depending on the degree of saturation in the soil moisture content). The surface layer values are either obtained through the use of a thin (order 10 m) prognostic layer or diagnosed."}
{"text": "The radiative heating in (4.3.6) is determined from the vertical divergence of the upward and downward \ufb02uxes of short- and long-wave radiation, obtained using the radiative transfer equation. See Kiehl (1992) for a review of the parameterization of radiation. The interaction between clouds and radiation is very complex, and is a major area of research. Early models speci\ufb01ed clouds climatologically (Manabe et al., 1965). In the 1980s the cloud cover was speci\ufb01ed diagnostically, based on relative humidity (Slingo, 1987, Campana, 1994). More recently, cloud and rain water were predicted using budget equations and cloud cover was deduced from the amount of cloud water (e.g., Zhao et al., 1997). The cloud properties are also important: rather than plane slabs, as generally assumed, clouds have a fractal structure, which effec- tively reduces their albedo and increases atmospheric absorption of solar radiation An important area of research is the effect of subgrid-scale mountains. Wallace et al. (1983) proposed representing the blocking effect of subgrid-scale hills and valleys by increasing the effective height of mountains above its grid average by a factor of order one times the standard deviation of the subgrid-scale orography. This approach has been denoted \u201cenvelope orography\u201d. Similarly, Mesinger et al. (1988) chose a method essentially that de\ufb01nes the grid mountain height by the tallest peaks (\u201csilhouette orography\u201d). Lott and Miller (1997) formulated a new parameterization using developments in the nonlinear theory of strati\ufb01ed \ufb02ows around obstacles, pay- ing special attention to the parameterization of the blocked \ufb02ow when the effective height of the subgrid-scale orography is high enough. They showed that this method can duplicate the results using envelope orography. In addition to its blocking effect, under stable conditions, small-scale orography generates internal gravity waves that propagate upwards, increase their amplitude, and eventually break at upper levels, depositing their low-level momentum (Lilly and Kennedy, 1973). The net result is 4.3 Overview of model parameterizations a deceleration due to surface orography at upper levels. Modelers have introduced a gravity-wave parameterization following Palmer et al. (1986), McFarlane (1987), and Lindzen (1988). Kim and Arakawa (1995) developed a parameterization of the drag due to gravity waves."}
{"text": "Other areas of research in the parameterization of subgrid processes are related to the fact that the underlying surfaces (ocean and land) have their own evolution and therefore provide a \u201clonger memory\u201d to the forecast model which cannot be represented diagnostically. Equation (4.3.8) indicates that over ocean it is necessary to know the surface stress \u03c4 and the SST. Short-range forecasts are performed with observed SSTs, under the assumption that they do not change signi\ufb01cantly with time, but this is clearly not a reasonable assumption for medium-range or longer forecasts (e.g., Pe\u02dcna et al., 2002)."}
{"text": "For seasonal and interannual predictions, the SST is predicted using an ocean model coupled to the atmospheric model (Ji et al., 1994, Trenberth, 1992). In addition, the surface \ufb02uxes over the ocean depend on the surface waves, which are driven by the wind. Currently most models use the Charnock (1955) parameterization relating an effective roughness length to the surface stress. In an iterative procedure the stress and the roughness length are obtained and bulk-aerodynamical formulas used to deduce the sensible and latent heat \ufb02uxes. However, in reality, ocean waves have a memory of their previous interactions with the atmosphere: swell (\u201cold sea\u201d) is smoother than \u201cnew sea\u201d where waves are driven by sudden changes in the wind, and, in turn, this affects the surface stress and the \ufb02uxes of heat and moisture. To take this effect into account, it is necessary to couple atmospheric models with ocean wave models."}
{"text": "Over land, similarly, the surface \ufb02uxes of heat and moisture are strongly depen- dent on the vegetation and soil moisture. Older models followed Manabe et al. (1965) by representing the effect of available soil moisture with a simple 15-cm \u201cbucket\u201d model, whose content was reduced by evaporation and increased by precipitation, with over\ufb02ow representing river runoff. The surface temperature was obtained diag- nostically assuming zero heat capacity for the land. Current models include coupling the atmospheric model with multilevel soil models with prognostic equations for the soil temperature and moisture, and include the very important controlling effect of plants on evapotranspiration (see reviews by Sellers (1992), Dickinson (1992), Pan In previous chapters we saw that NWP is an initial/boundary value problem: given an estimate of the present state of the atmosphere (initial conditions), and appropriate surface and lateral boundary conditions, the model simulates (forecasts) the atmo- spheric evolution. Obviously, the more accurate the estimate of the initial conditions, the better the quality of the forecasts. Currently, operational NWP centers produce initial conditions through a statistical combination of observations and short-range forecasts. This approach has become known as \u201cdata assimilation\u201d, whose purpose is de\ufb01ned by Talagrand (1997) as \u201cusing all the available information, to determine as accurately as possible the state of the atmospheric (or oceanic) \ufb02ow.\u201d There are several excellent reviews of this subject, which has become an im- portant science in itself. The book Atmospheric data analysis by Daley (1991) is a comprehensive description of methods for atmospheric data analysis and assim- ilation. Ghil and Malanotte-Rizzoli (1991) have written a rigorous discussion of present data assimilation methods with special emphasis on sequential methods. Ta- lagrand (1997) gives an elegant introductory overview of current methods of data assimilation, and Zupanski and Kalnay (1999) also provide a short introduction to the subject. The book Data assimilation in meteorology and oceanography: Theory and practice (Ghil et al., editors, 1997) contains a wealth of important papers on current methods for data assimilation. An earlier but still useful book is Dynamic meteorology: Data assimilation methods (Bengtsson et al., editors, 1981). Thiebaux and Pedder (1987) provided a description of spatial interpolation methods applied to meteorology. Several workshops on data assimilation have taken place at ECMWF, and their proceedings are extremely useful."}
{"text": "In the early NWP experiments, Richardson (1922) and Charney et al. (1950) performed hand interpolations of the available observations to a regular grid, and these \ufb01elds of initial conditions were then manually digitized, which was a very time consumingprocedure.Theneedforanautomatic\u201cobjectiveanalysis\u201dbecamequickly apparent (Charney, 1951), and interpolation methods \ufb01tting observations to a regular grid were soon developed. Panofsky (1949) developed the \ufb01rst objective analysis algorithm based on two-dimensional polynomial interpolation, a procedure that can be considered \u201cglobal\u201d since the same function is used to \ufb01t all the observations."}
{"text": "z(x, y) = a00 + a10x + a01y + a20x2 + a11xy + a02y2 The six coef\ufb01cients were determined by minimizing the mean square difference between the polynomial and observations close to the grid point (within a radius of in\ufb02uence of the grid point):"}
{"text": "of grid points (circles), observations (squares), and a radius of in\ufb02uence around a grid point i marked with the grid-point analysis is a combination of the forecast guess) and the observational minus \ufb01rst guess) computed at the observational points k."}
{"text": "In certain analysis schemes, like SCM, only observations of in\ufb02uence, indicated by a circle, affect the analysis Here pk, qk are empirical weighting coef\ufb01cients, and ug, vg are the geostrophic wind components computed from the gradient of the geopotential height z(x, y) at the observation point k, and K is the total number of observations within the radius of in\ufb02uence. Note that although the \ufb01eld being analyzed is just the geopotential height, the wind observations are useful as well because they provide additional information However, for operational primitive equation models, it is not enough to perform spatial interpolation of observations into regular grids, because not enough data are available to initialize current models. As pointed out in the introduction, the num- ber of degrees of freedom in a modern NWP model is of the order of 107, whereas the total number of conventional observations of the variables used in the mod- els (e.g., from rawinsondes) is of the order of 104. There are many new types of data currently available, including remotely sensed data such as satellite and radar observations, but they do not measure directly the variables used in the models (wind, temperature, moisture, and surface pressure). Moreover, their distribution in space and time is very nonuniform (Fig. 1.4.1), with regions like North America and Eurasia that are relatively data-rich, and others that are much more poorly For this reason, it became clear rather early in the history of NWP that, in addition to the observations, it was necessary to have a complete \ufb01rst guess estimate of the state of the atmosphere at all the grid points in order to generate the initial conditions for the forecasts (Bergthorsson and D\u00a8o\u00a8os, 1955). The \ufb01rst guess (also known as background \ufb01eld or prior information) should be our best estimate of the state of the atmospherepriortotheuseoftheobservations.Initiallyclimatology,oracombination of climatology and a short forecast were used as a \ufb01rst guess (e.g., Gandin, 1963, Bergthorsson and D\u00a8o\u00a8os, 1955). As forecasts became better, the use of short-range forecasts as a \ufb01rst guess was universally adopted in operational systems in what is called an \u201canalysis cycle\u201d (Fig. 5.1.2)."}
{"text": "The analysis cycle is an intermittent data assimilation system that continues to be used in most global operational systems, which typically use a 6-h cycle per- formed four times a day. The model forecast plays a very important role. Over data-rich regions, the analysis is dominated by the information contained in the observations. In data-poor regions, the forecast bene\ufb01ts from the information up- stream. For example, 6-h forecasts over the North Atlantic Ocean are very good, even in the absence of satellite data, because of the information coming from North America. The forecast is thus able to transport information from data-rich to data-poor areas, and for this reason, data assimilation using a short-range fore- cast as a \ufb01rst guess has become known as four-dimensional data assimilation In Section 5.2 we describe empirical analysis schemes (SCM and nudging), and Sections 5.3 et seq. are devoted to statistical interpolation schemes."}
{"text": "Global analysis (statistical interpolation) and balancing Regional analysis (statistical interpolation) and balancing 18 UTC. The observations should be valid for the same time as the \ufb01rst guess. In the global analysis this has usually meant the rawinsondes are launched mostly at the main observing times (00 and 12 UTC), and satellite data are lumped into windows centered at the main observing times. The observations can be direct observations of variables used by the model, or indirect observations of geophysical parameters, such as radiances, that depend on the variables used in the model. (b) Typical regional analysis cycle. The main difference with the global cycle is that boundary conditions coming from global forecasts are an additional requirement for the regional forecasts."}
{"text": "Empirical analysis schemes Successive corrections method (SCM) The \ufb01rst analysis method used in 4DDA was based on an empirical approach known as the SCM, developed by Bergthorsson and Doos (1955) in Sweden and by Cressman (1959) of the US Weather Service. In SCM the \ufb01rst estimate of the gridded \ufb01eld is given by the background (or \ufb01rst guess) \ufb01eld:"}
{"text": "After this \ufb01rst estimate, the following iterations are obtained by \u201csuccessive cor- i is the nth iteration estimation at the grid point i, f O surrounding the grid point i, f n k is the value of the nth \ufb01eld estimate evaluated at the observation point k (obtained by interpolation from the surrounding grid points), and \u03b52 is an estimate of the ratio of the observation error variance to the background error variance. The weights wn ik can be de\ufb01ned in different ways. Cressman (1959) de\ufb01ned the weights in the SCM as ik is the square of the distance between an observation point rk and a grid The radius of in\ufb02uence Rn is allowed to vary with the iteration, and K n number of observations within a distance Rn of the grid point i. For example, in the 1980s the Swedish operational system used R1 = 1500 km, R2 = 900 km for upper air analyses, and R1 = 1500 km, R2 = 1200 km, R3 = 750 km, R4 = 300 km for the surface pressure analysis. The reduction of the radius of in\ufb02uence results in a \ufb01eld that re\ufb02ects the large scales after the \ufb01rst iteration and converges towards the smaller scales after the additional iterations."}
{"text": "In the Cressman SCM, the coef\ufb01cient \u03b52 is assumed to be zero. This results in a \u201ccredulous\u201danalysisthatmorefaithfullyre\ufb02ectstheobservations,andforaverysmall radius of in\ufb02uence the analysis converges to the observation values if the observations 5.2 Empirical analysis schemes are located at the grid points. If the data are noisy (e.g., if an observation has gross errors, or if it contains an unrepresentative sample of subgrid-scale variability), this can lead to \u201cbull\u2019s eyes\u201d (many isolines around an unrealistic grid-point value) in the analysis. Including \u03b52 > 0 assumes that the observations have errors, and gives some weight to the background \ufb01eld."}
{"text": "Barnes (1964, 1978) developed another empirical version of the SCM that has been widely used for analyses where there is no available background or \ufb01rst guess \ufb01eld, such as the analysis of radar data or other small-scale observations. Since we have no information on the background \ufb01eld, its error variance can be considered to be very large, so that \u03b52 = 0. The weights are given by wn in\ufb02uence are changed by a constant factor at each iteration: R2 only the large scales are captured. For \u03b3 < 1 more details in the observations are reproduced in the analysis as more iterations are performed."}
{"text": "Although the SCM method is empirical, it is simple and economical, and it pro- vides reasonable analyses. Bratseth (1986) showed that if the weights are chosen appropriately instead of using the empirical formulas presented above, the SCM can be made to converge to a proper statistical interpolation (OI) (Section 5.3)."}
{"text": "Another empirical and fairly widely used method for data assimilation is Newtonian relaxation or nudging (Hoke and Anthes, 1976, Kistler, 1974). This consists of adding to the prognostic equations a term that nudges the solution towards the observations (interpolated to the model grid). For example, for a primitive equation model, the zonal velocity forecast equation is written as and similarly for the other equations."}
{"text": "The relaxation time scale, \u03c4, is chosen based on empirical considerations and may depend on the variable. If \u03c4 is very small, the solution converges towards the observations too fast, and the dynamics do not have enough time to adjust. If \u03c4 is too large, the errors in the model can grow too much before the nudging becomes effective. Hoke and Anthes indicated that \u03c4 should be chosen so that the last term is similar in magnitude to the less dominant terms. They used a very short time scale, about 20 minutes, in their experiments. Stauffer and Seaman (1990) used about one hour in experiments assimilating synoptic observations, and reported a fair amount of success. Zou et al. (1992) made optimal parameter estimations of the nudging time scale. Kaas et al. (1999) performed an interesting experiment, nudging a model towards a 15-y reanalysis from the ECMWF, and by averaging the mean forcing introduced by nudging, empirically determined corrections to reduce model Although this method is not generally used for large-scale assimilation, some groups use it for assimilating small-scale observations (e.g., radar observations) when there are no available statistics to perform a statistical interpolation."}
{"text": "Assimilation of meteorological or oceanographical observations can be described as the process through which all the available information is used in order to estimate as accurately as possible the state of the atmospheric or oceanic \ufb02ow. The available information essentially consists of the observations proper, and of the physical laws that govern the evolution of the \ufb02ow. The latter are available in practice under the form of a numerical model. The existing assimilation algorithms can be described as either sequential or variational."}
{"text": "Inthissectionwegive\u201cbabyexamples\u201dofbothsequentialandvariationalapproaches. The methodology and results derived from this simple case carry over to multivariate OI, Kalman \ufb01ltering, and 3D-Var and 4D-Var assimilation."}
{"text": "The best estimate of the state of the atmosphere (analysis) is obtained, as indicated by Talagrand (1997), from combining prior information about the atmosphere (back- ground or \ufb01rst guess) with observations, but in order to combine them optimally we also need statistical information about the errors in these \u201cpieces of information.\u201d A classic example of determining the best estimate of the true value of a scalar (e.g., the true temperature Tt) given two independent observations (or pieces of information), T1 and T2, serves as an introduction to statistical estimation:"}
{"text": "Equations (5.3.2), (5.3.3) and (5.3.4) represent the statistical information that we need about the actual observations. We try to estimate Tt from a linear combination of the two observations since they represent all the information that we have about The \u201canalysis\u201d Ta should be unbiased:"}
{"text": "Ta will be the best estimate of Tt if the coef\ufb01cients are chosen to minimize the mean a = E[(Ta \u2212Tt)2] = E[(a1(T1 \u2212Tt) + a2(T2 \u2212Tt))2] subject to the constraint (5.3.7). Substituting a2 = 1 \u2212a1, the minimization of \u03c3 2 i.e., the weights of the observations are proportional to the \u201cprecision\u201d or accuracy of the measurements (de\ufb01ned as the inverse of the variances of the observational errors)."}
{"text": "i.e., if the coefficients are optimal, and the statistics of the errors are exact, then the \u201cprecision\u201d of the analysis (de\ufb01ned as the inverse of the variance) is the sum of the precisions of the measurements."}
{"text": "Exercise 5.3.1: Derive equations (5.3.9), (5.3.10), and (5.3.11). Variational (cost function) approach We can also obtain the same best estimate of Tt by minimizing a function of the temperature de\ufb01ned as the sum of the square of the distance (or mis\ufb01t) of the estimate T to the two observations, weighted by their observational error precisions:"}
{"text": "One may ask the motivation for de\ufb01ning a cost function as in (5.3.12). We now show that (5.3.12) can be formulated using the maximum likelihood approach, where we ask the question: Given the two independent observations T1 and T2, which are assumed to have normally distributed errors with standard deviations \u03c31 and \u03c32, what is the most likely value of the true temperature T ? We de\ufb01ne the analysis as the most likely value of T given the observations and their statistical errors."}
{"text": "The probability distribution of an observation T1 given a true value T and an observational standard deviation \u03c31, is given by the gaussian distribution Conversely, the likelihood (Edwards, 1984) of a true value T given an observation T1 with a standard deviation \u03c31 is given by L\u03c31(T ||T1) = p\u03c31(T1|T ) = Similarly, the likelihood of a true value T given an observation T2 with a standard L\u03c32(T ||T2) = p\u03c32(T2|T ) = Therefore the most likely value of T given the two independent measurements T1 and T2 is the one that maximizes the joint probability, i.e., their product:"}
{"text": "Alternatively (Purser, 1984), the Bayesian derivation of (5.3.12) assumes we made the observation T1 (the background forecast in the data assimilation prob- lem), which implies a prior probability distribution of the truth pT1,\u03c31(T ) = 1 , prior to the second observation. Then Bayes formula for the a posteriori probability of the truth given observation T2 is p\u03c32(T |T2) = p\u03c32(T2|T )pT1,\u03c31(T ) is independent of T . The estimate of the truth that maximizes the a posteriori prob- ability (5.3.14) is obtained by maximizing the logarithm of the numerator, and is given, once again, by the minimum of the cost function (5.3.12)."}
{"text": "Note that the control variable for the minimization of (5.3.12) (i.e., the variable with respect to which we are minimizing the cost function J) is now the temperature itself, not the weights. The equivalence between the minimization of the analysis error variance (\ufb01nding the optimal weights through a least squares approach), and the variational cost function approach (\ufb01nding the optimal analysis that minimizes the distance to the observations weighted by the inverse of the error variance) is an important property. This equivalence also holds true for multidimensional problems (in which case we use the covariance matrix rather than the scalar variance), and it indicates that OI (Gandin, 1963) and 3D-Var (e.g., Sasaki, 1970, Parrish and Derber, 1992) are solving the same problem (Lorenc, 1986)."}
{"text": "analysis (the most likely value of the truth that maximizes the joint probability of T 1 and T 2) has a probability distribution with a maximum closer to T 2, and a smaller standard deviation (higher precision) than either observation."}
{"text": "Simplest sequential assimilation and Kalman \ufb01ltering This is a prototype of the full multivariate OI. Assume that one of the two pieces of information T1 = Tb is the forecast (or any other \u201cbackground\u201d value) and the other analysis T, given observations T1 and T2, using either the least squares approach or the Bayesian approach (after Purser, 1984)."}
{"text": "is an observation T2 = To. From (5.3.5) and (5.3.10), we can write the analysis as where (To \u2212Tb) is de\ufb01ned as the observational \u201cinnovation\u201d , i.e., the new infor- mation brought by the observation. It is also known as the observational increment (with respect to the background); W is the optimal weight, given by and the analysis error variance is, as before, The analysis variance can in turn be written as \u03c3 2 Exercise 5.3.3: Derive equations (5.3.15)\u2013(5.3.18)."}
{"text": "5.3 Introduction to least squares methods Equations (5.3.15)\u2013(5.3.18) have been derived for the simplest scalar case, but they are important for the problem of data assimilation because they have exactly the same form as the least squares sequential estimation methods used for real multidimen- sional problems (OI, interpolation, 3D-Var and even Kalman \ufb01ltering). Therefore we interpret these equations in detail:"}
{"text": "Equation (5.3.15) says: \u201cThe analysis is obtained by adding to the \ufb01rst guess (background) the innovation (difference between the observation and \ufb01rst guess) weighted by the optimal weight.\u201d Equation (5.3.16) says: \u201cThe optimal weight is the background error variance multiplied by the inverse of the total error variance (the sum of the background and the observation error variances).\u201d Note that the larger the background error variance, the larger the correction to the \ufb01rst guess."}
{"text": "Equation (5.3.17) says: \u201cThe precision of the analysis (inverse of the analysis error variance) is the sum of the precisions of the background and the Equation (5.3.18) says: \u201cThe error variance of the analysis is the error variance of the background, reduced by a factor equal to one minus the All these statements are important because they also hold true for sequential data assimilation systems (OI and Kalman \ufb01ltering) for multidimensional problems. In these problems, in which Tb and Ta are three-dimensional \ufb01elds of size order 107 and To is a set of observations (typically of size 105 or 106), we have to replace the expression \u201cerror variance\u201d by \u201cerror covariance matrix\u201d, and the \u201coptimal weight\u201d by an \u201coptimal gain matrix\u201d."}
{"text": "Note also from (5.3.16) that there is one essential \u201ctuning\u201d parameter in OI: the ratio of the a priori estimate of the observational to the background error variances Moreover, if the background is a forecast, we can use equations (5.3.15), (5.3.16), and (5.3.18) to create a simple sequential \u201canalysis cycle\u201d, in which the observation is used once at the time it appears and then discarded. Assume that we have completed the analysis at time ti (e.g., at 12 UTC), and we want to proceed to the next cycle (time ti+1, or 18 UTC in the example). The analysis cycle has two phases, a forecast phase to update the background Tb and its error variance \u03c3 2 b , and an analysis phase, to update the analysis Ta and its error variance \u03c3 2 In the forecast phase of the analysis cycle, the background is \ufb01rst obtained through where M representsaforecastmodel(whichcouldbeadynamicalmodel,persistence, climatology, extrapolation, etc.). We also need to estimate the error variance of the background. In OI, this is done by making some suitable simple assumption, such as that the model integration increases the initial error variance by a \ufb01xed amount, e.g., a factor a not much greater than 1 (such as 1.5 or 2)."}
{"text": "This allows the new weight W(ti+1) to be estimated using (5.3.16). In Kalman \ufb01ltering, (5.3.19) is the same as in OI, but instead of assuming a value b (ti+1) as in (5.3.20) we compute the forecast error covariance using the forecast model itself. If we applied the model (5.3.19) to update the true temperature, there would be an error, since the model is not perfect: Tt(ti+1) = M [Tt(ti)] \u2212\u03b5M."}
{"text": "The model error is assumed to be unbiased (unfortunately this is not in general a good assumption) with an error variance Q2 = E(\u03b52 \u03b5b,i+1 = (Tb \u2212Tt)i+1 = M(Ta)i \u2212M(Tt)i + \u03b5M = M\u03b5a,i + \u03b5M whereM= \u2202M/\u2202T isthelinearizedortangentlinearmodeloperator,andtheforecast for the background error covariance at the new time level is:"}
{"text": "Exercise 5.3.4: Derive (5.3.21) and (5.3.22). Intheanalysisphaseoftheanalysiscycle(forbothOIandKalman\ufb01ltering)wegetthe new observation To(ti+1), and we derive the new analysis Ta(ti+1) using (5.3.15), b from either (5.3.20) for OI, or (5.3.22) for Kalman \ufb01ltering, and the new analysis error variance \u03c3 2 a (ti+1) using (5.3.18). After the analysis, the cycle for time ti+1 is completed, and we can proceed to the next cycle."}
{"text": "In general, we cannot directly observe the model variables that we want to ana- lyze (i.e., temperature, moisture, wind, and surface pressure at the grid points of the model). Instead we have rawinsondes (which were designed to provide these de- sirable variables) but at locations which are different from the analysis grid points, so that we have to perform horizontal and vertical interpolations. A more complex problem is that we may have remote sensing instruments (like satellites and radars) that measure quantities in\ufb02uenced by the desired variables, like radiances, re\ufb02ectivi- ties, refractivities, and Doppler shifts, rather than the variables themselves. Typically, then, we have to use an observation operator H(Tb) (also known as an observational forward operator) to obtain from the \ufb01rst guess gridded \ufb01eld a \ufb01rst guess of the observations. The observation operator H includes spatial interpolations (or spectral to physical space transformation) from the \ufb01rst guess to the location of the observa- tions. It also includes transformations based on physical laws, such as the radiative 5.4 Multivariate statistical data assimilation methods transfer equations that go from a model vertical pro\ufb01le of temperature and moisture to \u201cobserved\u201d \ufb01rst guess satellite radiances."}
{"text": "Instead of using the observation operator, the operational assimilation of remotely sensed data used to be done following the \u201cretrieval\u201d approach. For example, TOVS (TIROS-N Operational Vertical Sounder) is an instrument that measures radiances in the infrared and microwave range of the spectrum. The forward operator model H(T,q,clouds) was inverted by \ufb01rst \ufb01ltering the clouds and then retrieving \u201cobserved\u201d pro\ufb01les of temperature and moisture, T (p) and q(p). The retrieved pro\ufb01les (that \u201clooked\u201d like rawinsonde observations) were then assimilated into the models. As indicated in Chapter 1, the direct assimilation of radiances, using the forward obser- vational model H to convert the \ufb01rst guess into \u201c\ufb01rst guess TOVS radiances\u201d and then the assimilation of the \u201cradiances innovations\u201d (observed minus \ufb01rst guess radi- ances) has resulted in major improvements in the forecasts in both hemispheres (e.g., Fig. 1.4.3). This will be discussed further in Section 5.5, but we remark here that the improvements obtained by direct assimilation of the radiances are due basically to (1) There are fewer independent radiance observations than vertical levels of T and q in the model, which means that the problem of deriving a \u201cretrieval\u201d using only radiances is underdetermined. Therefore, in order to \u201cretrieve\u201d (invert the forward observational operator) it is necessary to introduce additional and less accurate statistical information into the problem. The introduction of this ancillary information (usually based on climatology, and generally less accurate than a short-range forecast used as a \ufb01rst guess) is unnecessary with the direct assimilation of radiance innovations."}
{"text": "(2) The observation error covariance of the retrieved T and q pro\ufb01les is very dif\ufb01cult to determine, since it involves strong error correlations among retrievals in different latitude and longitude locations introduced by the use of ancillary information. On the other hand, observed radiances have \u201ccleaner\u201d error covariances, since they depend only on instrument errors, and not on how the data were processed. As a result, the observational error covariance for the radiances is usually diagonal."}
{"text": "Multivariate statistical data assimilation methods We now generalize the least squares method to obtain the OI equations for vectors of observations and background \ufb01elds. These equations were derived originally by Eliassen (1954, reproduced in Bengtsson et al., 1981). However, Gandin (1963) derived the multivariate OI equations independently and applied them to objective analysis in the Soviet Union. Gandin\u2019s work had a profound in\ufb02uence upon the research and operational community, and OI became the operational analysis scheme of choice during the 1980s and early 1990s. In this discussion we generally follow the notation proposed by Ide et al. (1997) for data assimilation methods. This short paper, although ostensibly devoted to notation, is also an excellent overview of data assimilation. Later in this section we show that 3D-Var is equivalent to the OI method, although the method for solving it is quite different and advantageous for operational Optimal interpolation (OI) In Section 5.3 we studied the formulation of the \u201coptimal\u201d analysis of a scalar at a single point. We now consider the complete NWP operational problem of \ufb01nding an optimum analysis of a \ufb01eld of model variables xa, given a background \ufb01eld xb available at grid points in two or three dimensions, and a set of p observations yo available at irregularly spaced points ri (Fig. 5.1.1)."}
{"text": "The unknown analysis and the known background can be two-dimensional \ufb01elds of a single variable like the temperature analysis Ta(x, y), or the three-dimensional \ufb01eldoftheinitialconditionsforallthemodelprognosticvariables:x=(ps, T, q, u, v)."}
{"text": "These model variables are ordered by grid point and by variable, forming a single vector of length n, where n is the product of the number of points times the number of variables. The (unknown) \u201ctruth\u201d xt, discretized at the model points, is also a vector Note that we have used a different variable yo for the observations than for the \ufb01eld we want to analyze. This is to emphasize that the observed variables are, in general, different from the model variables by: (a) being located in different points, and (b) possibly being indirect measures of the model variables. Examples of these measurements are radar re\ufb02ectivities and Doppler shifts, satellite radiances, and global positioning system (GPS) atmospheric refractivities."}
{"text": "As we did in (5.3.15) for a scalar, the analysis is cast as the background plus the innovation weighted by optimal weights which we will determine from statistical xt \u2212xb = W[yo \u2212H(xb)] \u2212\u03b5a = Wd \u2212\u03b5a but now the truth, the analysis, and the background are vectors of length n (the total number of grid points times the number of model variables) and the weights are given by a matrix of dimension (n \u00d7 p). The forward observational operator H converts the background \ufb01eld into \u201c\ufb01rst guesses of the observations.\u201d H can be nonlinear (e.g., the radiative transfer equations that go from temperature and moisture vertical pro\ufb01les to the satellite observed radiances). The observation \ufb01eld yo is a vector of length p, the number of observations. The vector d, also of length p, is the \u201cinnovation\u201d or 5.4 Multivariate statistical data assimilation methods \u201cobservational increments\u201d vector:"}
{"text": "where the overbar represents the expected value (i.e. is the same as E( )). A covariance matrix is symmetric and positive de\ufb01nite. The diagonal elements are the variances of the vector error components eiei = \u03c3 2 the covariance matrix, dividing each component by the product of the standard deviations eie j/\u03c3i\u03c3 j = corr(ei, e j) = \u03c1i j, we obtain a correlation is the diagonal matrix of the variances, then we can write (c) The transpose of matrix products is given by the product of the transposes, but in reverse order: [AB]T = BT AT ; a similar rule applies to the inverse of a (d) The general form of a quadratic function is F(x) = 1 A is a symmetric matrix, d is a vector and c a scalar. To \ufb01nd the gradient of this scalar function \u2207xF = \u2202F/\u2202x (a column vector) we use the following properties of the gradient with respect to x: \u2207(dT x) = \u2207(xT d) = d (since \u2207xxT = I, the identity matrix), and \u2207(xT Ax) = 2Ax. Therefore, (e) Multiple regression or best linear unbiased estimation (BLUE)."}
{"text": "Assume we have two time series of vectors centered about their mean value, E(x) = 0, E(y) = 0, i.e., vectors of anomalies. We derive now the best linear unbiased estimation of x in terms of y, i.e., the optimal value of the weight matrix W in the multiple linear which approximates the true relationship Here \u03b5(t) = xa(t) \u2212x(t) is the linear regression (\u201canalysis\u201d) error, and W is an n \u00d7 p matrix that minimizes the mean squared error E(\u03b5T \u03b5). To derive W we write the regression equation matrix components explicitly:"}
{"text": "5.4 Multivariate statistical data assimilation methods and the derivative with respect to the weight matrix components is wik yk(t)yi(t) \u2212xi(t)y j(t) so that if we take a long time mean, and choose W to minimize the mean squared error, we get the normal equation which gives the best linear unbiased estimation xa(t) = Wy(t)."}
{"text": "We de\ufb01ne the background error and the analysis error as vectors of length n: \u03b5\u03b5\u03b5b(x, y) = xb(x, y) \u2212xt(x, y) \u03b5\u03b5\u03b5a(x, y) = xa(x, y) \u2212xt(x, y) The p observations available at irregularly spaced points yo(ri) have observational \u03b5\u03b5\u03b5oi = yo(ri) \u2212yt(ri) = yo(ri) \u2212H[xt(ri)] We don\u2019t know the truth xt, thus we don\u2019t know the errors of the available background and observations, but we can make a number of assumptions about their statistical properties. The background and observations are assumed to be unbiased:"}
{"text": "E{\u03b5\u03b5\u03b5b(x, y)} = E{xb(x, y)} \u2212E{xt(x, y)} = 0 E{\u03b5\u03b5\u03b5o(ri)} = E{yo(ri)} \u2212E{yt(ri)} = 0 If the forecasts (background) and the observations are biased, in principle we can and should correct the bias before proceeding. Dee and Da Silva (1998) show how the model bias can actually be estimated as part of the analysis cycle."}
{"text": "We de\ufb01ne the error covariance matrices for the analysis, background and obser- The nonlinear observation operator H that transforms model variables into observed variables can be linearized as where H is a p \u00d7 n matrix, denoted the linear observation operator with elements hi, j = \u2202Hi/\u2202x j. We also assume that the background (usually a model forecast) is a good approximation of the truth, so that the analysis and the observations are equal to the background values plus small increments. Therefore, the innovation vector (5.4.2) can be written as d = yo \u2212H(xb) = yo \u2212H(xt + (xb \u2212xt)) = yo \u2212H(xt) \u2212H(xb \u2212xt) = \u03b5\u03b5\u03b5o \u2212H\u03b5\u03b5\u03b5b The H matrix transforms vectors in model space into their corresponding values in observation space. Its transpose or adjoint HT transforms vectors in observation space to vectors in model space."}
{"text": "The background error covariance B (a matrix of size n \u00d7 n) and the observation error covariance R (a matrix of size p \u00d7 p) are assumed to be known. In addition, we assume that the observation and background errors are uncorrelated:"}
{"text": "From (5.4.16), d = yo \u2212H(xb) = \u03b5\u03b5\u03b5o \u2212H\u03b5\u03b5\u03b5b, and from (5.4.10) the optimal weight matrix W (also known as the gain matrix K) that minimizes \u03b5\u03b5\u03b5T W = E{(xt \u2212xb)[yo \u2212H(xb)]T }(E{[yo \u2212H(xb)][yo \u2212H(xb)]T })\u22121 = E[(\u2212\u03b5\u03b5\u03b5b)(\u03b5\u03b5\u03b5o \u2212H\u03b5\u03b5\u03b5b)T ]{E[(\u03b5\u03b5\u03b5o \u2212H\u03b5\u03b5\u03b5b)(\u03b5\u03b5\u03b5o \u2212H\u03b5\u03b5\u03b5b)T ]}\u22121 Recall that in (5.4.17) we assumed that the background errors are not correlated with the observational errors, i.e., that their covariance is equal to zero. Substituting the de\ufb01nitions of background error covariance B and observational error covariance R 5.4 Multivariate statistical data assimilation methods (5.4.14) into (5.4.18) we obtain the optimal weight matrix Finally, we derive the analysis error covariance."}
{"text": "For convenience, we repeat the basic equations of OI, and express in words their interpretation, which is similar to that for a scalar least square problem from the last xa = xb + W[yo \u2212H(xb)] = xb + Wd We will see in Section 5.5 (where we derive the variational approach or 3D-Var) that the weight matrix (5.4.19) can be written in an alternative equivalent form as W = (B\u22121 + HT R\u22121H)\u22121HT R\u22121 (see (5.5.11) in Section 5.5) where the subscript n is a reminder that the identity matrix is in the analysis or model The interpretation of these equations is very similar to the scalar case discussed Equation (5.4.1) says: \u201cThe analysis is obtained by adding to the \ufb01rst guess (background) the product of the optimal weight (or gain) matrix and the innovation (the difference between the observation and the \ufb01rst guess)."}
{"text": "The \ufb01rst guess of the observations is obtained by applying the observation operator H to the background vector.\u201d Also, note that from (5.4.15), H(xb) = H(xt) + H(xb \u2212xt) = H(xt) + H\u03b5\u03b5\u03b5b, where the matrix H is the linear tangent perturbation of H."}
{"text": "Equation (5.4.19a) says: \u201cThe optimal weight (or gain) matrix is given by the background error covariance in the observation space (BHT ) multiplied by the inverse of the total error covariance (the sum of the background and the observation error covariances).\u201d Note that the larger the background error covariance compared with the observation error covariance, the larger the correction to the \ufb01rst guess."}
{"text": "Equation (5.4.20) says: \u201cThe error covariance of the analysis is given by the error covariance of the background, reduced by a matrix equal to the identity matrix (n \u00d7 n) minus the optimal weight matrix.\u201d Finally we derive an alternative formulation for the analysis error covariances show- ing(asinthescalarcase)theadditivepropertiesofthe\u201cprecisions\u201d(ifallthestatistical assumptions hold true). From (5.4.1), (5.4.16), and (5.4.19b) we can show that \u03b5\u03b5\u03b5a = \u03b5\u03b5\u03b5b + [B\u22121 + HT R\u22121H]\u22121HT R\u22121(\u03b5\u03b5\u03b50 \u2212H\u03b5\u03b5\u03b5b) = [B\u22121 + HT R\u22121H]\u22121[B\u22121\u03b5\u03b5\u03b5b + HT R\u22121\u03b5\u03b5\u03b5o] If we again compute Pa = E{\u03b5\u03b5\u03b5a\u03b5\u03b5\u03b5T a } from (5.4.21), and make use of E{\u03b5\u03b5\u03b5b\u03b5\u03b5\u03b5T Equation (5.4.22) says: \u201cThe analysis precision, de\ufb01ned as the inverse of the analysis error covariance, is the sum of the background precision and the observation precision projected onto the model space.\u201d Note that all these statements are dependent on the assumption that the statistical estimates of the errors are accurate. If the observations and/or background error co- variances are poorly known, if there are biases, or if the observations and background errors are correlated, the analysis precision can be considerably worse than implied (a) It is important to note that the observation error variances come from two different sources: one is the instrumental error variances proper, the second is the presence in the observations of subgrid-scale variability not represented in the grid-average values of the model and analysis. The second type of error is denoted \u201cerror of representativeness\u201d. By performing a grid average similar to the Reynolds average discussed in Chapter 4, we obtain that the observational error variance R is the sum of the instrument error variance Rinstr and the representativeness error variance Rrepr, assuming that these errors are not correlated. If in addition we allow for errors in the observation operator H with observation error covariance RH, these can also be included in the observation error covariance (Lorenc, 1986):"}
{"text": "5.4 Multivariate statistical data assimilation methods (b) The equations for Kalman \ufb01ltering, discussed in detail in Section 5.5, are very similar as those for OI. The main difference \u00afis that the background error covariance B, instead of being assumed to be constant in time as in OI or 3D-Var, is updated (forecasted) from the previous analysis time tn to the new analysis time tn+1. The model forecast starts from the analysis at time a), where M is the nonlinear model. Therefore, subtracting where \u03b5M is the model error. From (5.4.24) we obtain the Kalman \ufb01lter new forecast error covariance T = M(tn)Pa(tn)MT (tn) + Q(tn) m) is the forecast model error covariance, M is the linear tangent model and MT its adjoint. With this change, the weight matrix becomes the Kalman gain matrix K. Although this is apparently a small change from OI, the matrix multiplications by M in (5.4.20) are approximately equivalent to integrating the forecast model n/2 times, where n is the number of degrees of freedom of the model."}
{"text": "Approximations made in the practical We have seen that, in matrix form, the analysis is obtained from or if we de\ufb01ne increments from the background as \u03b4x = x \u2212xb, then the analysis The optimal weight matrix W that minimizes the analysis error covariance is given If all the statistical assumptions are accurate, i.e., the background error covariance B and the observations error covariance R are known exactly, then the formulas (5.4.26) or (5.4.27), and (5.4.28) provide the OI analysis. In that case, the analysis error covariance is given by If, as occurs in reality, the statistics are only approximations of the true statistics, then (5.4.26) and (5.4.28) provide a \u201cstatistical interpolation\u201d, not necessarily \u201coptimal OI is typically performed in physical space, either grid point by grid point (e.g., McPherson et al., 1979, DiMego et al., 1985) or over limited volumes (Lorenc, 1981). In the implementation of OI, (5.4.27) and (5.4.28) are solved point by point (or volume by volume) in grid-point space. To make the formulation in physical space clearer, we expand the matrix equations:"}
{"text": "H is the linear perturbation (Jacobian) of the forward observational model H, and HT is its transpose or adjoint. Multiplying by H on the left transforms grid-point increments into observation increments (e.g., by linear interpolation), and HT trans- forms from observation points back to grid points. There are n grid points, or if we are considering several variables, n is the product of the number of grid points and the variables. Consider a speci\ufb01c grid point with the subscript g. The subscripts j and k represent particular observations affecting the grid point g, and there are p such ob- servations. Recall that B is the background error covariance, so that the background error is \u03b5b(x, y) = xb(x, y) \u2212xt(x, y), and b jk = E[\u03b5b(x j, y j)\u03b5T expected value is the average over many cases."}
{"text": "where the optimal weight matrix is obtained from the system of equations (5.4.31). As an illustration, let us write (5.4.31) for the simple case of three grid points e, f, g, and two observations, 1 and 2 (Fig. 5.4.1)."}
{"text": "observation points, 1 and 2. 5.4 Multivariate statistical data assimilation methods g)T , and similarly for the background xb = g)T . The observation vector is yo = (yo 2)T , and the background values at the observation points are The coef\ufb01cients of the observation operator H are obtained from linear or higher order interpolation of the grid location to the observation location. We are assuming that the observed and analyzed variables are the same, so that the coef\ufb01cients of the matrix H are simply interpolation coef\ufb01cients. For example, if we used linear interpolation, H would be The background error covariance matrix elements are the covariances between grid is an approximation by interpolation of the background error covariances between grid to observation points, e.g., bg2 = bgeh2e + bg f h2 f + bggh2g. Then isanapproximationbybackinterpolationofthebackgrounderrorcovariancebetween observation points. The observation error covariance for this case is It is usually a reasonable assumption that measurement errors made at different locations are uncorrelated, in which case R is a diagonal matrix. (Measurement errors could be correlated, but only within small groups of observations made by the same instrument, in which case R is a block diagonal matrix, where the blocks are still easily invertible.) From this simple example, it is apparent that, in general, we can write the OI equation for a particular grid point g in\ufb02uenced by p observations as:"}
{"text": "where the weights are the solution of the linear system In (5.4.34), wgj is the weight that multiplies the observation increment \u03b4y j to con- tribute to the analysis increment \u03b4xa g, r jk is the observation error covariance between two observation points j and k, b jk is the background error covariance between the observation points j and k, and bgk is the background error covariance between the grid point g and the observation point k. We are assuming that the observation vector has already been transformed into the same type of variables as the model, i.e., that a retrieval method is used, and the interpolation matrix H has already been multiplied by B, generating grid-to-observation and observation-to-observation background correlations as in the simple example above. There are equations like (5.4.34) and (5.4.33) for each grid point, and in the case of multivariate analysis (e.g., the geopotential height and the two horizontal wind components z, u, v), for each variable at each grid point."}
{"text": "Equations (5.4.33) and (5.4.34) constitute the OI scheme written for each grid point g. Note the fundamental role that the background error covariance plays in the determination of the optimal weights (5.4.34). The background error covariance determinesthescaleandthestructureofthecorrectionstothebackground.Intheprac- tical implementation of (5.4.33) and (5.4.34), there are a number of commonly made additional simpli\ufb01cations, especially in the background error covariance elements b."}
{"text": "(a) Perhaps the most important advantage of statistical interpolation schemes such as OI and 3D-Var over empirical schemes such as SCM is that the correlation between observational increments is taken into account. Recall that in SCM, the weights of the observational increments depend only on their distance to the grid point. Therefore, in SCM all observations will be given similar weight even if a number of them are \u201cbunched up\u201d in one quadrant (Fig. 5.1.1), with just a single observation in a different quadrant. In OI (or 3D-Var), by contrast, the isolated observational increment will be given more weight in the analysis than observations that are close together and therefore less independent. The fact that isolated observations have more independent information than observations close together is a result of the fact that the forecast error correlations b jk/\u001cb j jbkk at the observation points j, k are large if the observation points are close together."}
{"text": "5.4 Multivariate statistical data assimilation methods (b) When several observations are too close together, then the solution of (5.4.34) becomes an ill-posed problem. In those cases, it is common to compute a \u201csuperobservation\u201d combining the close individual observations. This has the advantage of removing the ill-posedness, while at the same time reducing by averaging the random errors of the individual observations. The superobservation should be a weighted average that takes into account the relative observation errors of the original close observations."}
{"text": "In order to develop an OI-based data assimilation, we need to make actual estimates of the prior error covariances, B and R and the observation operator H. We saw before that,forasimplesysteminwhichtheobservationsarethesameasthemodelvariables, the observation operator is simply an interpolator from the model to the observation location. If the variables are different, it has to include not only the interpolation to the observation location, but also the \u201cforward model\u201d that represents the observations that would be obtained if the model was true. For example, if the observations are satellite radiances, the observation operator interpolates from the model grid to the radiance observation location, and then uses radiative transfer theory to convert a model column of temperature and moisture as a function of pressure into synthetic \u201cradiances\u201d. The observational error covariance is obtained from instrument error estimates. If the measurements are independent, the matrix R is diagonal, which is a major advantage. The forecast error covariance B is the most dif\ufb01cult error covariance to estimate, and has a crucial impact on the results."}
{"text": "Most of the rest of this section is devoted to a brief review of the methods that were in use in the 1980s to estimate B in OI applications. They were based on esti- mations of the horizontal and vertical correlations between forecast errors, estimated as the differences between the short-range forecasts and the rawinsonde observations (Thiebaux and Pedder, 1987, Hollingsworth and L\u00a8onnberg, 1986). In contrast, for 3D-Var, the method that has been almost universally adopted does not depend on measurements at all, but on the difference between forecasts verifying at the same time. This is known as the \u201cNMC method\u201d and is discussed in Section 5.4.8."}
{"text": "are the correlations of the background errors at two observational points i, j, and \u03c3 2 It has been commonly assumed in OI implementations that the background error correlations can be separated into the product of the horizontal correlation and the vertical correlation. As we will see, these simpli\ufb01ed correlations are typically de\ufb01ned as functions of distance only."}
{"text": "We present an example of simpli\ufb01cations frequently made in practice by con- sidering the two-dimensional analysis of z, u, v, at a single pressure level p, using rawinsondes as the only observations. Since the observation errors of two separate rawinsondes at points i and j are not correlated (although the geopotential errors are correlated in the vertical), we can assume that the observation error covariance matrix is a diagonal matrix:"}
{"text": "With these assumptions, (5.4.34) for a grid point g becomes bg is the mean square of the relative error of observations compared with the background error, a parameter frequently \u201ctuned\u201d to give more or less weight to the observations. We can also show from (5.4.29) that the relative analysis error at the grid point is given by:"}
{"text": "We now further assume that the background error correlation between two points in the same horizontal surface is homogeneous and isotropic (i.e., it doesn\u2019t change with a rigid translation or rotation of the two points). In that case the background error correlation of the geopotential height depends only on the distance between the two points. Gandin (1963), Schlatter (1975), Thiebaux and Pedder (1987) and others have used a Gaussian exponential function for the geopotential error correlation:"}
{"text": "i j = (xi \u2212x j)2 + (yi \u2212y j)2 is the square of the distance between two points i and j, and L\u03c6, typically of the order of 500 km, de\ufb01nes the background error 5.4 Multivariate statistical data assimilation methods correlation scale. Gaussian functions have also been used for the vertical correlation functions. These assumptions are clearly crude, and only qualitatively begin to re\ufb02ect the true structure of the background error correlation. For example, in the real atmo- sphere the background error correlation length should depend on the Rossby radius of deformation, and therefore should be a function of latitude, with longer horizontal error correlations in the tropics than in the extratropics (Balgovind et al., 1983, Baker et al., 1987). It should also depend on the data density: at the boundaries between data-rich and data-poor regions, the correlations of the forecast errors should not be isotropic (Cohn and Parrish, 1986). We refer the reader to the discussions in Daley (1991) and Thiebaux and Pedder (1987) for further details and references."}
{"text": "Another important assumption usually made in the OI analysis of large-scale \ufb02ow is that the background wind error correlations are geostrophically related to the geopotential height error correlations. This has two advantages: it avoids having to estimate independently the wind error correlation, and it imposes an approxi- mate geostrophic balance of the wind and height analysis increments, and therefore improves the balance of the analysis (see Remark 5.4.3(c)). Once a functional as- sumption for the background error correlation of the height like (5.4.40) is made, then the multivariate correlation between heights and winds can be obtained from the height correlations. For example, consider the background error correlation between two horizontal wind components:"}
{"text": "Now, since the geopotential error at the point x j is independent of yi and vice versa, we can combine the derivatives, use (5.4.40) and write The standard deviation of the wind increments can also be derived from the geostrophic relationship, E(\u03b4u2 i )1/2 = (g\u03c3z/ fi), E(\u03b4v2 j)1/2 = (g\u03c3z/ f j), so that we obtain the correlation of the increments of the two wind components by dividing (5.4.42) by these standard deviations: \u03c1u,v = \u2212\u22022\u00b5i j/\u2202yi\u2202x j. Similarly, we can ob- tain the correlations between the increments of any two of the three variables at two points i, j: \u03c1h,h = \u00b5i j, \u03c1h,u = \u2212\u2202\u00b5i j Fig. 5.4.2 shows schematically the shape of typical wind/height correlation func- tionsusedinOI(e.g.,Schlatter,1975).Notethattheu\u2013hcorrelationshavetheopposite sign than the h\u2013u correlations because the \ufb01rst and second variables correspond to the \ufb01rst and second points i and j respectively. For example, a height observation leading to a positive analysis increment of h will result in positive increments of u to its north."}
{"text": "Conversely, a positive increment of u will lead to negative increments of h to its north. functions for multivariate OI analysis derived using the geostrophic increment assumption (after Gustaffson, 1981). Both x- and y-axes go from \u2212 Equations (5.4.42) are not valid at the equator, and additional approximations have to be made in the tropics to allow for a smooth decoupling of wind and height increments (Lorenc, 1981)."}
{"text": "In addition, it is common to select the observations to be included in solving the linear system for the weight coef\ufb01cients (5.4.38), depending on the computer resources available for the analysis, allowing for a maximum number of observations affecting each grid point. Rules for the selection of the subset of observations to be used typically depend on the distance to the grid point (within a maximum radius of in\ufb02uence), the types of observations (giving priority to the most accurate) and their distribution (trying to \u201ccover\u201d all quadrants, and choosing the closest stations)."}
{"text": "Lorenc (1981) gave a comprehensive description of an OI as implemented at the ECMWF. Although later improvements were implemented in the error 5.4 Multivariate statistical data assimilation methods covariances(HollingsworthandL\u00a8onnberg,1986,L\u00a8onnbergandHollingsworth,1986, Hollingsworth, 1989) and other components of the analysis cycle, it remained the backbone of the ECMWF analysis until its replacement by 3D-Var in 1996. A more recent operational implementation of OI including advanced three-dimensional es- timations of the three-dimensional background error covariance is that of Mitchell et al. (1990) in the Canadian operational system."}
{"text": "Bratseth\u2019s iterative method for OI Bratseth (1986) proposed a variation of the SCM that converges to OI analysis (see also Daley (1991), Appendix F). It is based on the convergence of the geometric (I \u2212A)k = A\u22121 if (I \u2212A)k \u21920 as k \u2192\u221e The OI algorithm is written as where d is a correction vector which is determined by successive iterations. M is a diagonal (and hence easily invertible) matrix which is chosen to speed convergence."}
{"text": "d\u03bd = [I \u2212(HBHT + R)M\u22121]d\u03bd\u22121 Using the iteration formula (5.4.45), the corrections are The summation in (5.4.46) is a geometric series. Because M is chosen to ensure convergence, in the limit of large j, the series converges to d\u221e= [(HBHT + R)M\u22121]\u22121\u03b4y = M(HBHT + R)\u22121\u03b4y After a suf\ufb01cient number of iterations, the correction (5.4.47) is substituted into (5.4.44), which results in the desired OI solution (independent of M):"}
{"text": "g + (BHT)g (HBHT + R)\u22121\u03b4y Since the diagonal matrix M is arbitrary, Bratseth suggested a choice that speeds where b jk are the elements of HBHT, and r jk the elements of R. This method is further illustrated in the following examples."}
{"text": "One-dimensional example of OI and comparison Consider a simple example in one-dimensional grid space, with just two geopotential observations along the x-axis, at the points x1 = 0, and x2 = \u03b1L\u03c6, where L\u03c6 is the correlation length of geopotential observations. The grid points are also on the x-axis:"}
{"text": "xg = g\u0016x. For each grid point we can rewrite equations (5.4.33) and (5.4.34) as where the weights are obtained from the solution of the linear system (b11 + r11)wg1 + b12wg2 = b1g b21wg1 + (b22 + r22)wg2 = b2g As noted before, we have assumed that the observational errors are uncorrelated at different points, and that they are uncorrelated with the background errors."}
{"text": "We assume as before that the background error correlation is Gaussian, and that the ratio of the observation and background error variances is If one of the observations is of geopotential and the second is a wind observation v, the term bi j can be computed as In Exercise 5.4.2, this OI problem is solved directly. We now write Bratseth\u2019s iterative scheme for the same grid point and allow for more than two observations."}
{"text": "First we need to compute correction vector d by performing enough successive iterations. Assume p observations are in\ufb02uencing the grid point g. The \ufb01rst iteration is simply the vector of observational increments:"}
{"text": "5.4 Multivariate statistical data assimilation methods The following iterations are given by The elements ai j of the matrix A are obtained as in (5.4.45), from ai j = \u03b4i j \u2212(bi j + \u03b4i jri j)/m j j where \u03b4i j = 1 if i = j and zero otherwise, and the scaling diagonal matrix M\u22121 has chosen by Bratseth to speed up convergence."}
{"text": "Exercise 5.4.2: (adapted from Bratseth, 1986) (1) Write a FORTRAN or MATLAB code to solve an OI analysis (5.4.59) with two observations. Assume that the background is zero, i.e., compute just the analysis increments for the geopotential, using the following default values:"}
{"text": "\u03c3\u03c6 = 200 m2/s2; \u03c3o = 100 m2/s2; f = 10\u22124 s\u22121; L\u03c6 = 500 km; \u0016x = 50 km; distance between the observations x2 \u2212x1 = \u03b1L\u03c6, \u03b1 = 1; the observational increments are (\u03c6o Compute the geopotential analysis increments at all the grid points Plot the analysis at the grid points and the two observational points."}
{"text": "Compute and plot the estimated analysis error at each point. (2) Same as (1) but varying the distance between the observations (\u03b1 = 1, (3) Do the default exercise again but change the ratio of the observation and background error variances, \u03b7 = 0.0, 0.5, 1.0, 2.0."}
{"text": "(4) Assume that one has a \u201ccredulous\u201d analysis that believes that \u03b7 = 0.0, whereas the real ratio between observation and background error variance is \u03b7 = 1.0. Compare the true analysis error with the estimated analysis error."}
{"text": "Exercise 5.4.3: Adapt the program written in Exercise 5.4.2 to two multivariate observations, \u03c61,v1; \u03c62,v2. Take the default values in Exercise 5.4.2, and assume Perform the analysis over the grid points as in Exercise 5.4.2.(1) Exercise 5.4.4: Perform a Bratseth iterative OI and determine how many iterations are required to obtain a satisfactory approximation to OI (compare with the exact OI solution obtained in Exercise 5.4.2)."}
{"text": "3D-Var, the physical space analysis scheme (PSAS), and their relationship to OI We saw in Section 5.3 that there is an important equivalence between the formulation of the optimal analysis of a scalar by minimizing the analysis error variance (\ufb01nding the optimal weights through a least squares approach) and solving the same problem through a variational approach (\ufb01nding the analysis that minimizes a cost function measuring its distance to the background and to the observations). The same is true when the analysis involves a full three-dimensional \ufb01eld, as in Section 5.4.1. In the derivation of OI, we found the optimal weight matrix W that minimized the analysis error covariance (a matrix). Lorenc (1986) showed that this solution is equivalent to a speci\ufb01c variational assimilation problem: Find the optimal analysis xa \ufb01eld that minimizes a (scalar) cost function, where the cost function is de\ufb01ned as the distance between x and the background xb, weighted by the inverse of the background error covariance, plus the distance to the observations yo,weighted by the inverse of the observations error covariance:"}
{"text": "2J(x) = (x \u2212xb)T B\u22121(x \u2212xb) + [yo \u2212H(x)]T R\u22121[yo \u2212H(x)] We saw in Section 5.3.2 for the case of a simple scalar with two measurements that the variational cost function can be derived through a maximum likelihood approach, 5.5 3D-Var, the physical space analysis scheme (PSAS) i.e., the analysis is the most likely state of the atmosphere, given the two independent measurements. Similarly we can de\ufb01ne here the likelihood (Edwards, 1984) of the true state given the background \ufb01eld (6-h forecast) and the new observations:"}
{"text": "2 [(yo\u2212H(x))T R\u22121(yo\u2212H(x))] Since the background and new observations are independent, their joint probability is the product of the two Gaussian probabilities. The most likely state x of the atmosphere (analysis) maximizes the joint probability. This maximum is also attained when the logarithm of the joint probability is maximized, which is the same as minimizing the cost function (5.5.1)."}
{"text": "Alternatively, the cost function (5.5.1) can also be derived based on a Bayesian approach.1 In this case we assume that the true \ufb01eld is a realization of a random process de\ufb01ned by the prior probability distribution function (given the background Bayes theorem indicates that given the new observations yo, the a posteriori proba- bility distribution of the true \ufb01eld is The Bayesian estimate of the true state is the one that maximizes the a posteriori probability (5.5.3). The denominator in (5.5.3) is the \u201cclimatological\u201d distribution of observations, and does not depend on the current true state x. Therefore the maximum of the a posteriori probability is attained when the numerator is maximum, and is given by the minimum of the cost function (5.5.1)."}
{"text": "The minimum of J(x) in (5.5.1) is attained for x = xa, i.e., the analysis is given An exact solution can be obtained in the following way. As we did in Section 5.4.1, we can expand the second term of (5.5.1), the observational differences, assuming that the analysis is a close approximation to the truth and therefore to the observations, and linearizing H around the background value:"}
{"text": "1 I am grateful to Peter Lyster for a discussion about this topic. Substituting (5.5.5) into (5.5.1) we get 2J(x) = (x \u2212xb)T B\u22121(x \u2212xb) Expanding the products, and using the rules to transpose matrix products, we then 2J(x) = (x \u2212xb)T B\u22121(x \u2212xb) + (x \u2212xb)T HT R\u22121H(x \u2212xb) \u2212{yo \u2212H(xb)}T R\u22121H(x \u2212xb) \u2212(x \u2212xb)T HT R\u22121{yo \u2212H(xb)} +{yo \u2212H(xb)}T R\u22121{yo \u2212H(xb)} The cost function is a quadratic function of the analysis increments (x \u2212xb) and therefore we can use Remark 5.4.1(d): Given a quadratic function F(x) = 1 dT x + c, where A is a symmetric matrix, d is a vector and c a scalar, the gradient is given by \u2207F(x) = Ax + d. The gradient of the cost function J with respect to x (or with respect to (x \u2212xb)) is \u2207J(x) = B\u22121(x \u2212xb) + HT R\u22121H(x \u2212xb) \u2212HT R\u22121{yo \u2212H(xb)} We now set \u2207J(xa) = 0 to ensure that J is a minimum, and obtain an equation for (B\u22121 + HT R\u22121H)(xa \u2212xb) = HT R\u22121{yo \u2212H(xb)} xa = xb + (B\u22121 + HT R\u22121H)\u22121HT R\u22121{yo \u2212H(xb)} which, in incremental form, is \u03b4xa = (B\u22121 + HT R\u22121H)\u22121HT R\u22121\u03b4yo Formally, this is the solution of the variational (3D-Var) analysis problem, but in practice the solution is obtained through minimization algorithms for J(x) using iterative methods for minimization such as the conjugate gradient or quasi-Newton Note that the control variable for the minimization (i.e., the variable with respect to which we are minimizing the cost function J) is now the analysis, not the weights as in OI. The equivalence between the minimization of the analysis error variance (\ufb01nding the optimal weights through a least squares approach), and the three-dimensional variational cost function approach (\ufb01nding the optimal analysis that minimizes the distance to the observations weighted by the inverse of the error variance) is an 5.5 3D-Var, the physical space analysis scheme (PSAS) Equivalence between the OI and 3D-Var We now demonstrate the equivalence of the 3D-Var solution and the OI analysis solution obtained in Section 5.4.1 (Lorenc, 1986). We have to show that the weight matrix that multiplies the innovation {yo \u2212H(xb)} = \u03b4yo in (5.5.10) is the same as the weight matrix obtained with OI, i.e., that W = (B\u22121 + HT R\u22121H)\u22121HT R\u22121 = (BHT )(R + HBHT )\u22121 This identity is a variant of the Sherman\u2013Morrison\u2013Woodbury formula (Golub and Van Loan, 1996). If the variables that we are observing are the same as the model variables, i.e., if H = HT = I, then it is rather straightforward to prove (5.5.11), using the rules for the inverse and transpose of a matrix product. However, in general H is rectangular, and noninvertible. The equality (5.5.11) can be proven2 by considering the following block matrix equation:"}
{"text": "where w is a vector which will be discussed further in Section 5.5.2. We want to derive from (5.5.12) an equation of the form \u03b4xa = W\u03b4yo. Eliminating w from both block rows in (5.5.12) we \ufb01nd that \u03b4xa = (B\u22121 + HT R\u22121H)\u22121HT R\u22121\u03b4yo, the 3D-Var version of the weight matrix. On the other hand, eliminating \u03b4xa from both block rows, we obtain an equation for the vector w, w = (R + HBHT )\u22121\u03b4yo. From this, substituting w in the second block row of (5.5.12), we obtain the OI version of the weight matrix: \u03b4xa = BHT (HBHT + R)\u22121\u03b4yo. This demonstrates the formal equiv- alence of the problems solved by 3D-Var and OI. Because the methods of solution are different, however, their results are different, and most centers have adopted the Exercise 5.5.1: Prove (5.5.11), using the rules for the inverse and transpose of a matrix product assuming H = 1."}
{"text": "Physical space analysis system (PSAS) Da Silva et al. (1995) introduced another scheme related to 3D-Var and OI, in which the minimization is performed in the (physical) space of the observations, rather than in the model space as in the 3D-Var scheme (spectral variables in the NCEP scheme (Parrish and Derber, 1992)). They solved the same OI/3D-Var equation (5.5.10), 2 I am very grateful to Jim Purser for suggesting this elegant proof of (5.5.11) and for pointing out the relationship (5.5.17)."}
{"text": "The \ufb01rst step is the most computer intensive, and is solved by minimization of a cost 2wT (R + HBHT )w \u2212wT [yo \u2212H(xb)] If the number of observations is much smaller than the number of degrees of freedom in the model, this is a more ef\ufb01cient method for achieving results similar to those of Exercise 5.5.1 indicates that the intermediate solution vector w is also given by w = R\u22121(\u03b4yo \u2212H\u03b4xa) = R\u22121[yo \u2212H(xa)] i.e., it is the mis\ufb01t of the observations to the analysis weighted by the inverse of the observation covariance matrix."}
{"text": "Final comments on the relative advantages Although the three statistical interpolation methods, 3D-Var, OI, and PSAS, have been shown to formally solve the same problem, there are important differences in the methods of solution. As indicated before, in practice OI requires the introduction of a number of approximations, and local solution of the analysis, grid point by grid point, or small volume by small volume (Lorenc, 1981). This in turn requires the use of a \u201cradius of in\ufb02uence\u201d and selection of only the stations closest to the grid point or volume being analyzed. The background error covariance matrix also has to be Despite their formal equivalence, 3D-Var and the closely related PSAS have sev- eral important advantages with respect to OI, because the cost function (5.5.1) is minimized using global minimization algorithms, and as a result it makes unneces- sary many of the simplifying approximations required by OI (Parrish and Derber, 1992, Derber et al., 1991, Courtier et al., 1998, Rabier et al., 1998, Andersson et al., 5.5 3D-Var, the physical space analysis scheme (PSAS) (a) In 3D-Var (and PSAS) there is no data selection, all available data are used simultaneously. This avoids jumpiness in the boundaries between regions that have selected different observations."}
{"text": "(b) In OI the background error covariance has been crudely obtained assuming, for example, separability of the correlation into products of horizontal and vertical gaussian correlations, and that the background errors are in geostrophic balance. The background error covariance matrix for 3D-Var, although it may still require simplifying assumptions, can be de\ufb01ned with a more general, global approach, rather than the local approximations used in OI. In particular, most centers have adopted the \u201cNMC method\u201d (Parrish and Derber, 1992) for estimating the forecast error covariance:"}
{"text": "B \u2248\u03b1E{[x f (48 h) \u2212x f (24 h)][x f (48 h) \u2212x f (24 h)]T } As indicated in (5.5.18), in the \u201cNMC\u201d (now NCEP) method, the structure of the forecast or background error covariance is estimated as the average over many (e.g., 50) differences between two short-range model forecasts verifying at the same time. The magnitude of the covariance is then appropriately scaled. In this approximation, rather than estimating the structure of the forecast error covariance from differences with rawinsondes (Thiebaux and Pedder, 1987, Hollingsworth and L\u00a8onnberg, 1986), the model\u2013forecast differences themselves provide a multivariate global forecast difference covariance. The forecast covariance (5.5.18) strictly speaking is the covariance of the forecast differences and is only a proxy for the structure of forecast errors. Nevertheless, it has been shown to produce better results than previous estimates computed from forecast minus observation estimates."}
{"text": "An important reason for this improvement is that the rawinsonde network does not have enough density to allow a proper estimate of the global structures (Parrish and Derber, 1992, Rabier et al., 1998), whereas (5.5.18) provides a global representation of the forecast error structures. In the NCEP system, the analysis variables are based on the spectral model forecast variables. This allows a major simpli\ufb01cation: the assumption of horizontal homogeneity and isotropy of the error covariance imply that the spectral model errors are uncorrelated, i.e., the background error covariance in spectral space is diagonal. In the vertical, Parrish and Derber (1992) use an empirical orthogonal function expansion of (5.5.18)."}
{"text": "(c) The background error covariance B has a fundamental impact in determining the characteristics of the OI analysis increment. Essentially, the analysis increment can only occur within the subspace spanned by B."}
{"text": "This can be easily demonstrated if we assume that the background error covari- ance is spanned by a single vector b, i.e., B = bbT . This assumes that the forecast error can take place only in the direction of b. Assume also for simplicity that H = I, i.e., that the model variables are observed at all the model grid points, and that R = \u03b12I, i.e., that the observational errors are uncorrelated and equal. Then the so- lution of the OI problem \u03b4xa = xa \u2212xb = BHT [HBH + R]\u22121 [yo \u2212H(xb)] can be written exactly as \u03b4xa = bbT \u03b4yo/(bT b + \u03b12)."}
{"text": "Note that the analysis increment has the direction of b, and that its magnitude is proportional to the projection of the observational increment upon the subspace of the vector b (a similar formula with \u03b12 = 0 was used by Kalnay and Toth, (1994))."}
{"text": "\u00d7 [yo \u2212H(xb)], and from the previous formula we obtain again showing that the analysis increment takes place in the direction of b, with an amplitude proportional to the projection of innovation onto the subspace of b."}
{"text": "Exercise 5.5.2: Prove (5.5.19) for a vector b of dimension 2. i=1 bi, spanning a subspace of dimension k < n, the dimension of the model, then the 3D-Var cost function 2J(x) = (x \u2212xb)T B\u22121(x \u2212xb)+ [yo \u2212 H(x)]T R\u22121[yo \u2212H(x)] can be used to show again that the analysis increment has to be within the k-dimensional subspace spanned by the vectors bi. This is because outside this subspace, the inverse of the covariance matrix is in\ufb01nitely large, and therefore increments not within the k-dimensional subspace are forbidden because they would result in large increases of the value of the cost function."}
{"text": "(d) It is possible to add constraints to the cost function without increasing the cost of the minimization. For example, Parrish and Derber (1992) included a \u201cpenalty\u201d term in the cost function (5.5.1) forcing simultaneously the analysis increments to approximately satisfy the linear global balance equation. In OI, the imposition of the geostrophic constraint on the increments ensured only an approximate balance in the analysis. In practice it was found necessary to follow the OI analysis with a nonlinear normal mode initialization (Section 5.7). With the global balance equation added as a weak constraint to the cost function, the NCEP global model spin up (indicated for example by the change of precipitation over the \ufb01rst 12 hours of integration) was reduced by more than an order of magnitude compared with the results obtained with OI. In other words, with the implementation of 3D-Var it became unnecessary to perform a separate initialization step in the analysis cycle."}
{"text": "5.6 Advanced data assimilation methods (e) It is also possible to incorporate important nonlinear relationships between observed variables and model variables in the H operator in the minimization of the cost function (5.5.1) by performing \u201cinner\u201d iterations with the linearized H observation operator kept constant and \u201couter\u201d iterations in which it is updated. This is harder to do in the OI approach."}
{"text": "(f) The introduction of 3D-Var has allowed three-dimensional variational assimilation of radiances (Derber and Wu, 1998). In this approach, there is no attempt to perform retrievals and, instead, each satellite sensor is taken as an independent observation with uncorrelated errors. As a result, for each satellite observation spot, even if some channel measurements are rejected because of cloud contamination, others may still be used. In addition, because all the data are assimilated simultaneously, information from one channel at a certain location can in\ufb02uence the use of satellite data at a different geographical location. The quality control of the observations becomes easier and more reliable when it is made in the space of the observations than in the space of the retrievals.3 (g) It is also possible to include quality control of the observations within the 3D-Var analysis (Section 5.6)."}
{"text": "(h) Cohn et al. (1998) have shown that the observation-space form of 3D-Var (PSAS) offers opportunities, through the grouping of data, to improve the preconditioning of the problem in an iterative solution."}
{"text": "Advanced data assimilation methods with evolving forecast error covariance In Section 5.4 we have discussed OI, which minimizes the expected analysis error covariance, and its practical implementations, and in Section 5.5 the closely related 3D-Var and PSAS methods, solving essentially the same problem but minimizing a cost function. In these methods, the forecast error covariance matrix is estimated once and for all, as if the forecast errors were statistically stationary."}
{"text": "From Fig. 5.6.1 we can evaluate whether this is indeed a good approximation. It shows the 6-h forecast errors in the western and eastern thirds of the USA from 3 Joiner and Da Silva (1998) pointed out that the use of retrievals from remotely sensed observations is a viable option within the variational analysis approach, as long as the innovation vector is computed consistently with the use of retrievals from radiances. If Dyo is the (linearized) retrieval algorithm applied to satellite radiances to obtain, e.g., temperature and moisture pro\ufb01les, then the innovation vector should be computed consistently as Dyo \u2212DFxb, where F is the forward (linearized radiative transfer) algorithm that converts model variables into model radiances. In other words, the forward observational operator in (5.5.13) is H = DF, and the observational error covariance for the retrievals becomes E[(D\u03b4yo)(D\u03b4yo)T ] = DE[\u03b4yo\u03b4yoT ]DT = DRDT instead of R, which is the observation error covariance when radiances are directly assimilated. If this method is applied to OI, the weight matrix becomes K = BFT BT (DFBFT DT + DRDT )\u22121."}
{"text": "1958 f06 500 mb RMS Z\u2013inc West US vs East US analysis over two data-rich regions in the USA, from the NCEP-NCAR reanalysis (Kistler et al., 2001): (a) 1958; (b) 1996. The average error is indicated in the box."}
{"text": "the NCEP/NCAR reanalysis (Kistler et al., 2001) estimated from the difference be- tween the forecast and the analysis or analysis increments. Since in these data-rich regions the analysis is close to the truth, the analysis increment is a good estimate of the forecast error. Figure 5.6.1(a) corresponds to 1958 and Fig. 5.6.1(b) is calcu- lated for 1996. The NCEP/NCAR reanalysis used a 3D-Var data assimilation system unchanged in time, so that the difference between the \ufb01gures is due only to the changes in the observing system. Over these four decades the improvements in the observing system in the Northern Hemisphere show a positive impact on the 6-h forecast errors of about 20%, with the average analysis increment reduced from about 10 m to 8 m. Note that the NCEP/NCAR reanalysis used satellite temperature retrievals, not direct assimilation of radiances, which would probably have resulted in larger improvements. However, the most striking result apparent in the \ufb01gure is that the day-to-day variability in the forecast error (with a time scale of a few days) is about as large as the average error, not just in 1958 but even in 1996. This \ufb01gure emphasizes the importance of the \u201cerrors of the day\u201d (Kalnay et al., 1997), which 5.6 Advanced data assimilation methods f06 500 mb RMS Z\u2013inc West US vs East US in these areas are dominated presumably by baroclinic instabilities of synoptic time scales, and which are ignored when the forecast error covariance is assumed to be In this section we give a brief introduction to more advanced (and much costlier) schemes that include, at least implicitly, the evolution of the forecast error covariance."}
{"text": "A number of papers in Ghil et al. (1997) provide more details about the theory and practice of some of these methods. Ide et al. (1997) is a brief but extremely clear As we discussed in Section 5.3 for the simple case of a scalar analysis, the Kalman \ufb01lter (KF) is formally very similar to OI, but with one major difference: the forecast or background error covariance P f (ti) is advanced using the model itself, rather than estimating it as a constant covariance matrix B."}
{"text": "Following the notation of Ide et al. (1997), let x f (ti) = Mi\u22121 [xa(ti\u22121)] represent the (nonlinear) model forecast that advances from the previous analysis time ti\u22121 to the current ti. The model is imperfect (in particular, it has been discretized, so that subgrid processes are not included). Therefore, we assume that for the true xt(ti) = Mi\u22121[xt(ti\u22121)] + \u03b7(ti\u22121) where \u03b7 is a noise process with zero mean and covariance matrix Qi\u22121 = E(\u03b7i\u22121\u03b7T (in other words, when starting from perfect initial conditions, the forecast error is given by \u2212\u03b7i\u22121, where the negative sign is chosen for convenience). Although we are assuming that the mean error is zero, in reality model errors have signi\ufb01cant biases that should be taken into account. Dee and DaSilva (1998) showed how to estimate and remove these model biases."}
{"text": "In the extended Kalman \ufb01lter, the forecast error covariance is obtained linearizing the model about the nonlinear trajectory of the model between ti\u22121 and ti, so that if we introduce a perturbation in the initial conditions, the \ufb01nal perturbation is given x(ti) + \u03b4x(ti) = Mi\u22121 [x(ti\u22121) + \u03b4x(ti\u22121)] = Mi\u22121 [x(ti\u22121)] + Li\u22121\u03b4x(ti\u22121) + O(|\u03b4x|2) The linear tangent model Li\u22121 is a matrix that transforms an initial perturbation at time ti\u22121 to the \ufb01nal perturbation at time ti (Lorenz, 1965). The linear tangent model and its transpose or adjoint model LT i\u22121 will be discussed in more detail in Chapter 6, which is devoted to predictability, and in Appendix B. We point out here that if there are several steps in a time interval t0 \u2212ti, the linear tangent model that advances a perturbation from t0 to ti is given by the product of the linear tangent model matrices that advance it over each step:"}
{"text": "Therefore, the adjoint model (transpose of the linear tangent model) is given by Equation (5.6.4) shows that the adjoint model \u201cadvances\u201d a perturbation backwards in time, from the \ufb01nal to the initial time. Adjoint models are discussed in more detail in Chapter 6 and in Appendix B."}
{"text": "From these equations we can de\ufb01ne extended Kalman \ufb01ltering which consists of a \u201cforecast step\u201d that advances the forecast and the forecast error covariance, followed by an \u201canalysis\u201d or update step, a sequence analogous to OI. After the forecast step, an optimal weight matrix or Kalman gain matrix is calculated as in OI, and this matrix is used in the analysis step."}
{"text": "The formula for the Kalman gain or weight matrix in (5.6.10), computed after completing the forecast step, is obtained by minimizing the analysis error covariance i . It is given by the same formula derived for OI, but with the constant background error covariance B replaced by the evolved forecast error covariance P f (ti):"}
{"text": "The extended Kalman \ufb01lter is the \u201cgold standard\u201d of data assimilation. Even if a system starts with a poor initial guess of the state of the atmosphere, the extended Kalman \ufb01lter may go through an initial transient period of a week or so, after which it should provide the best linear unbiased estimate of the state of the atmosphere and its error covariance. However, if the system is very unstable, and the observations are not frequent enough, it is possible for the linearization to become inaccurate, and the extended Kalman \ufb01lter may drift away from the true solution (Miller et al., 1994)."}
{"text": "The updating of the forecast error covariance matrix ensures that the analysis takes into account the \u201cerrors of the day\u201d. Unfortunately the extended Kalman \ufb01lter is exceedingly expensive, since the linear model matrix Li\u22121 has size n, the number of degrees of freedom of a modern model (more than 106) and updating the error covariance is equivalent to performing O(n) model integrations. For this reason, this step has been replaced by the use of simplifying assumptions (e.g., a lower order model and/or infrequent updating)."}
{"text": "One promising simpli\ufb01cation of Kalman \ufb01ltering is ensemble Kalman \ufb01ltering. In this approach, an ensemble of K data assimilation cycles is carried out simulta- neously (Houtekamer et al., 1996, Houtekamer and Mitchell, 1998, 2001, Hamill and Snyder, 2000, Hamill et al., 2001, Anderson, 2001). All the cycles assimilate the same real observations, but in order to maintain them realistically independent, different sets of random perturbations are added to the observations assimilated in each member of the ensemble data assimilations. This ensemble of data assimila- tion systems can be used to estimate the forecast error covariance (Evensen, 1994, Evensen and van Leewen, 1996, Houtekamer and Mitchell, 1998, Hamill and Snyder, 2000). After completing the ensemble of analyses at time ti\u22121, and the K forecasts k(ti\u22121)], one can obtain an estimate of the forecast error covariance k (ti). For example one could assume where the overbar represents the ensemble average, but this would tend to underesti- mate the variance of the forecast errors because every forecast is used to compute the estimate of its own error covariance. Houtekamer and Mitchell (1998) and Hamill and Snyder (2000) suggested instead to compute the forecast error covariance for ensemble member ifrom an ensemble that excludes the forecast l:"}
{"text": "Hamill and Snyder (2000) also suggested a hybrid between 3D-Var and ensemble Kalman \ufb01ltering, where the forecast error covariance is obtained from a linear com- bination of the (constant) 3D-Var covariance B3D-Var:"}
{"text": "where \u03b1 is a tunable parameter that varies from 0, pure ensemble Kalman \ufb01ltering from (5.6.12) to 1, pure 3D-Var. In (5.6.12) the ensemble Kalman \ufb01ltering covariance is estimated from only a limited sample of ensemble members K \u22121, compared with a much larger number of degrees of freedom of the model, it is rank de\ufb01cient. The 5.6 Advanced data assimilation methods combination with the 3D-Var, computed from many estimated forecast errors (using for example the method of Parrish and Derber (1992)) may ameliorate this sampling problem and \u201c\ufb01ll out\u201d the error covariance. In the experiments of Hamill and Snyder (2000) the best results were obtained for low values of \u03b1, between 0.1 and 0.4, indicating good impact of the use of the ensemble-evolved forecast error covariance."}
{"text": "They found that 25\u201350 ensemble members were enough to provide the bene\ufb01t of ensemble Kalman \ufb01ltering (but this may be different when using a more complex model than the quasi-geostrophic model used here)."}
{"text": "The ensemble Kalman \ufb01ltering approach has several advantages: (a) K is of the order of 10\u2013100, so that the computational cost (compared with OI or 3D-Var) is increased by a factor of 10\u2013100. Although this increased cost may seem large, it is small compared to extended Kalman \ufb01ltering, which requires a cost increase of the order of the number of degrees of freedom of the model. (b) Ensemble Kalman \ufb01ltering does not require the development of a linear and adjoint model. (c) It does not require the linearization of the evolution of the forecast error covariance. (d) It may provide excellent initial perturbations for ensemble forecasting. Despite these advantages, at the time of writing no operational center has yet implemented this system, although Canada has plans to do so. Ott et al. (2002) proposed to do en- semble Kalman \ufb01ltering based on the bred vectors available from operations, and taking advantage of the local low dimensionality observed by Patil et al. (2001). En- semble Kalman \ufb01ltering appears at the present time to be one of the most promising approaches for the future."}
{"text": "This is an important extension of the 3D-Var which allows for observations dis- tributed within a time interval (t0, tn) (e.g., Lewis and Derber, 1985, Courtier and Talagrand, 1990, Derber, 1989, Daley, 1991, Zupanski, 1993, Bouttier and Rabier, 1997). The cost function includes a term measuring the distance to the background at the beginning of the interval, and a summation over time of the cost function for each observational increment computed with respect to the model integrated to the The control variable (the variable with respect to which the cost function is min- imized) is the initial state of the model with the time interval x (t0), whereas the analysis at the end of the interval is given by the model integration from the solution x (tn) = M0 [x (t0)]. Thus, the model is used as a strong constraint (Sasaki, 1970), i.e., the analysis solution has to satisfy the model equations. In other words, 4D-Var seeks an initial condition such that the forecast best \ufb01ts the observations within the assimilation interval. The fact that the 4D-Var method assumes a perfect model is a disadvantage since, for example, it will give the same credence to older observations at the beginning of the interval as to newer observations at the end of the interval (Menard and Daley, 1996). Derber (1989) suggested a method of correcting for a constant model error (a constant shape within the assimilation interval), see also A variation in the cost function when the control variable x (t0) is changed by a small amount \u03b4x(t0) is given by \u03b4J = J[x(t0) + \u03b4x(t0)] \u2212J[x(t0)] \u2248 where the gradient of the cost function [\u2202J/\u2202x(t0)] j = \u2202J/\u2202x j(t0) is a column vector."}
{"text": "As suggested by (5.6.15), iterative minimization schemes require the estimation of the cost function gradient. In the simplest scheme, the steepest descent method, the change in the control variable after each iteration is chosen to be opposite to the gradient \u03b4x(t0) = \u2212a\u2207x(t0) J = \u2212a \u2202J/\u2202x(t0). Other, more ef\ufb01cient methods, such as the conjugate gradient or quasi-Newton (Navon and Legler, 1987) method, also require the use of the gradient, so that in order to solve this minimization problem ef\ufb01ciently, we need to be able to compute the gradient of J with respect to the elements As we saw in Sections 5.4 and 5.5, given a symmetric matrix A and a functional 2xT Ax, the gradient is given by \u2202J/\u2202x = Ax. If J = yT Ay, and y = y(x), then where [\u2202y/\u2202x]k,l = \u2202yk/\u2202xl is a matrix."}
{"text": "We can write (5.6.14) as J = Jb + Jo, and from the rules discussed above, the gradient of the background component of the cost function Jb = 1 0 [x(t0) \u2212xb(t0)] with respect to x (t0) is given by i ] is more complicated because xi = Mi[x(t0)]. If we introduce a perturbation to the initial state, then \u03b4xi = L(t0, ti)\u03b4x0, so that As indicated by (5.6.18), the matrices Hi, Li are the linearized Jacobians \u2202H/\u2202xi, \u2202M/\u2202xo. Therefore, from (5.6.16) and (5.6.18) the gradient of the 5.6 Advanced data assimilation methods cost function for a period of 12 h, observations every 3 h and the adjoint model that integrates backwards within each interval."}
{"text": "observation cost function is given by Equation (5.6.19) shows that every iteration of the 4D-Var minimization requires the computation of the gradient, i.e., computing the increments [H(xi) \u2212yo observation times ti during a forward integration, multiplying them by HT integrating these weighted increments back to the initial time using the adjoint model."}
{"text": "Since parts of the backward adjoint integration are common to several time intervals, the summation in (5.6.19) can be arranged more conveniently. Assume, for example that the interval of assimilation is from 00 Z to 12 Z, and that there are observations every 3 h (Fig. 5.6.2). We compute during the forward integration the weighted neg- ative observation increments di = HT i\u22121 applied on a vector \u201cadvances\u201d it from ti to ti\u22121. Then we can write (5.6.19) in the example shown in Fig. 5.6.2 as From (5.6.17) plus (5.6.19) or (5.6.20) we obtain the gradient of the cost function, and the minimization algorithm modi\ufb01es appropriately the control variable x(t0)."}
{"text": "4D-Var can also be written in an incremental form with the cost function de\ufb01ned and the observational increment de\ufb01ned as in (5.6.10). Within the incremental for- mulation, it is possible to choose a \u201csimpli\ufb01cation operator\u201d that solves the problem of minimization in a lower dimensional space w than that of the original model S is meant to be rank de\ufb01cient (as would be the case, for example, if a lower resolution spectral truncation was used for w than for x), so that its inverse doesn\u2019t exist, and we have to use a generalized inverse S\u2212I = [SST ]\u22121ST . Then the minimum of the and a new \u201couter iteration\u201d at the full model resolution can be carried out (Lorenc, The iteration process can also be accelerated through the use of \u201cpre-condition- ing\u201d, a change of control variables that makes the cost function more \u201cspherical\u201d, and therefore each iteration can get closer to the center (minimum) of the cost function (e.g., Parrish and Derber, 1992, Lorenc, 1997)."}
{"text": "The most important advantage of 4D-Var is that if we assume that: (a) the model is perfect, and (b) the a priori error covariance at the initial time B0 is correct, it can be shown that the 4D-Var analysis at the \ufb01nal time is identical to that of the extended Kalman \ufb01lter (Lorenc, 1986, Daley, 1991). This means that implicitly 4D-Var is able to evolve the forecast error covariance from B0 to the \ufb01nal time (Thepaut et al., 1993)."}
{"text": "Unfortunately, this implicit covariance is not available at the end of the cycle, and neither is the new analysis error covariance. In other words, 4D-Var is able to \ufb01nd the best linear unbiased estimation but not its error covariance. To mitigate this problem, a simpli\ufb01ed Kalman \ufb01lter algorithm has been proposed to estimate the evolution of the analysis errors in the subspace of the dynamically most unstable modes (Fischer and Courtier, 1995, Cohn and Todling, 1996)."}
{"text": "Finally, we mention that Bennett (1992), Bennett et al. (1997) and Egbert et al., (1994) developed a variational method with a weak constraint, rather than a strong constraint, thus accounting for the existence of model errors in the evolution of the forecast and the forecast error covariance. The forecast errors appear as random forcings on the dynamics, with an a priori forecast error covariance matrix Q. The cost function therefore becomes [xm+1 \u2212Mm(xm)]T Qm,m\u2032\u22121[xm\u2032+1 \u2212Mm\u2032(xm\u2032)] 5.7 Dynamical and physical balance in the initial conditions where the second term accounts for the model errors. In principle this cost function makes the problem equivalent to Kalman \ufb01ltering. However, the solution is made feasible using the method of representers within the observational space (Bennett, 1992, Egbert and Bennett, 1994)."}
{"text": "Formally, the representer method can be written as a four-dimensional OI x(tN) = xb(tN) + W[yo \u2212H(xb)] where the weight matrix and the observational and background vectors in d(ti) = [yo \u2212H(xb)] are de\ufb01ned not only in space but also in time (at the time of the obser- vations). The details of the method of solution are described in Egbert et al. (1994), Bennett et al. (1996), and Uboldi and Kamachi (2000). Uboldi and Kamachi (2000) showed how it is applied to the nonlinear Burger equation, and how the \u201crepresen- ters\u201d, which are the a posteriori error covariance functions for each observation, in\ufb02uence the solution beyond the time of observation."}
{"text": "Dynamical and physical balance in the initial conditions We saw in Chapter 1 that Richardson\u2019s (1922) experiment resulted in a disastrous estimation of the initial surface pressure tendency (a forecast of a change of 146 hPa in 6 h, whereas the actual pressure remained almost unchanged) be- cause of noisy data and the presence of fast inertia-gravity waves in the solu- tion of the primitive equations. If there are fast and slow waves in the solution of a model, \u03c9fast >> \u03c9slow, and u = Uslowe\u2212i\u03c9slowt + Ufaste\u2212i\u03c9fastt, then \u2202u/\u2202t = \u2212i\u03c9slowUslowe\u2212i\u03c9slowt \u2212i\u03c9fastUfaste\u2212i\u03c9fastt. As shown schematically in Fig. 1.2.1 it is clear that, unless the amplitude of the fast waves component is made very small, the fast waves will dominate the initial tendency."}
{"text": "We saw in Section 2.5 that in the SWEs, the simplest example of primitive equa- tions, there are two types of wave solutions: (a) steady or slowly evolving, quasi- geostrophically balanced \u201cweather\u201d modes, satisfying \u03c9 \u22480, and (b) fast inertia- gravity waves with a frequency dispersion relationship \u03c92 \u2248f 2 + gD(k2 + l2), where k,l are the horizontal wavenumbers, and D is the mean depth of the model. For the external mode, with about 10 km equivalent depth, inertia-gravity waves travel at a speed of about 300 m/s, and unless they are \ufb01ltered out of the initial conditions, they can indeed produce a very noisy forecast. After a while, though, the inertia-gravity waves subside as the solution evolves towards quasi-geostrophic balance (a process known as geostrophic adjustment), so that, unless they interact nonlinearly with the slower\u201cweatherwaves\u201d,theinertia-gravitywavesdonotnecessarilyruintheforecast."}
{"text": "In this section we \ufb01rst consider the geostrophic adjustment process that takes place within a linear SWE system. This also allows us to assess what types of observations are most useful for NWP. We then consider the nonlinear case and describe the non- linear normal mode initialization method, which was used for many years to reduce the imbalance in the initial conditions. Finally we describe a more recently introduced type of dynamical initialization, denoted digital \ufb01ltering, which is simple and very effective. We should note that if the analysis is out of balance, all of the initial infor- mation that projects on inertia-gravity waves will be lost, whether the inertia-gravity waves are \ufb01ltered out during the model integration through geostrophic adjustment, or through the other initialization methods. For this reason it is preferable to enforce bal- ance within the analysis as done in 3D-Var, since this reduces the loss of information."}
{"text": "Geostrophic adjustment and the relative importance of different observations If the initial conditions of a model are not in quasi-geostrophic balance, the balanced portion of the initial \ufb01eld will project on the quasi-geostrophic mode, and the unbal- ancedportionwillprojectontoinertia-gravitywaves.Thesewaveshavelargehorizon- tal divergence and propagate horizontally, dispersing quite fast. Because of horizontal dispersion, after a while the amplitude of the inertia-gravity waves becomes much smaller, and the leftover \ufb01elds remain in quasi-geostrophic balance. Rossby (1936, 1938) \ufb01rst described this process of \u201cgeostrophic adjustment.\u201d The time scale for geostrophic adjustment is of the order of f \u22121 (about 12 h). Arakawa (1997) provided an analytic solution of the linear geostrophic adjustment problem for the SWEs."}
{"text": "In Section 2.4 we showed that the potential vorticity is conserved for individual Consider small perturbations on a basic state of rest. The linearized SWE potential vorticity is obtained assuming that the relative vorticity \u22072\u03c8 is small compared with the Coriolis parameter and that the total geopotential height \u0007 = gD + \u03c6, where the perturbations to the free surface are small compared to the mean depth of the \ufb02uid."}
{"text": "In that case, the conservation of potential vorticity becomes Parcels will evolve conserving their initial potential vorticity \u03b7(0) = f + \u22072\u03c8 \u2212 ( fo/gD)\u03c6 even as they undergo the geostrophic adjustment process. This important conservation property allows us to assess how much of the initial mass and wind increments will project on the slow modes, and be \u201cremembered\u201d by the model after geostrophic adjustment, and how much information will be lost through inertia- gravity waves. If we introduce a perturbation \u03b4\u03c8(0), \u03b4\u03c6(0) in the initial conditions through data assimilation, the perturbation in potential vorticity will remain in the solution even after geostrophic adjustment: \u03b4\u03b7g = \u03b4\u03b7(0). Recall that after geostrophic adjustment, the winds become geostrophic, so that \u03c8g = \u03c6g/f0."}
{"text": "5.7 Dynamical and physical balance in the initial conditions Assume that within a univariate analysis, the introduction of observations results in an analysis increment \ufb01eld associated with either mass observations \u03b4\u03c6(x, y) or wind observations \u03b4\u03c8(x, y). After about 12\u201324 h the initial unbalanced \ufb01eld will disperse away as inertia-gravity waves, and the remaining increments will be in geostrophic \u03b4\u03c6 \u2192\u03b4\u03c6g, \u03b4\u03c8 \u2192\u03b4\u03c8gwith \u03b4\u03c8g = \u03b4\u03c6g/fo Assume that the analysis increment \ufb01eld was of the form \u03b4\u03c6 = Aei(kx+ly), with k2 + l2 = n2 = (2\u03c0/L)2. From (5.7.2), the \ufb01nal increment of potential vorticity after geostrophicadjustmentisequaltotheinitialanalysisincrement\u03b4\u03b7g(x, y) = \u03b4\u03b7(x, y)."}
{"text": "Consider the effect of introducing only mass observations (performing a univariate analysis). They will result in an analysis increment \u03b4\u03c6, and the potential vorticity initial (and \ufb01nal) increments are then The impact of the mass observation after geostrophic adjustment is therefore 0 is the Rossby radius of deformation, the natural quasi- geostrophic horizontal scale given a rotation rate and mean depth. Equation (5.7.5) indicates that the response depends strongly on whether the waves are short or long compared with R. For long waves, for which n2R2 \u226a1, the model \u201cremembers\u201d the mass data: \u03b4\u03c6g \u2248\u03b4\u03c6, i.e., it retains it after geostrophic adjustment. For short waves, for which n2R2 \u226b1, on the other hand, the model \u201cforgets\u201d the mass information:"}
{"text": "The situation is reversed for a univariate analysis of only wind data, leading to an analysis increment \u03b4\u03c8. After geostrophic adjustment, so that for long waves, n2R2 \u226a1, the model \u201cforgets\u201d the wind data: \u03b4\u03c8g \u22480, whereas wind information is retained in short waves: \u03b4\u03c8g \u2248\u03b4\u03c8."}
{"text": "Now we have to determine which waves are \u201cshort\u201d and which \u201clong\u201d. In mid- latitudes, with f0 \u223c10\u22124, for the external or barotropic mode D \u223c10 km, so that short waves are those for which n2R2 \u226b1 or L \u226a all but planetary waves are very short. This implies that the model essentially will ignore surface pressure data, which is the mass data corresponding to the external mode, and it will adjust its surface pressure to the barotropic component of the wind."}
{"text": "For the \ufb01rst internal mode D \u223c1 km, and waves are \u201cshort\u201d if shorter than about 6000 km. In other words, most of the energy in the mid-latitude atmosphere is actually in short waves. In the tropics, where f is an order of magnitude smaller, the statement that \u201cmost waves are short\u201d applies even more strongly. For this reason, winds tend to be more effective in providing initial conditions for an NWP model than mass data, but temperature data are more important for shallower vertical modes, for which the wind will adjust to the temperature observations."}
{"text": "The \u201cacceptance\u201d of wind and/or mass data is enhanced by the use of multivariate analysis schemes, because the geostrophic correlation assumed in the background error covariance enforces an approximate geostrophic balance in the analysis incre- ments. However, even with multivariate analysis, because nonlinearities do not allow for a perfect balance, wind observations still have the strongest impact on the skill of a forecast in modern data assimilation systems. The full impact of an observing system depends on the extent to which they contribute to de\ufb01ne the potential vorticity, as discussed above, and on other factors:"}
{"text": "This allows a simple estimation of the relative contributions to the analysis precision of mass and wind measurements. If we observe heights \u03c6 = f0\u03c8mass with an error \u03b4\u03c6ob = g\u03b4zob, and winds v = k \u00d7 \u2207\u03c8wind with an error |\u03b4vob|, they both contribute to the streamfunction. If we combine them optimally, the analysis error precision suggesting that the wind \ufb01eld contributes more accuracy to the analysis for short waves or in the tropics. The higher accuracy of winds for short waves is a result of the fact that they measure a gradient \ufb01eld, and is independent of the geostrophic Data coverage, both in the horizontal and in the vertical Obviously, the denser an observing system is, the more it will contribute to the accuracy of the analysis. This is true in the horizontal as well as in the vertical, so that vertical pro\ufb01les of winds, temperature, or moisture are found to be more useful than single level observations. An observing system will also contribute more to the skill of the forecasts in the absence of other observing systems. For example, in the Northern Hemisphere, which has a relatively good network of rawinsondes, the contribution of satellite data to the improvement of NWP forecasts is much lower than in the Southern Hemisphere, where rawinsondes are much fewer."}
{"text": "5.7 Dynamical and physical balance in the initial conditions Physical model adjustments and model spin-up In the same way that mass and wind initial \ufb01elds undergo a dynamical adjustment towards geostrophic balance, other variables quickly evolve towards thermal and hydrological balance within the model. For example, because of the low heat capacity of the surface layer, surface air temperatures (at 2 m) adjust very rapidly towards equilibrium with the sea or land surface temperatures. As a result, it is dif\ufb01cult to effectively use surface air temperature observations since the model will tend to \u201cforget\u201d them (replace them by model adjusted values). Similarly, if the moisture analysis pro\ufb01les are wetter or drier than what the model hydrological equilibrium would require (i.e., the model has a wet or dry bias), the analysis moisture pro\ufb01les are quickly replaced by model adjusted pro\ufb01les. Therefore, a model with a dry climatological bias will produce excessive rain during the analysis cycle, as the initial conditions bring in higher observed moisture pro\ufb01les every 6 h, and the model rains out what it perceives as \u201cexcessive moisture\u201d. This adjustment process, which is also affected by other physical parameterizations such as surface \ufb02uxes and radiation, is known as the \u201cspin-up\u201d or \u201cspin-down\u201d of the model, depending on whether there is an initial increase or decrease in the precipitation. The spin-up process is strongest immediately after the analysis and takes between 12 and 36 h before reaching model balance. It can be reduced by \u201cphysical initialization\u201d or assimilation of precipitation, in which temperature and moisture pro\ufb01les are modi\ufb01ed so that the model during the analysis cycle is forced to produce precipitation similar to the observed precipitation (e.g., Krishnamurti et al., 1988, Treadon, 1996, Falkovich et al., 2000)."}
{"text": "Fig. 5.7.1 shows the results of a comparison of different 5-day forecasts performed every 12 h for February 1998 using the 2000 operational NCEP data assimilation system (courtesy of Michiko Masutani and Stephen Lord). The forecasts are based on different data assimilation experiments, in which: (a) all available data was as- similated (ALL); (b) the satellite radiances were not assimilated (No TOVS 1B); (c) the rawinsonde temperatures were not assimilated (No RAOB Temp); and (d) the rawinsondes winds were not assimilated (No RAOB Wind). The results illustrate several conclusions, in agreement with the discussion above:"}
{"text": "In the Northern Hemisphere where the rawinsonde network is fairly abundant, the forecasts from the assimilation without satellite data No TOVS 1B, are, on the average, slightly worse than the ALL data experiments."}
{"text": "Eliminating the rawinsonde winds from the data assimilation has a much larger negative impact than eliminating the rawinsonde temperatures in the In the Southern Hemisphere, where there are relatively few rawinsondes, the satellite radiances are the backbone of the information needed in the data Day 5 Z500 Anom Corr Verified vs. Control(All) analysis Top NH, Bot SH every 12 h with the NCEP data assimilation system with different combinations of data: crosses, all data; open circles, no TOVS radiances; open squares: no rawinsonde winds; full circles, no rawinsonde temperatures. (Courtesy of M. Masutani.) assimilation. Without satellite radiances, the forecasts in the Southern Hemisphere substantially deteriorate."}
{"text": "We should also note that there is substantial day-to-day variability in the 5-day forecast skill. This can be attributed to the changes in atmospheric predictability, i.e., on some days the atmosphere is simply easier to predict than on others. This will be studied in detail in Chapter 6."}
{"text": "An approach that has been widely used to improve the initial imbalance is that of 5.7 Dynamical and physical balance in the initial conditions nonlinear normal modes initialization, introduced by Machenhauer (1977) and by Baer and Tribbia (1977). As indicated by its name, it requires the determination of the (linear) normal modes of a model as a \ufb01rst step. Daley (1991), Temperton and Williamson (1981), and others give a complete discussion about how this procedure is carried out in a three-dimensional model. Here we only illustrate how it would be applied to a simple SWE model on a periodic f-plane model. The SWEs are written separating them into their linear terms (on the left-hand sides) and the nonlinear terms (on the right-hand sides):"}
{"text": "The \ufb01rst step is to determine the (linear) normal modes, and for this purpose we \ufb01nd the eigensolutions or normal modes of (5.7.8) setting Ru = Rv = R\u03c6 = 0. In the case of a doubly periodic domain the normal modes are simply of the form \uf8fa\uf8fbei(kx+ly)e\u2212i\u03c9t = Xkle\u2212i\u03c9t In a three-dimensional model there would be an additional functional dependence on the vertical, which can be represented by a vertical wavenumber. If we plug (5.7.10) into (5.7.8) with zero on the right-hand side, we obtain three solutions for (\u03c9F)2 = f 2 + gD(k2 + l2) where the subscripts S and F refer to the slow (quasi-geostrophic) and the fast (inertia- gravity wave) modes. Because the equations are homogeneous, (U, V, F) are related to each other and the amplitude of the normal mode is arbitrary. For the slow modes, the relationship is UklS = \u2212ilFklS/f, VklS = ikFklS/f (geostrophic balance). For the fast modes, a similar but more complicated relationship exists which, if f 2 \u226a The normal modes in (5.7.10) can therefore be written as a set of slow (Ykl) and fast (Zkl) modes:Xklm = (Ykl, Zkl) where the subscript m is S for slow modes and F for fast modes. The normal modes Xklm constitute a complete orthonormal basis if we normalize them by their total energy:"}
{"text": "Any time-dependent \ufb01eld of winds and heights can be expanded in terms of the where the (time-dependent) coef\ufb01cients can be determined from a back Fourier trans- [gD(uUklm + vVklm) + \u03c6Fklm]e\u2212i(kx+ly)dydx Now we return to the full nonlinear SWE (5.7.8), multiply the three equations by respectively, add them and integrate over the domain to obtain the nonlinear equations for the amplitudes of the slow and fast modes:"}
{"text": "Here we have separated slow and fast modes. Rklm(Y, Z) is the result of applying this operation to the right-hand side nonlinear terms and depends on both the vector Y of slow modes coef\ufb01cients and on the vector Z of fast modes coef\ufb01cients. Recall (5.7.11) that for this simple geometry, \u03c9klS = 0."}
{"text": "is the linear equation for the fast modes. 5.7 Dynamical and physical balance in the initial conditions We can choose to perform a linear normal mode initialization by zeroing out the initial amplitude of the fast modes: aklF(t) = aklF(0)e\u2212i\u03c9klFt = 0 in (5.7.17). This will make the linear time derivative of the fast modes equal to zero. But (5.7.16) shows that this initialization, which is equivalent to a perfect geostrophic balance, is not accurate enough for the realistic nonlinear case. The presence of nonlinear forcing will generate fast oscillations even if the basic state is geostrophic. Therefore Machenhauer (1977) suggested instead to zero out the time derivative of the fast modes in the nonlinear equation for the fast modes in (5.7.16), whose right-hand side depends on both slow and fast modes. Therefore, from + i\u03c9FklaFkl = RFkl(Y(0), Z(0)) we obtain the nonlinear normal mode initialization condition:"}
{"text": "aFkl(0) = RFkl(Y(0), Z(0)) Since the coef\ufb01cient aklF(0) appears both in the left- and right-hand sides of (5.7.19) in a component of the vector of fast coef\ufb01cients Z, Machenauer (1977) suggested iterating equation (5.7.19) until convergence."}
{"text": "Nonlinear normal mode initialization (NLNMI) has been widely used in many op- erational data assimilation systems, since it is quite effective in substantially reducing the amplitude of the inertia-gravity waves from the initial conditions, much better than a simple geostrophic balance. It requires the determination of the linear normal modes of a model, but Temperton (1988) derived a formulation denoted \u201cimplicit NLNMI\u201d without this requirement."}
{"text": "NLNMI has some problems, however: (a) In the tropics, diabatic heating plays a fundamental role, essentially balancing the vertical advection of static stability. Therefore, diabatic forcing has to be included in the nonlinear terms, and this requires estimating the heating from short-term forecasts (Wergen, 1988)."}
{"text": "(b) There is some arbitrariness in de\ufb01ning which \u201cfast\u201d modes need to be initialized. For example, inertia-gravity waves with high vertical wavenumbers are quite slow (see the discussion in the previous subsection), so that only the \ufb01rst few vertical modes are usually initialized. On the other hand, NLNMI eliminates the high-frequency but real atmospheric tides from the solution, since they appear as fast modes. This requires a special handling (c) NLNMI is only an approximation of the true slow evolution of the atmosphere: if we apply NLNMI to a model that has been running for a day or longer, ideally it should not modify it, since it has already reached slow modes equilibrium. However, NLNMI will change the initial \ufb01elds signi\ufb01cantly."}
{"text": "Ballish et al. (1992) developed a modi\ufb01cation of the procedure denoted incremen- tal NLNMI, in which the initialization is applied to the analysis increments, rather than to the full analysis \ufb01eld. This procedure is able to substantially solve the three problems indicated above."}
{"text": "In recent years the use of NLNMI after the analysis step has become less popular because of the development of three alternative approaches. The \ufb01rst one is the use of 3D-Var, which allows the introduction within the cost function of a term that penalizes the lack of balance. Parrish and Derber (1992) included a penalty term based on the global linear balance equation applied on the analysis increments. They found that this, combined with the use of a more realistic global background error covariance based on differences between 24- and 48-h forecasts verifying at the same time, yielded an analysis that was well balanced. As a result, the NLNMI step became unnecessary in the NCEP system. This is a major advantage of 3D-Var over the standard OI procedure followed by NLNMI. It eliminates the arti\ufb01cial separation of the analysis step, which produces \ufb01elds that are close to the observations but out of balance, and the initialization step, which produces \ufb01elds that are balanced but further away from the observations."}
{"text": "Another method that seems to achieve similar balanced results and minimize the spin-up problems is the incremental analysis update (Bloom et al., 1996), in which the analysis increment is added in small \u201cdrips\u201d throughout the 6-h forecast rather than once as a large change at the analysis time. Assume that there are n time steps in the 6-h forecast. In the incremental analysis update the analysis increments are computed at the analysis time. Then the forecast at the analysis time minus 3 h is integrated for 6 h adding at each time step the analysis increment divided by n, until the forecast reaches the analysis time plus 3 h. At that time a preliminary 3-h integration of the model without analysis increments is performed until the next analysis time is reached, and the cycle is repeated. The overhead of this method is only the additional preliminary integration of the model during the second half of the interval between analysis times."}
{"text": "Dynamic initialization using digital \ufb01lters Some numerical schemes, like the Euler-backwards or Matsuno scheme, damp high frequencies, and this property has been used in order to reduce the accumulation of high-frequency noise within an assimilation cycle. Assume we have an equation The Matsuno scheme is a predictor\u2013corrector type of scheme (see Table 3.2.1) where 5.7 Dynamical and physical balance in the initial conditions so that un+1 = \u03c1un, where the ampli\ufb01cation factor is \u03c1 = (1 \u2212\u03c92\u0016t2) \u2212i\u03c9\u0016t."}
{"text": "|\u03c1|2 = (1 \u2212\u03c92\u0016t2 + \u03c94\u0016t4) It is evident from (5.7.22) that as long as the CFL stability condition |\u03c9max\u0016tC FL| \u22641 is satis\ufb01ed, high frequencies are damped at every Matsuno time step. This has been found to be reasonably satisfactory for avoiding excessive accumulation of noise in the analysis cycle (e.g., Halem et al., 1982), but the damping is slow except for high frequencies close to 1/\u0016t. Using a damping time scheme does not balance the initial \ufb01elds; rather the balance is achieved only after integrating the model for a while (e.g., 6 h in the analysis cycle). Several dynamical initialization methods using for- ward/backward integrations were suggested to balance the initial \ufb01eld (e.g., Nitta and Hovermale, 1969, Okamura, 1969). Grant (1975) suggested a more ef\ufb01cient dynamic initialization based on linear combinations of forward/backward integrations, with combinations of time steps some of which are longer than allowed by the CFL condi- tion. Dynamic initialization never became widely used, despite its simplicity, because it is not ef\ufb01cient, requiring many forward/backward iterations to substantially reduce Exercise 5.7.1: Show that the net damping at t = 0 after the application of a Matsuno time step followed by another Matsuno time step integrating backwards in time (changing the sign of \u0016t) is given by (5.7.22)."}
{"text": "Exercise 5.7.2: Find the damping of the Okamura scheme: u+ = un(1 + i\u03c9\u0016t); u\u2212= u+(1 \u2212i\u03c9\u0016t); un+1 = 2un \u2212u\u2212. The Okamura\u2013Rivas scheme is the same except that the time step cycles over three iterations: \u0016t = \u0016tC FL; 1.4\u0016tC FL; 2\u0016tC FL, resulting in an even faster damping (Grant, 1975)."}
{"text": "The introduction of dynamic initialization based on digital \ufb01ltering by Lynch and Huang (1992) and Lynch (1997) has changed this situation substantially, and es- sentially eliminated the need for NLNMI. In a digital \ufb01lter, the model is integrated forward and backward in time between \u2212tM and tM, as in regular dynamic initializa- tion. The difference is that the model \ufb01elds are used at every time step to compute a weighted average valid at the initial time t = 0, and the weights are optimally chosen in order to damp high frequencies, rather than simply using a damping time scheme."}
{"text": "The idea of a digital \ufb01lter is to choose the \ufb01ltering weights in such a way that for low frequencies \u03c9 \u226a\u03c9s the amplitude of the solution with that frequency remains mostly unchanged, whereas for high frequencies \u03c9 \u2265\u03c9s the amplitude is substan- tially reduced. Given a time step \u0016t and the corresponding threshold computational frequency \u03b8s = \u03c9S\u0016t, low frequencies are characterized by 0 \u2264|\u03b8| \u226a\u03b8s, and the high frequencies that we want to \ufb01lter by \u03b8s \u2264|\u03b8| \u2264\u03c0."}
{"text": "where x0 = 1/ cos(\u03b8S/2), since for the low-frequency range, H(\u03b8) falls from 1 to r = 1/T2M(x0) as |\u03b8| goes from 0 to \u03b8s, and for the high frequency range \u03b8S \u2264|\u03b8| \u2264 \u03c0, H(\u03b8) oscillates within \u00b1r (e.g., see Fig. 5.7.2)."}
{"text": "From the de\ufb01nition of the Tchebychev polynomials it can be shown that theta=2pi*delta t/period for Dolph\u2013Tchebychev filter Damping of original amplitude Matsuno forward step followed by backward step Dolph\u2013Tchebychev \ufb01lter with \u03b8s = \u03c0/3, M = 3, and a forward/backward Matsuno dynamical initialization."}
{"text": "so that at the end of this procedure the amplitude of each frequency in u is modulated by H(\u03b8). The parameters are chosen in the following way: Choose a period \u03c4S such that waves with periods shorter than this are to be \ufb01ltered. The cut-off frequency is then given by \u03b8s = 2\u03c0\u0016t/\u03c4s. The time span of the integration Ts = 2tM = 2M\u0016t gives a \ufb01lter of order N = 2M + 1."}
{"text": "In practice, Lynch (1997) recommended to \ufb01rst perform a backward integra- tion with just the dry adiabatic dynamics (since they are reversible), from t = 0 to t = \u2212TS. An application of the weighted average (5.7.28) gives a \ufb01ltered \ufb01eld centered at t = \u2212TS/2. Then a forward integration from t = \u2212TS/2 to t = +TS/2 using the full model with physics results in a \ufb01eld centered at t = 0, \ufb01ltered for the second time including \ufb01ltering of the effects of irreversible diabatic processes. In the high resolution-limited area model, with a time step of half an hour, and a \ufb01ltering period of 3 h, the time span is also half an hour, with N = 2M = 7. The double \ufb01lter gives a reduction in energy of the high frequencies of more than 99%."}
{"text": "Dolph\u2013Tchebychev \ufb01lter with \u0016t = 30 min, \u03b8s = \u03c0/3 (\ufb01ltering periods shorter than 3 h) and M = 3. It compares this with the result of using one and three iterations of the forward/backward Matsuno time step."}
{"text": "The \ufb01lter is similar to the Dolph\u2013Tchebychev one, but now the Lanczos weights in Fig. 5.7.3 compares the responses of the Dolph\u2013Tchebychev and Lanczos \ufb01lters re- quiring the same number of time steps (6), since for the Lanczos \ufb01lter the coef\ufb01cients for n = 4 and 5 are zero. The response for the Lanczos \ufb01lter with \u03b8crit = \u03c0/4, M = 5 is competitive with the Dolph\u2013Tchebychev \ufb01lter."}
{"text": "In summary, initialization using digital \ufb01ltering is a very simple process that avoids the determination of the model normal modes and the need of NLNMI. Most im- portantly, it dampens the high-frequency solutions according to their actual model- determined frequency rather than from an arbitrary separation into inertia-gravity waves and quasi-geostrophic modes. It does not make any additional approximation \u03b8s = \u03c0/3, M = 3, and the Lanczos \ufb01lter, with \u03b8crit = \u03c0/3, M = 4 and \u03b8crit = \u03c0/4, M = 5. All the choices require six time integrations and are therefore computationally comparable."}
{"text": "Exercise 5.7.3: Is it better to achieve balance within 3D-Var or to apply a digital Quality control of observations The reported atmospheric observations used in data assimilation are not perfect; they contain several kinds of errors, including instrumental errors and errors of human origin. The reported observations may also contain \u201cerrors of representativeness\u201d, i.e., actually correct observations may re\ufb02ect the presence of a subgrid-scale at- mospheric phenomenon that cannot be resolved by the model or the analysis. The representativeness error indicates the observation is not representative of the areally averaged measurement required by the model grid. The instrumental and represen- tativeness errors can be systematic or random. Systematic errors and biases should be determined by calibration or other means such as time averages. Random errors are generally assumed to be normally distributed."}
{"text": "In addition to randomly distributed errors, the reported observations may contain errors that are so large that the observations have no useful information content and should be tossed out. Frequently, these rough or gross errors are of human origin, and take place during the computation or the transmission of the observation. There 5.8 Quality control of observations are other sources of observation errors (wrong date, time or location, uncalibrated instruments, etc.). The use of an observation with a rough error can cause a dispro- portionally big error in the analysis, so there has been a tendency to use observations conservatively (\u201cwhen in doubt, throw it out\u201d). In recent years, however, quality con- trol systems have become more sophisticated and many observations that would have been thrown out in the past are now corrected, resulting in an improvement in the initial conditions, and hence in the forecasts. Newer quality control systems allow for a continuous weighting of the observation suspected of having a gross error, rather than a \u201cyes or no\u201d decision to toss it out."}
{"text": "Quality control is based on a comparison between observations and some kind of expected value (which could be based on climatology, an average of nearby ob- servations, or the \ufb01rst guess). The difference between the expected value and the reported observation is denoted the \u201cresidual\u201d. If the residual is very large (measured in standard deviations of the estimate), the observation may be considered to be erro- neous. The more sophisticated the estimate of the expected value (i.e., the smaller its expected standard deviation with respect to the true value), the more discriminating the quality control algorithm will be, i.e., the better it will be able to distinguish be- tween observations with large errors, which should not be given credence, and correct observations reporting unusual states of the atmosphere, such as very low pressures or unusually high winds reported within an area affected by a tropical cyclone. In the latter case it is important to keep the observation in order to improve the initial conditions of the forecast."}
{"text": "Earlier quality control systems were based on several checks performed in series (one after another) before the analysis. For example DiMego et al. (1985) compared each observation with a climatological distribution to see whether it was within a reasonable range (\u201cgross check\u201d). If the reported value was outside a prescribed range, or differed from the climatological mean by more than, for example, \ufb01ve standard deviations, the observation was tossed out. If the observation survived this test, it was then compared to the average of nearby observations, and again tossed out if outside a reasonable range. This check (called a \u201cbuddy check\u201d, comparing the observations with their \u201cbuddies\u201d) could also salvage an observation previously tossed out even if it was quite different from the expected climatological value."}
{"text": "An analysis is, in principle, more accurate than either the \ufb01rst guess or the ob- servation. This led to the development of an \u201cOI\u201d quality control: each observation is compared with a simple OI value that would be obtained at the observation lo- cation using the \ufb01rst guess (background) \ufb01eld and nearby observations, but without including the observation being checked (Lorenc, 1981, Woollen, 1991). When the residual (difference between the observed and analyzed value) is larger than a certain number of analysis error standard deviations, it is tossed out. The analysis is iterated so some observations may be salvaged after \ufb01rst being tossed out."}
{"text": "The basic idea is to estimate several independent residuals and then apply a decision making algorithm based on the information provided by all the independent residuals, rather than performing decisions either in a sequential order, or as a single OI quality control check using all the information at once. Collins and Gandin (1990) and Collins (1998) applied this approach to rawinsonde heights and temperatures with great success. The power of this method lies in the fact that several independent checks can support each other and reduce the level of uncertainty. Furthermore, if the residuals are large and agree reasonably well with each other, they provide the basis for a correction of the observation."}
{"text": "The independent residuals obtained from the different checks used at NCEP for the complex quality control of rawinsondes temperature and heights are: (1) Incre- mental check (the residual is the increment between the reported observation and the 6-h forecast) (2) Horizontal check based on a simple OI horizontal analysis of the increments, using one observation per quadrant, and only observations within 1000 km of the reported observation. The horizontal residual is the difference be- tween the increment at the observation location and the horizontally interpolated value. (3) Vertical check: the vertical residual is the difference between the observed increment and the increment interpolated vertically from the nearest data points for the same station, one above and one below. (4) Hydrostatic check, the most powerful of the checks since it takes advantage of the redundancy between temperature and height information reported in rawinsonde observations. The hydrostatic residual is the difference between the values of the thickness of a layer between mandatory level heights, calculated using the reported heights, and the thickness calculated in- dependently from the reported virtual temperatures. (5) Baseline check computed by making a hydrostatic computation downward, from the \ufb01rst mandatory level above the surface with complete heights and temperatures, to the reported surface pressure."}
{"text": "The baseline residual is the difference between the station elevation, given by the report, and the hydrostatically determined height at the surface pressure. Another possible check used at NCEP is based on a temporal interpolation of observations at the same station 12 or 24 h before and after the observation time. This check is particularly useful for isolated stations within a \u201creanalysis\u201d mode. It is also possible to perform a check of the stability of the lapse rate."}
{"text": "Collins and Gandin (1990) and Collins (1998) developed a sophisticated decision making algorithm that makes generally con\ufb01dent decisions correcting computation or communication errors of human origin. They assumed that human errors have a simple structure: a single digit or a sign is wrong or missing. The following example of such error detected at a single level in the heights is typical:"}
{"text": "Example of CQC and the decision making algorithm correction of gross errors (Collins and Gandin, 1998) Reported 1000 hPa height: 8 m Incremental residual: \u221272 m; Vertical residual: \u221266 m; 5.8 Quality control of observations Hydrostatic residual (using signi\ufb01cant levels): 60 m; Hydrostatic (using only mandatory levels): 65 m; Baseline residual: \u221258 m."}
{"text": "With this information, the decision making algorithm concluded that a simple correction changing one digit (adding 60 m to make the observation 68 m instead of the reported 8 m) could be con\ufb01dently made."}
{"text": "The NCEP global system has also an OI-based complex quality control for all other data (winds, moisture, satellite retrievals) that makes the \ufb01rst three checks discussed above and rejects observations with large residuals but does not attempt to make corrections, since the strong redundancy of the hydrostatic check is not available for variables other than rawinsonde temperatures and heights (Woollen, The effect of modern quality control systems is dif\ufb01cult to gauge, but Kistler et al. (2001) showed an impressive example of the positive impact from the mod- ern approach compared with the quality control that was operational at NCEP in the 1970s. They pointed out that in 1974 NMC (now NCEP) introduced a modern observation formatting system (known as Of\ufb01ce Note 29, ON29), which later be- came the basis of the of\ufb01cial World Meteorological Organization (WMO) Binary Universal Format Representation system for the encoding of observations. ON29 in- cluded more information about the observation than previously used encoders. This change in formatting required a complete overhaul of the NMC decoding system, and errors must have been introduced during this complex reprogramming process."}
{"text": "The NMC operational forecast skill actually went down and it took a few years before it recovered to the pre-1974 error levels (Kalnay et al., 1998). During the production of the NCEP/NCAR reanalysis (Kalnay et al., 1996), both the com- plex quality control for rawinsonde heights and temperatures and the OI quality control for other observations were used to screen observations. The complex qual- ity control found and corrected an unusually large number of rawinsondes errors starting in 1974, and, presumably as a result of this correction and the OI qual- ity control screening of the other information, the bene\ufb01t of using the more ad- vanced formatting system ON29 became realized. In the reanalysis, the forecast skill increased substantially in 1974, rather than deteriorating as in the operational Another approach that has also become popular is variational quality control performed within 3D-Var or 4D-Var, rather than before the analysis, like OI and complex quality controls (Purser, 1984, Lorenc and Hammon, 1988, Ingleby and Lorenc, 1993, Andersson and Jarvinen, 1999, Collins, 2001a,b). It has the advan- tage that it is performed as part of the analysis itself, rather than as a preprocess- ing step like OI quality control, but because it computes a single (iterative) resid- ual for each observation, it is not able to correct observations like complex quality The variational quality control approach is based on modifying the observational component Jo of the variational cost function J = Jb + Jo to take into account the possibility of gross errors. Note that in the variational analysis approach, the gradient \u2207yo Jo of the cost function with respect to an observation yo determines how quickly the analysis estimate x will shift towards that observation (Fig. 5.8.1(b))."}
{"text": "A = 0.05, d = 5. (b) Observational cost function derived from the logarithm of the error distributions in (a). (c) Weight factor applied to the gradient of the cost function for different values of the a priori probability of gross errors A and the width of the 5.8 Quality control of observations Consider the cost function term for a single uncorrelated observation yo without allowing for gross errors:"}
{"text": "where x is a close approximation of the true value (analysis). In variational quality control one assumes that there is an a priori probability A of having a gross error, esti- mated from past statistics. 1 \u2212A is then the a priori probability of not having a gross error, in which case the observation is assumed to have random errors with a Gaussian distribution. Without gross errors (A = 0), the probability that an observation yo, if H(x) is the true value, is given by the normal distribution is where (as discussed in Section 5.3) J N o = \u2212ln N + const. (Fig. 5.8.1(a), full line) and the constant is chosen arbitrarily to make J N(0) = 0."}
{"text": "where F is a \ufb02at (uniform) distribution for the gross errors: Here d is the maximum number of standard deviations allowed for gross errors (e.g., d = 5). If |yo \u2212H(x)| > d\u03c3o, it is assumed that the observation was so obviously wrong that it was eliminated in a preliminary check against climatology or the back- ground. The integral of F is therefore equal to 1."}
{"text": "We can then modify the contribution to the cost function made by the obser- vation by including the probability of gross errors: J QC (Fig. 5.8.1(a), full line). Since the \ufb02at probability distribution of gross errors does not depend on the value yo, the modi\ufb01ed gradient of the cost function is:"}
{"text": "is the a posteriori probability of having a gross error (after making the observation yo). In other words, the gradient of the cost function including variational quality control is the gradient without variational quality control multiplied by a weight (the probability of NOT having a gross error), which is close to 1 for |yo \u2212H(x)|/\u03c3o < d and goes to zero for |yo \u2212H(x)|/\u03c3o \u2265d. Figure 5.8.1(c) shows that the weights are not very sensitive to the choice of parameters. A small value for the a priori probability of gross errors A will result in a steeper reduction of weights."}
{"text": "Because H(x) is assumed to be close to the truth, at ECMWF variational quality control is not turned on (i.e., the weight multiplying the gradient during the mini- mization is W = 1) during the \ufb01rst 40 iterations of the 4D-Var algorithm, while the solution starts to converge towards the analysis. It is then turned on (W = W QC) for the last 30 iterations of the 4D-Var, thus giving less weight to observations that are likely to contain gross errors (Andersson and Jarvinen, 1999)."}
{"text": "Atmospheric predictability and Introduction to atmospheric predictability In his 1951 paper on NWP, Charney indicated that he expected that even as models improved there would still be a limited range to skillful atmospheric predictions, but he attributed this to inevitable model de\ufb01ciencies and \ufb01nite errors in the initial condi- tions. Lorenz (1963a,b) discovered the fact that the atmosphere, like any dynamical system with instabilities, has a \ufb01nite limit of predictability (which he estimated to be about two weeks) even if the model is perfect, and even if the initial conditions are known almost perfectly. He did so by performing what is now denoted an \u201cidentical twin\u201d experiment: he compared two runs made with the same model but with initial conditions that differed only very slightly. Just from round-off errors, he found that after a few weeks the two solutions were as different from each other as two random trajectories of the model."}
{"text": "Lorenz (1993) described how this fundamental discovery took place: His original goal had been to show that statistical prediction could not match the accuracy at- tainable with a nonlinear dynamical model, and therefore that NWP had a potential for predictive skill beyond that attainable purely through statistical methods. He had acquired a Royal-McBee LGP-30 computer, with a memory of 4K words and a speed of 60 multiplications per second, which for the late 1950s was very powerful. He developed and programmed in machine language a \u201clow-order\u201d atmospheric model (i.e., a model whose evolution was described by only 12 variables) driven by external heating and damped by dissipation. During 1959 he changed parameters in the model for several months trying to \ufb01nd a nonperiodic solution (since a periodic solution would be perfectly predictable from past statistics, and that would have defeated his 6 Atmospheric predictability and ensemble forecasting purpose). He submitted a preliminary title, \u201cThe statistical prediction of solutions of dynamical equations\u201d, to the NWP conference that was going to take place during 1960 in Tokyo, gambling that he would indeed be able to \ufb01nd, for the \ufb01rst time in history, a nonperiodic numerical solution. After making the external heating a func- tion of both latitude and longitude, he \ufb01nally found the nonperiodic behavior that he was seeking. He rounded off and printed the evolution of the variables with three signi\ufb01cant digits, which seemed suf\ufb01cient to de\ufb01ne the state of the model with plenty of accuracy. After running the model for several simulated years and satisfying him- self that the solution had no periodicities, he decided to repeat part of an integration in more detail. When he came back from a coffee break, Lorenz found that the new solution was completely different from the original run. Before calling service for the computer, he checked the results and found that at the beginning the new run did coincide with the original printed numbers, but that after a few days the last digit became different, and then the next one, and after about two months any resemblance with the original integration disappeared (Lorenz, 1993):"}
{"text": "The initial round-off errors were the culprits; they were steadily amplifying until they dominated the solution. In today\u2019s terminology, there was chaos. . . . It soon struck me that, if the real atmosphere behaved like the simple model, long-range forecasting would be impossible. . . . In due time I convinced myself that the ampli\ufb01cation of small differences was the cause of the lack of periodicity. Later, when I presented my results at the Tokyo meeting, I added a brief description of the unexpected response of the equations to the round-off errors."}
{"text": "Lorenz (1963a,b) thus discovered the fundamental theorem of predictability: Un- stable systems have a \ufb01nite limit of predictability, and conversely, stable systems are in\ufb01nitely predictable (since they are either stationary or periodic), as suggested by the schematic Fig. 6.1.1."}
{"text": "In his 1972 talk \u201cPredictability: does the \ufb02ap of a butter\ufb02y\u2019s wings in Brazil set off a tornado in Texas?\u201d Lorenz further reviewed basic ideas on atmospheric predictability . . . I am proposing that over the years minuscule disturbances neither increase nor decrease the frequency of occurrence of various weather events such as tornadoes; the most that they can do is to modify the sequence in which these events occur. The question which really interests us is whether they can even do this \u2013 whether, for example, two particular weather situations differing by as little as the immediate in\ufb02uence of a single butter\ufb02y will generally after suf\ufb01cient time evolve into two situations differing by as much as the presence of a tornado. In more technical language, is the behavior of the atmosphere unstable with respect to perturbations of small amplitude?1 1 Gleick (1987) pointed out that the concept of a \u201cbutter\ufb02y effect\u201d existed in some form from literary sources long before Lorenz\u2019s work. There is a short story by Ray Bradbury which deals with this nearly as well as Lorenz does, right down to the butter\ufb02y. In this story, A Sound of 6.1 Introduction to atmospheric predictability illustrating trajectories of:"}
{"text": "(a) a dynamical system with initially are, inevitably drift system with stationary or possible transient stage, the trajectories stay close to each other, i.e., they become The connection between this question and our ability to predict the weather is evident. Since we do not know how many butter\ufb02ies there are, nor whether they are all located, let alone which ones are \ufb02apping their wings at any instant, we cannot, if the answer to our question is af\ufb01rmative, accurately predict the occurrence of tornadoes at a suf\ufb01cient distant future time. More signi\ufb01cantly, our general ability to detect systems as large as thunderstorms when they slip between weather stations may impair our ability to predict the general weather pattern even in the near future. . . ."}
{"text": ". . . The evidence [that the answer to the question whether the atmosphere is unstable is af\ufb01rmative] is overwhelming. The most signi\ufb01cant results are the 1. Small errors in the coarser structure of the weather pattern \u2013 those features which are readily resolved by conventional observing networks \u2013 tend to double in about three days.2 As the errors become larger the growth rate subsides. This limitation alone would allow us to extend the range of acceptable prediction by three days every time we cut the observation error in Thunder, some time-travelers go back to the prehistoric era and are very careful not to touch anything lest they alter the future to which they wish to return. When they return to the present, they \ufb01nd everything altered (for the worse it seems to them). It turns out one of them had accidentally stepped on a butter\ufb02y. \u2018 \u201cNot a little thing like that! Not a butter\ufb02y!\u201d cried Eckels."}
{"text": "It fell to the \ufb02oor, an exquisite thing, a small thing that could upset balances and knock down a line of small dominoes and then big dominoes and then gigantic dominoes, all down the years across Time. Eckels\u2019 mind whirled. It couldn\u2019t change things. Killing one butter\ufb02y couldn\u2019t be that important! could it?\u2019. This story was \ufb01rst published in 1952. (Courtesy of Bill Martin, pers."}
{"text": "6 Atmospheric predictability and ensemble forecasting half, and would offer the hope of eventually making good forecasts several 2. Small errors in the \ufb01ner structure \u2013 e.g. the positions of individual clouds \u2013 tend to grow much more rapidly, doubling in hours or less. This limitation alone would not seriously reduce our hopes for extended-range forecasting, since ordinarily we do not forecast the \ufb01ner structure at all."}
{"text": "3. Errors in the \ufb01ner structure, having attained appreciable size, tend to produce errors in the coarser structure. This result, which is less \ufb01rmly established than the previous ones, implies that after a day or so there will be appreciable errors in the coarser structure, which will thereafter grow just as if they had been present initially. Cutting the observations in the \ufb01ner structure in half \u2013 a formidable task \u2013 would extend the range of acceptable prediction of even the coarser structure only by hours or less. The hopes for predicting two weeks or more in advance are thus greatly diminished."}
{"text": "4. Certain special quantities such as weekly average temperatures and weekly total rainfall may be predictable at a range at which entire weather patterns Since the early days of Lorenz\u2019s momentous discovery, which gave impetus to the new science of chaos,3 additional progress has been made, but his \ufb01ndings have not been changed in any fundamental way. In NWP, substantial progress has been made through the realization that the chaotic behavior of the atmosphere requires the replacement of single \u201cdeterministic\u201d forecasts by \u201censembles\u201d of forecasts with differences in the initial conditions and in the model characteristics that re- alistically re\ufb02ect the uncertainties in our knowledge of the atmosphere. This real- ization led to the introduction of operational ensemble forecasting at both NCEP and ECMWF in December 1992. It also led to work on extending the usefulness of NWP forecasts through a systematic exploitation of the chaotic nature of the Brief review of fundamental concepts Lorenz (1963a) introduced a three-variable model that is a prototypical example of chaos theory. These equations were derived as a simpli\ufb01cation of Saltzman\u2019s (1962) nonperiodic model for convection. Like Lorenz\u2019s (1962) original 12-variable model, the three-variable model is a dissipative system. This is in contrast to Hamiltonian 3 It should be noted that Poincar\u00b4e (1897, see Alligood et al., 1997) had already discovered that the planetary system is chaotic, i.e., that the orbits of the planets cannot be predicted well beyond a certain number of (millions of) years. He showed this for the simplest three-body problem of two stars with circular orbits moving on a plane around their center of mass, and a third \u201casteroid\u201d with negligible mass in comparison with the \ufb01rst two, moving in the same plane. He found that the motion of the third body was sensitively dependent on the initial conditions, the hallmark of chaos (Alligood et al., 1997)."}
{"text": "6.2 Brief review of fundamental concepts about chaotic systems systems, which conserve total energy or some other similar property of the \ufb02ow. The systemisnonlinear(itcontainsproductsofthedependentvariables)butautonomous (the coef\ufb01cients are time-independent). Sparrow (1982) wrote a whole book on the Lorenz three-variable model that provides a nice introduction to the subject of chaos, bifurcations and strange attractors. Lorenz (1993) is a superbly clear introduction to chaos with a very useful glossary of the nomenclature used in today\u2019s literature."}
{"text": "The Lorenz (1963a) equations are The solution obtained by integrating the differential equations in time is called a \ufb02ow. The parameters \u03c3, b,r are kept constant within an integration, but they can be changed to create a family of solutions of the dynamical system de\ufb01ned by the differential equations. The particular parameter values chosen by Lorenz (1963a), \u03c3 = 10, b = 8/3, r = 28, result in chaotic solutions (sensitively dependent on the initial conditions), and since this publication they have been widely used in many papers. The solution of a time integration from a given initial condition de\ufb01nes a trajectory or orbit in phase space. The coordinates of a point in phase space are de\ufb01ned by the simultaneous values of the independent variables of the model, x(t), y(t), z(t). The dimension of the phase space is equal to the number of inde- pendent variables (in this case three). The dimension of the subspace actually visited by the solution after an initial transient period (i.e., the dimension of the attractor) can be much smaller than the dimension of the phase space. A volume in phase space can be de\ufb01ned by a set of points in phase space such as a hypercube V = \u03b4x\u03b4y\u03b4z, a hypersphere V = {\u03b4r; |\u03b4r| \u2264\u03b5}, etc."}
{"text": "The fact that the Lorenz system (6.2.1) is dissipative can be seen from the diver- which shows that an original volume V contracts with time to V e\u2212(\u03c3+b+1)t. This proves the existence of a bounded globally attracting set of zero volume (i.e., an attractor of dimension smaller than n, the dimension of the phase space). A solu- tion may start from a point away from the attracting set but it will eventually settle on the attractor. This initial portion of the trajectory is known as a transient. The attracting set (the set of points approached again and again by the trajectories after 6 Atmospheric predictability and ensemble forecasting the transients are over) is called the attractor of the system. The attractor can have several components: stationary points (equilibrium or steady state solutions of the dynamical equations), periodic orbits, and more complicated structures known as strange attractors (which can also include periodic orbits). The different compo- nents of the attractor have corresponding basins of attraction in the phase space, which are all the initial conditions that will evolve to the same attractor. The fact that any initial volume in phase space contracts to zero with time is a general prop- erty of dissipative bounded systems, including atmospheric models with friction."}
{"text": "Hamiltonian systems, on the other hand, are volume-conserving. If we change the parameters of a dynamical system (in this example \u03c3, b,r) and obtain families of solutions, we \ufb01nd that there is a point at which the behavior of the \ufb02ow changes abruptly. The point at which this sudden change in the charac- teristics of the \ufb02ow occurs is called a bifurcation point. For example in Lorenz\u2019s equations the origin is a stable, stationary point for r < 1, as can be seen by investi- gating the local stability at the origin. The local stability of a point can be studied by linearizing the \ufb02ow about the point and computing the eigenvalues of the lin- ear \ufb02ow. For r < 1 the stationary point is stable: all three eigenvalues are negative."}
{"text": "This means that all orbits near the origin tend to get closer to it. At r = 1 there is a bifurcation, and for r > 1 two new additional stationary points C\u00b1 are born, with coordinates (x, y, z)\u00b1 = (\u00b1\u221ab(r \u22121),\u00b1\u221ab(r \u22121),r \u22121). For r > 1 the origin be- comes nonstable: one of the three eigenvalues becomes positive (while the other two remain negative), indicating that the \ufb02ow diverges locally from the origin in one direction. For 1 < r < 24.74. . . , C+ and C\u2212are stable, and at r = 24.74. . . there is another bifurcation so that above that critical value C+ and C\u2212also become unstable. As discussed by Lorenz (1993), a ubiquitous phenomenon is the occur- rence of bifurcations of periodic motion leading to period doubling, and sequences of period doubling bifurcations leading to chaotic behavior (see Sauer et al., A solution of a dynamical system can be de\ufb01ned to be stable if it is bounded, and if any other solution once suf\ufb01ciently close to it remains close to it for all times."}
{"text": "This indicates that a bounded stable solution must be periodic (repeat itself exactly) or at least almost periodic, since once the trajectory approaches a point in its past history, the trajectories will remain close forever (Fig. 6.1.1(b)). A solution that is not periodic or almost periodic is therefore unstable: two trajectories that start very close will eventually diverge completely (Fig. 6.1.1(a))."}
{"text": "Thelong-termstabilityofadynamicalsystemofn-variablesischaracterizedbythe Lyapunov exponents. Consider a point in a trajectory, and introduce a (hyper)sphere of small perturbations about that point. If we apply the model to evolve each of those perturbations, we \ufb01nd that after a short time the sphere will be deformed into a (hyper)ellipsoid. In an unstable system, at least one of the axes of the ellipsoid will become larger with time, and once nonlinear effects start to be signi\ufb01cant the ellipsoid will be deformed into a \u201cbanana\u201d (Fig. 6.2.1). Consider the linear phase, 6.2 Brief review of fundamental concepts about chaotic systems (a) Initial volume: a small (c) Nonlinear phase: folding needs to take place in order for the solution to stay within the bounds (d) Asymptotic evolution to a strange attractor of zero volume and fractal structure. All (b) Linear phase: a hyper in a bounded dissipative system. Initially (during the linear phase) the volume is stretched into an ellipsoid while the volume decreases. The solution space is bounded, and a bound is schematically indicated in the \ufb01gure by the hypercube. The ellipsoid continues to be stretched in the unstable directions, until (because the solution phase space is bounded) it has to fold through nonlinear effects. This stretching and folding continues again and again, evolving into an in\ufb01nitely foliated (fractal) structure. This structure, of zero volume and fractal dimension, is called a \u201cstrange attractor.\u201d The attractor is the set of states whose vicinity the system will visit again and again (the \u201cclimate\u201d of the system). Note that in phases (a), (b), and (c), there is predictive knowledge: we know where the original perturbations generally are. In (d), when the original sphere has evolved into the attractor, all predictability is lost: we only know that each original perturbation is within the climatology of possible solutions, but we don\u2019t know where, or even in which region of the attractor it may be."}
{"text": "during which the sphere evolves into an ellipsoid. We can maintain the linear phase for an in\ufb01nitely long period by taking an in\ufb01nitely small initial sphere, or, alternatively, by periodically scaling down the ellipsoid dimensions dividing all its dimensions by the same scalar. Each axis j of the ellipsoid grows or decays over the long term by amountsgivenbye\u03bb jt,wherethe\u03bb jsaretheLyapunovexponentsorderedbysize\u03bb1 \u2265 \u03bb2 \u2265. . . \u2265\u03bbn. The total volume of the ellipsoid will evolve like V0e\u2212(\u03bb1+\u03bb2+\u00b7\u00b7\u00b7\u03bbn)t."}
{"text": "Therefore, a Hamiltonian (volume-conserving) system is characterized by a sum 6 Atmospheric predictability and ensemble forecasting of Lyapunov exponents equal to zero, whereas for a dissipative system the sum is Because the attractor of a dissipative system is bounded (the trajectories are en- closed within some hyperbox), if the \ufb01rst Lyapunov exponent is greater than zero, at least one of the axes of the ellipsoid keeps getting longer with time. The ellipsoid will eventually be distorted into a banana shape: it has to be folded in order to continue \ufb01tting into the box. The banana will be further stretched along the unstable axis and then necessarily folded again and again onto itself in order to continue \ufb01tting into the box. Since the volume of the ellipsoid eventually goes to zero for a dissipative system, the repeated stretching and folding of the ellipsoid of a chaotic system eventually converges to a zero-volume attractor with an in\ufb01nitely foliated structure (a process similar to the stretching and folding used to make \u201cphyllo\u201d dough!). This structure is known as \u201cstrange attractor\u201d (Ruelle, 1989). It has a fractal structure: a dimension which in general is not an integer and is smaller than the original space dimension n, estimated by Kaplan and Yorke (1979) to be where the sum of the \ufb01rst k Lyapunov exponents is positive, and the sum of the \ufb01rst k + 1 exponents is negative. If the system is Hamiltonian, its invariant manifold has the same dimension as the phase space."}
{"text": "In summary, a stable system has all Lyapunov exponents less than or equal to zero. A chaotic system has at least one Lyapunov exponent greater than zero: if at least \u03bb1 > 0 chaotic behavior will take place because at least one axis of the ellipsoid will be continuously stretched, leading to the separation of orbits originally started closely along that axis. Note that a chaotic bounded \ufb02ow must also have a Lyapunov exponent equal to zero, with the corresponding local Lyapunov vector parallel to an orbit. This can be understood by considering two initial conditions such that the second is equal to the \ufb01rst after applying the model for one time step. The solutions corresponding to these initial conditions will remain close together, since the second orbit will always be the same as the \ufb01rst orbit shifted by one time step, and on the average, the distance between the solutions will remain constant. If we add a tiny perturbation, though, the second solution will diverge from the \ufb01rst one because there is a positive Lyapunov exponent."}
{"text": "Tangent linear model, adjoint model, singular vectors, and Lyapunov vectors In 1965 Lorenz published another paper based on a low-order model that behaved like the atmosphere. It was a quasi-geostrophic two-level model in a periodic chan- nel, with a \u201cLorenz\u201d vertical grid (velocity and temperature variables de\ufb01ned at the same two levels, see Section 3.3), and a spectral (Fourier) discretization in longitude 6.3 Tangent linear and adjoint models, singular and Lyapunov vectors and latitude. By keeping only two Fourier components in latitude and three in lon- gitude, and choosing appropriate values for the model parameters, he was able to \ufb01nd a model able to reproduce baroclinic instability and nonlinear wave interactions with just 28 variables. In this fundamental paper, Lorenz introduced for the \ufb01rst time (without using their current names) the concepts of the tangent linear model, adjoint model, singular vectors, and Lyapunov vectors for the low-order atmospheric model, and their consequences for ensemble forecasting. He also pointed out that the pre- dictability of the model is not constant with time: it depends on the stability of the evolving atmospheric \ufb02ow (the basic trajectory or reference state). In the following introduction to these subjects we follow Lorenz (1965), Szunyogh et al. (1997) and Tangent linear model and adjoint model Consider a nonlinear model. Once it has been discretized in space using, for example, \ufb01nite differences or a spectral expansion leading to n independent variables (or degrees of freedom), the model can be written as a set of n nonlinear coupled ordinary This is the model in differential form. Once we choose a time-difference scheme (e.g., Crank\u2013Nicholson, see Table 3.2.1), it becomes a set of nonlinear-coupled dif- ference equations. Typically, an atmospheric model consists of one such system of difference equations which, for example, using a two-time level Crank\u2013Nicholson scheme would be of the form A numerical solution of (6.3.1) starting from an initial time t0 can be readily obtained by integrating the model numerically using (6.3.2) between t0 and a \ufb01nal time t (i.e., \u201crunning the model\u201d). This gives us a nonlinear model solution that depends only on the initial conditions:"}
{"text": "This system of linear ordinary differential equations is the tangent linear model in differential form. Its solution between t0 and t can be obtained by integrat- ing (6.3.5) in time using the same time difference scheme used in the nonlinear Here L(t0, t) = \u2202M/\u2202x is an (n \u00d7 n) matrix known as the resolvent or propagator of the tangent linear model: it propagates an initial perturbation at time t0 into the \ufb01nal perturbation at time t. Because it is linearized over the \ufb02ow from t0 to t, L depends on the basic trajectory x(t) (the solution of the nonlinear model), but it does not depend on the perturbation y. (The original nonlinear model is autonomous since F(x) depends on x(t) but not explicitly on time, but the linear tangent model is nonautonomous). Lorenz (1965) introduced the concept of the tangent linear model of an atmospheric model, but he actually obtained it directly from (6.3.4), neglecting terms quadratic or higher order in the perturbation y:"}
{"text": "He did so by creating as initial perturbations a \u201csphere\u201d of small perturbations of size \u03b5 along the n unit basis vectors yi(t0) = \u03b5ei and applying (6.3.7) to each of these perturbations. With this choice of initial perturbations, subtracting (6.3.3) he obtained the matrix that de\ufb01nes the tangent linear model:"}
{"text": "The Euclidean norm of a vector is the inner product of the vector with itself: The Euclidean norm of y(t) is therefore related to the initial perturbation by \u2225y(t)\u22252 = (Ly(t0))T Ly(t0) = \u27e8Ly(t0), Ly(t0)\u27e9= \u27e8LT Ly(t0), y(t0)\u27e9 The adjoint of an operator K is de\ufb01ned by the property \u27e8x, Ky\u27e9\u2261\u27e8KT x, y\u27e9. In this case of a model with real variables, the adjoint of the tangent linear model L(t0, t) is simply the transpose of the tangent linear model."}
{"text": "Now assume that we separate the interval (t0, t) into two successive time intervals. For example, if t0 < t1 < t, L(t0, t) = L(t1, t)L(t0, t1) 6.3 Tangent linear and adjoint models, singular and Lyapunov vectors Since the adjoint of the tangent linear model is the transpose of the TLM, the property of the transpose of a product is also valid:"}
{"text": "LT (t0, t) = LT (t0, t1)LT (t1, t) Equation (6.3.11) shows that the tangent linear model can be cast as a product of the tangent linear model matrices corresponding to short integrations, or even single time steps. Equation (6.3.12) shows that the adjoint of the model can also be separated into single time steps, but they are executed backwards in time, starting from the last time step at t, and ending with the \ufb01rst time step at t0. For low-order models the tangent linear model and its adjoint can be constructed by repeated integrations of the nonlinear model for small perturbations, as done by Lorenz (1965), equation (3.7), and by Molteni and Palmer (1993) with a global quasi-geostrophic model."}
{"text": "For large NWP models this approach is too time consuming, and instead it is customary to develop the linear tangent and adjoint codes from the nonlinear model code following some rules discussed in Appendix B. An example of a FORTRAN code for a nonlinear model, and the corresponding tangent linear model and adjoint models are also given in Appendix B."}
{"text": "Recall that for a given basic trajectory and an interval (t0, t1) the tangent linear model is a matrix that when applied to a small initial perturbation y(t0) produces the \ufb01nal Singular value decomposition theory (e.g., Golub and Van Loan, 1996) indicates that for any matrix L there exist two orthogonal matrices U, V such that S is a diagonal matrix whose elements are the singular values of L."}
{"text": "If we left multiply (6.3.14) by U, we obtain 6 Atmospheric predictability and ensemble forecasting where vi are the columns of V and ui the columns of U. This implies that Equation (6.3.17) de\ufb01nes the vis as the right singular vectors of L, hereafter referred to as initial singular vectors, since they are indeed valid at the beginning of the optimization interval over which L is de\ufb01ned."}
{"text": "We now right multiply (6.3.14) by VT and obtain: Transposing (6.3.18), we obtain The uis are the left singular vectors of L and will be referred to as \ufb01nal (or evolved) singular vectors, since they correspond to the end of the interval of optimization."}
{"text": "From (6.3.17) and (6.3.20) we obtain Therefore the initial singular vectors can be obtained as the eigenvectors of LT L, a normal matrix whose eigenvalues are the squares of the singular values. Since U, V are orthogonal matrices, the vectors vi and ui that form them constitute orthonormal bases, and any vector can be written in the following form:"}
{"text": "where \u27e8x, y\u27e9is the inner product of two vectors x, y. Therefore, using (6.3.22a) and If we now take the inner product of (6.3.23) with ui we obtain \u27e8y(t1), ui\u27e9= \u03c3i\u27e8y(t0), vi\u27e9 This indicates that by applying the tangent linear model L each initial vector vi component will be stretched by an amount equal to the singular value \u03c3i(or contracted if \u03c3i < 1), and the direction will be rotated to that of the evolved vector ui. Similarly 6.3 Tangent linear and adjoint models, singular and Lyapunov vectors applying the adjoint of the tangent linear model, LT , each \ufb01nal vector ui will be stretchedbyanamountequaltothesingularvalue\u03c3i androtatedtotheinitialvectorvi."}
{"text": "Exercise 6.3.1: Use (6.3.20) and (6.3.22b) to show that \u27e8y(t0), vi\u27e9= \u03c3i\u27e8y(t1), ui\u27e9. If we consider all the perturbations y(t0) of size 1, from (6.3.24) we obtain that \u27e8y(t0), vi\u27e9= \u2225y(t0)\u22252 = 1 so that an initial sphere of radius 1 becomes a hyperellipsoid of semiaxes \u03c3i. The \ufb01rst initial singular vector v1 is also called an \u201coptimal vector\u201d since it gives the direction in phase space (i.e., the shape in physical space) of the perturbation that will attain maximum growth \u03c31 in the interval (t0, t1) (Fig. 6.3.1)."}
{"text": "Note that applying L is the same as running the tangent linear model forward in time, from t0 to t1. Applying LT is like running the adjoint model backwards, from t1 to t0. From (6.3.21) we see that if we apply the adjoint model to a sphere of \ufb01nal perturbations of size 1 (expanded on the basis formed by the evolved or left singular vectors), they also become stretched and rotated into a hyperellipsoid of semiaxes in the directions of the vi with length \u03c3i (Fig. 6.3.2)."}
{"text": "Therefore, if we apply LT L (i.e., run the tangent linear model forward in time, and then the adjoint backwards in time, the \ufb01rst initial singular vector will grow by a 1 (see Fig. 6.3.3), and the other initial singular vectors will grow or decay by their corresponding singular value squared \u03c3 2 i . In other words, the (initial) singular vectors vi are the eigenvectors of LT L with singular values \u03c3 2 apply the adjoint model \ufb01rst (integrate the adjoint model backwards from the \ufb01nal to the initial time), followed by the tangent linear model (integrate forward to the \ufb01nal time), the \ufb01nal singular vectors ui will grow both backward and forward, by a tangent linear model to a sphere of perturbations of size 1 for a given interval adjoint of the tangent linear perturbations of size 1 at the 6 Atmospheric predictability and ensemble forecasting forward in time followed by the adjoint of the tangent linear model to a sphere of perturbations of size 1 at the adjoint of the tangent linear linear model forward to a sphere of perturbations of total factor also equal to \u03c3 2 i (Figure 6.3.4). In other words, the \ufb01nal singular vectors are the eigenvectors of LLT , and again they have eigenvalues equal to the square of the singular values of L. Alternatively, once the initial singular vectors are obtained using, for example, the Lanczos algorithm, the \ufb01nal singular vectors can be derived by integrating the tangent linear model ((6.3.17))."}
{"text": "If we apply LT L repeatedly over the same interval (t0, t), we obtain the leading initial singular vector, or \ufb01rst optimal vector. Additional leading singular vectors can be obtained by a generalization of the power method (Lanczos algorithm, Golub and Van Loan, 1996), which requires running the tangent linear model and its adjoint about three times the number of singular vectors required. For example, to get the leading 30 singular vectors optimized for t1 \u2212t0 = 36 h, the ECMWF performed 100 iterations, equivalent to running the tangent linear model for about 300 days It is important to note that the adjoint model and the singular vectors are de\ufb01ned with respect to a given norm. So far we have used an Euclidean norm in which the weight matrix that de\ufb01nes the inner product is the identity matrix:"}
{"text": "The leading (initial) singular vectors are the vectors of equal size (initial norm equal to one \u2225y(t0)\u22252 = y(t0)T y(t0) = \u27e8y(t0), y(t0)\u27e9= 1), that grow fastest during the 6.3 Tangent linear and adjoint models, singular and Lyapunov vectors optimization period (t0, t1), i.e., the initial vectors that maximize the norm at the \ufb01nal J(y(t0)) \u2261\u2225y(t1)\u22252 = [Ly(t0)]T Ly(t0) = \u27e8LT Ly(t0), y(t0)\u27e9 If we de\ufb01ne a norm using any other weight matrix W applied to y, then the requirement that the initial perturbations be of equal size implies:"}
{"text": "\u2225y(t0)\u22252 = (Wy(t0))T Wy(t0) = y(t0)T WT Wy(t0) = 1 We can use a different norm to de\ufb01ne the size of the perturbation to be maximized at the \ufb01nal time than the norm W used for the initial time (6.3.28). For example the \ufb01nal norm could be a projection operator P at the end of the interval. Then the function that we want to maximize is, instead of (6.3.27):"}
{"text": "From the calculus of variations, the maximum of (6.3.29) subject to the strong constraint (6.3.28) can be obtained by the unconstrained maximum of another K(y(t0)) = J(y(t0)) + \u03bb[1 \u2212y(t0)T WT Wy(t0)] = y(t0)T LT PT PLy(t0) + \u03bb[1 \u2212y(t0)T WT Wy(t0)] where the \u03bb are the Lagrange multipliers multiplying the square brackets (equal to zero due to the constraint (6.3.28))."}
{"text": "(W\u22121)T LT PT PLW\u22121\u02c6y(t0) = \u03bb\u02c6y(t0) subject to the constraint Therefore, the transformed vectors \u02c6y(t0) are the eigenvectors of the matrix (W\u22121)T LT PT PLW\u22121 in (6.3.33), with eigenvalues equal to the Lagrange multi- pliers \u03bbi. After the leading eigenvectors \u02c6y(t0) are obtained (using, for example, the Lanczos algorithm), the variables are transformed back to y(t0) using (6.3.32). The 6 Atmospheric predictability and ensemble forecasting eigenvalues of this problem are the square of the singular values of the tangent linear This allows great generality (as well as arbitrariness4) in the choice of initial norm and \ufb01nal projection operator. Errico and Vukicevic (1992), showed that the singular vectors are very sensitive to both the choice of norm and the length of the optimization interval (the interval from t0 to t1). In another example, Palmer et al. (1998) tested different weight matrices W de\ufb01ning the initial norm. They used \u201cstreamfunction,\u201d \u201censtrophy,\u201d \u201ckinetic energy,\u201d and \u201ctotal energy\u201d norms, which measured, as the \u201cinitial size\u201d the square of the perturbation streamfunction, vorticity, wind speed and weighted temperature, wind and surface pressure, respectively. They found that the use of different initial norms resulted in extremely different initial singular vectors, and concluded that the total energy was the norm of choice for ensemble forecasting."}
{"text": "In 1995, ECMWF included in their ensemble system a projection operator P that measures only the growth of perturbations north of 30\u25e6N, i.e., a matrix that multiplies variables that correspond to latitudes greater than or equal to 30\u25e6N by the number 1, and by 0 otherwise) (Buizza and Palmer, 1995). One could use any other pair of initial W and \ufb01nal P weights (norms) to answer the related question of forecast sensitivity."}
{"text": "An example of a forecast sensitivity problem is: \u201cWhat is the optimal (minimum size) initial perturbation (measured by the square of the change in surface pressure over the states of Oklahoma and Texas) that produces the maximum \ufb01nal change after a 1-day forecast (measured by the change in vorticity between surface and 500 hPa over the eastern USA)?\u201d ECMWF has been routinely carrying out experiments to \ufb01nd out \u201cWhat is the change in the initial conditions from 3 days ago that would lead to the best veri\ufb01cation of today\u2019s analysis?\u201d (see Errico (1997), Rabier et al. (1996), Pu et al. (1997a,b) for more details)."}
{"text": "As we saw in Section 6.2, if we start a set of perturbations on a sphere of very small size, it will evolve into an ellipsoid. The growth of the axis of the hyperellipsoid after a \ufb01nite interval s is given by the singular values \u03c3i(t0 + s). The (global) Lyapunov exponents describe the linear long-term growth of the hyperellipsoid:"}
{"text": "4 Jon Ahlquist (2000, pers. communication) showed, given a linear operator L, a set of arbitrary vectors xi, and a set of arbitrary nonnegative numbers \u03c3i arranged in decreasing order, how to construct an inner product and a norm such that the \u03c3iand the xi are ith singular values and singular vectors of L. He pointed out that \u201cBecause anything not in the null space can be a singular vector, even the leading singular vector, one cannot assign a physical meaning to a singular vector simply because it is a singular vector. Any physical meaning must come from an additional aspect of the problem. Said in another way, nature evolves from initial conditions without knowing which inner products and norms the user wants to use.\u201d 6.3 Tangent linear and adjoint models, singular and Lyapunov vectors In other words, the Lyapunov exponents describe the long-term average exponential rate of stretching or contraction in the attractor. (We call the Lyapunov exponents \u201cglobal\u201dtodistinguishthemfromthe\ufb01nitetimeor\u201clocal\u201dLyapunovexponentswhich are useful in predictability applications.) There are as many Lyapunov exponents as the dimension of the model (number of independent variables or degrees of freedom)."}
{"text": "If the model has at least one \u03bbigreater than zero, then the system can be called chaotic, i.e., there is exponential separation of trajectories. In other words, there is at least one direction of the ellipsoid that continues to be stretched, and therefore two trajectories willdivergeintimeandeventuallybecomecompletelydifferent.Conversely,asystem with all negative Lyapunov exponents is stable, and will remain predictable at all times. The \ufb01rst Lyapunov exponent can be estimated by running the tangent linear model for a long time starting from any randomly chosen initial perturbation y(t0)."}
{"text": "which is independent of the norm. In practice, the \ufb01rst Lyapunov exponent is ob- tained by running the tangent linear model for a long period from random initial conditions, and renormalizing the perturbation vector periodically in order to avoid When we are dealing with atmospheric predictions, we are not really interested in the global growth properties, which correspond to the atmosphere\u2019s attractor (clima- tology), i.e., relevant average properties over many decades. Instead, in predictability problems we are interested in the growth rate of perturbations at a given time and space: we need to know the local stability properties in space and time, which are related to our ability to make skillful forecasts. We can de\ufb01ne the leading local Lyapunov vector (LLV) at a certain time t, as the vector towards which all random perturbations y(t \u2212s) started a long time s before t will converge (Fig. 6.3.5)."}
{"text": "Once a perturbation has converged to the leading LLV l1(t), the leading local Lya- punov exponent can be computed from the rate of change of its norm. In practice, the local leading Lyapunov exponent, also known as \ufb01nite time Lyapunov exponent, can be estimated over a \ufb01nite period \u03c4:"}
{"text": "The argument of the logarithm is de\ufb01ned as the ampli\ufb01cation rate A(t, \u03c4). The\ufb01rstLLVisindependentofthede\ufb01nitionofnorm,andrepresentsthedirection in which maximum sustainable growth (or minimum decay) can occur in a system without external forcing. In fact, after a \ufb01nite transition period T takes place, every 6 Atmospheric predictability and ensemble forecasting initial perturbation will turn in the direction of the LLV at every point of the trajectory."}
{"text": "This also includes the \ufb01nal singular vectors ui for a suf\ufb01ciently long optimization Trevisan and Legnani (1995) introduced the notion of the leading LLV. Additional LLVs can be obtained by Gramm\u2013Schmidt orthogonalization, and this would seem to indicate that they are norm-dependent. However, Trevisan and Pancotti (1998) showed that it is also possible, at least in theory, to de\ufb01ne additional LLVs (denoted characteristic vectors by Legras and Vautard, 1996) without the use of norms. The LLVs are therefore a fundamental characteristic of dynamical systems. It should be noted that regrettably, at this time, there is not a universally accepted nomenclature for LLVs. Legras and Vautard (1996) call the LLVs \u201cbackward Lyapunov vectors\u201d, since they were started an in\ufb01nitely long time in the past. Unfortunately, this name is extremely confusing, since they represent forward evolution rather than backward evolution as this name would imply. The LLVs are also the \ufb01nal singular vectors optimized for an in\ufb01nitely long time, i.e., the eigenvectors (valid at time t) of L(t \u2212 T, t)LT (t \u2212T, t) for T \u2192\u221e. Similarly, Legras and Vautard de\ufb01ne as \u201cforward Lyapunov vectors\u201d the initial singular vectors obtained from a very long backward integration with the adjoint of the model, i.e., they are the eigenvectors (valid at time t) of LT (t, t + T )L(t, t + T ) for very large T ."}
{"text": "Legras and Vautard (1996) showed (as did Trevisan and Pancotti (1998)) that a complete set of LLVs (which they denote characteristic Lyapunov vectors) can be de\ufb01ned from the intersection of the subspaces spanned by the \u201cforward\u201d and \u201cbackward\u201d Lyapunov vectors. The (characteristic) LLVs are therefore independent of the norm, and grow in time with a rate given by the local Lyapunov exponents. As such, they are a fundamental characteristic of dynamical systems."}
{"text": "Several authors have shown that the leading (\ufb01rst few) LLVs of low-dimensional dynamical systems span the attractor, i.e., they are parallel to the hypersurface in phase space that the dynamical system visits again and again (\u201crealistic solutions\u201d)."}
{"text": "6.3 Tangent linear and adjoint models, singular and Lyapunov vectors Leading singular vectors, on the other hand, have very different properties. They can grow much faster than the leading LLVs, but are initially off the attractor: they point to areas in the phase space where solutions do not naturally occur (e.g., Legras and Vautard, 1996, Trevisan and Legnani, 1995, Trevisan and Pancotti, 1998, Pires et al., 1996), see also next section."}
{"text": "For ensemble forecasting, Ehrendorfer and Tribbia (1997) showed that if V is the initial analysis error covariance (which unfortunately we don\u2019t know and can only estimate), then the initial singular vectors de\ufb01ned with the norm W = V\u22121/2 evolve into the eigenvectors of the evolved error covariance matrix. This implies that the leading singular vectors, de\ufb01ned using the initial error covariance, are optimal in describing the forecast errors at the end of the optimization period. The initial error covariance norm yields singular vectors quite different from those derived using the energy norm. Barkmeijer et al. (1998) used the ECMWF estimated 3D-Var error covariance as the initial norm (instead of the total energy norm) and obtained initial perturbations with structures closer to the bred vectors (i.e., leading LLVs) used at NCEP (see Section 6.5.1)."}
{"text": "Simple examples of singular vectors and eigenvectors In order to get a more intuitive feeling of the relationship between singular vectors and Lyapunov vectors, we consider a simple linear model in two dimensions:"}
{"text": "We compute the two-dimensional tangent linear model, constant in time: The propagation or evolution of any perturbation (difference between two solutions) over a time interval (t, t + T) is given by Note that the translation terms in (6.3.39) do not affect the perturbations. The eigen- vectors of L (which for this simple constant tangent linear model are also the Lyapunov vectors) are proportional to 6 Atmospheric predictability and ensemble forecasting corresponding to the eigenvalues \u03bb1 = 2, \u03bb2 = 0.5, respectively, which in this case are the two Lyapunov numbers (their logarithms are the Lyapunov exponents). If we normalize them, so that they have unit length, the Lyapunov vectors are The Lyapunov vectors are not orthogonal, they are separated by an angle of 153.4\u25e6 (Fig. 6.3.6(a)). We will see that because they are not orthogonal it is possible to \ufb01nd linear combinations of the Lyapunov vectors that grow faster than the leading Lyapunov vector. We will also see that the leading Lyapunov vector is the attractor of the system, since repeated applications of L to any perturbation makes it evolve (thin arrows l1 and l2), and the corresponding two initial singular vectors (thick arrows v1(0)and v2(0)), optimized for the interval (0, T ), for the tangent linear model with eigenvalues 2 and 0.5. (a) Time t = 0, showing the initial singular vectors v1(0) and v2(0), as well as the Lyapunov vectors l1 and l2. (b) Time t = T, evolved singular vectors, u1(T ) = Lv1(0), u2(T ) = Lv2(0) at the end of the optimization period; the Lyapunov vectors have grown by factors of 2 and 0.5 respectively, whereas the leading singular vector has grown by 3.63. The second evolved singular vector has grown by 0.275, and is still orthogonal to the \ufb01rst singular vector. (c) Time t = 2T ."}
{"text": "Beyond the optimization period T , the evolved singular vectors u1(t + 2T ) = Lu1(t + T ), u2(2T ) = Lu2(T ) are not orthogonal and they approach the leading Lyapunov vector with similar growth rates."}
{"text": "6.3 Tangent linear and adjoint models, singular and Lyapunov vectors Applying \ufb01rst L and then its transpose LT we obtain the symmetric matrix whose eigenvectors are the initial singular vectors, and whose eigenvalues are the squares of the singular values. The initial singular vectors (eigenvectors of LT L) are 2 = 0.076. As indicated before, the singular values of L are the square roots of the eigenvalues of LT L, i.e., \u03c31 = 3.63, \u03c32 = 0.275."}
{"text": "Note that this implies that during the optimization period (0, T ) the leading singular vector grows almost twice as fast as the leading Lyapunov vector (3.63 vs. 2). The angle that the leading initial singular vector has with respect to the leading Lyapunov vector is 56.82\u25e6, whereas the second initial singular vector is perpendicular to the The \ufb01nal or evolved SVs at the end of the optimization period (0, T ) are the eigen- and after normalization, they are Note again that the operators LT L and LLT are quite different, and the \ufb01nal singular vectors are different from the initial singular vectors, but they have the same Alternatively, the evolved singular vectors at the end of the optimization period can alsobeobtainedbyapplyingLtotheinitialsingularvectors,whichiscomputationally inexpensive. In this case, which is the same as (6.3.46) but without normalization."}
{"text": "The\ufb01nalleadingsingularvectorhasstronglyrotatedtowardstheleadingLyapunov vector: at the end of the optimization period the angle between the leading singular vector and the leading Lyapunov vector is only 6.6\u25e6(Fig. 6.3.6(b)), and because the singular vectors have been optimized for this period, the \ufb01nal singular vectors are 6 Atmospheric predictability and ensemble forecasting To obtain the evolution of the singular vectors beyond the optimization period (0, T ) we apply L again to the evolved singular vector valid at t = T and obtain During the interval (T , 2T ) the leading singular vector grows by a factor of just 2.33, which is not very different from the growth rate of the leading Lyapunov vector. At the end of this second period (Fig. 6.3.6(c)) the angle with the leading Lyapunov vector is only 1.41\u25e6. The angle of the second evolved singular vector at time T , after applying the linear tangent model L and the leading Lyapunov vector is also quite small (10.24o), and because it was further away from the attractor, the second singular vector (whose original, transient, singular value was 0.5), grows by a factor of 2.79."}
{"text": "This example shows how quickly all perturbations, including all singular vectors, evolve towards the leading Lyapunov vector, which is the attractor of the system. It is particularly noteworthy that during the optimization period (0, T ), the \ufb01rst singular vector grows very fast as it rotates towards the attractor, but once it gets close to the leading Lyapunov vector, its growth returns to the normal leading Lyapunov vector\u2019s Let us now choose as the tangent linear model another matrix with the same eigenvalues 2 and 0.5, i.e., with eigenvectors (Lyapunov vectors) that still grow at a rate of 2/T and 0.5/T respectively. However, now the angle between the \ufb01rst and the second Lyapunov vector is 177\u25e6, i.e. the Lyapunov vectors are almost antiparallel. In this case, the \ufb01rst singular vector grows by a factor of over 30 during the optimization period, but beyond the optimization period it essentially continues evolving like the leading Lyapunov vector."}
{"text": "These results do not depend on the fact that one Lyapunov vector grows and the other decays. As a third example, we choose with two Lyapunov vectors growing with rates 2/T and 1.5/T . The Lyapunov vectors arealmostparallel,withanangleof170\u25e6,andtheleadingsingularvectorgrowsduring the optimization period by a factor of 3.83. Applying the tangent linear model again to the evolved singular vectors we obtain that at time 2T the leading singular vector has grown by a factor of 2.9 and its angle with respect to the leading Lyapunov vector is 1\u25e6. Because it is not decaying, the second Lyapunov vector is also part of the attractor, but only those perturbations that are exactly parallel to it will remain parallel, all others will move towards the \ufb01rst Lyapunov vector."}
{"text": "6.4 Ensemble forecasting: early studies These examples illustrate the fact that the fast growth of the singular vectors during the optimization period depends on the lack of orthogonality between Lyapunov vectors. A very fast \u201csupergrowth\u201d of singular vectors is associated with the presence of almost parallel Lyapunov vectors, and it takes place when the initial singular vector, which is not in the attractor, rotates back towards the attractor. At the end of the optimization period, the leading singular vector tends to be much closer to the attractor, more parallel to the leading Lyapunov vector. The second (trailing) singular vector is also moving towards the leading Lyapunov vector."}
{"text": "Finally, we point out that this introductory discussion is appropriate for relatively low-dimensional systems. For extremely high-dimensional systems like the atmo- sphere, there may be multiple sets of Lyapunov exponents corresponding to different types of instabilities. For example, as pointed out by Toth and Kalnay (1993), con- vective instabilities have very fast growth but small amplitudes, whereas baroclinic instabilities have slower growth but much larger amplitudes, and each of these can lead to different types of Lyapunov vectors. If we are interested in the predictability characteristics associated with baroclinic instabilities, then the analysis of growth rates of in\ufb01nitesimally small Lyapunov vectors over in\ufb01nitely long times may not be appropriate for the problem (Lorenz, 1996). In that case, it may be better to consider the \ufb01nite amplitude, \ufb01nite time extension of Lyapunov vectors introduced by Toth and Kalnay (1993, 1997) as bred vectors. Bred vectors are discussed in Section 6.5.1, and their relationship to Lyapunov vectors in Kalnay et al. (2002)."}
{"text": "Ensemble forecasting: early studies We saw in previous sections that Lorenz (1963a,b,1965) showed that the forecast skill of atmospheric models depends not only on the accuracy of the initial conditions and on the realism of the model (as it was generally believed at the time), but also on the instabilities of the \ufb02ow itself. He demonstrated that any nonlinear dynamical system with instabilities, like the atmosphere, has a \ufb01nite limit of predictability. The growth of errors due to instabilities implies that the smallest imperfection in the forecast model or the tiniest error in the initial conditions, will inevitably lead to a total loss of skill in the weather forecasts after a \ufb01nite forecast length. Lorenz estimated this limit of weather predictability as about two weeks. With his simple model he also pointed out that predictability is strongly dependent on the evolution of the atmosphere itself:"}
{"text": "some days the forecasts can remain accurate for a week or longer, and on other days the forecast skill may break down after only 3 days. This discovery made inevitable the realization that NWP needs to account for the stochastic nature of the evolution of the atmosphere (Fig. 6.4.1). As previously discussed, Lorenz (1965) studied the error growth of a complete \u201censemble\u201d of perturbed forecasts, with the ensemble size equal to the dimension of the phase space (one perturbation for each of the 28 model variables). In this paper he introduced for the \ufb01rst time concepts related 6 Atmospheric predictability and ensemble forecasting for forecasts starting from a representative set of perturbed initial conditions within a circle representing the uncertainty of the initial conditions (ideally the analysis error covariance) and ending within the range of possible solutions. For the shorter range, the forecasts are close to each other, and they may be considered deterministic, but beyond a certain time, the equally probable forecasts are so different that they must be considered stochastic. The transition time is of the order of 2\u20133 days for the prediction of large-scale \ufb02ow, but can be as short as a few hours for mesoscale phenomena like the prediction of individual storms. The transition time is shorter for strongly nonlinear parameters: even for large-scale \ufb02ow, precipitation forecasts show signi\ufb01cant divergence faster than the 500-hPa \ufb01elds. The forecasts may be clustered into subsets A and B. (Adapted from Tracton and Kalnay, 1993.) to singular vectors and LLVs discussed in the previous section. This was followed by several early approaches to the problem of accounting for the variable predictability of the atmosphere reviewed in this section."}
{"text": "Stochastic-dynamic forecasting Historically, the \ufb01rst forecasting method to explicitly acknowledge the uncertainty of atmospheric model predictions was developed by Epstein (1969), who introduced the idea of stochastic-dynamic forecasting. He derived a continuity equation for the probability density \u03d5(X; t) of a model solution X of a dynamical model \u02d9X = G(X(t)), where the model has dimension D:"}
{"text": "This equation indicates that in an ensemble of forecast solutions, \u201cno member of the ensemble may be created or destroyed\u201d. An ensemble starting from an in\ufb01- nite number of perturbed integrations spanning the analysis uncertainty gives the \u201ctrue\u201d probability distribution (with all its moments), but even for a simple low-order model, the integration of (6.4.1) is far too expensive. Therefore Epstein introduced an approximation to predict only the \ufb01rst and second moments of the probability distri- bution (expected means and covariances) rather than the full probability distribution."}
{"text": "6.4 Ensemble forecasting: early studies Epstein assumed that the model equations are of the form The forecast equations for the expected \ufb01rst and second moments are \u02d9\u03c1ij = E(xi \u02d9x j + \u02d9xix j) The covariances \u03c1ij are related to the second order moments by \u03c3i j = E[(xi \u2212 \u00b5i)(x j \u2212\u00b5 j)]. Substituting (6.4.2) into (6.4.3) gives rise to forecast equations for \u02d9\u00b5 and \u02d9\u03c3 that contain triple moments (xix jxk). As done in turbulence mod- els with a second order closure for the triple products (Chapter 4), Epstein in- troduced a closure assumption for the third order moments around the mean \u03c4ijk = E[(xi \u2212\u00b5i)(x j \u2212\u00b5 j)(xk \u2212\u00b5k)]. He assumed that #kla jkl\u03c4ikl + aikl\u03c4 jkl = 0, which then gives a closed set of equations for the means and covariances:"}
{"text": "a jkl(\u00b5k\u03c3il + \u00b5l\u03c3ik) + aikl(\u00b5k\u03c3 jl + \u00b5l\u03c3 jk) Epstein tested these \u201capproximate\u201d stochastic equations for a Lorenz three-variable model. The \u201ctrue\u201d probability distribution was computed from a Monte Carlo ensem- ble of 500 members, and the comparison indicated good agreement, at least for several simulated days. Note that in his case, the number of ensemble members was much larger than the number of degrees of freedom of the model, a situation that would be impossible to replicate with current models with millions of degrees of freedom. In his paper, Epstein also introduced the idea of using stochastic-dynamic forecasting in the analysis cycle, with the background forecast and error covariance provided by stochastic-dynamic forecasts combined with observations that also contain errors Unfortunately, although the stochastic-dynamic forecasting method was intro- duced as a shortcut to an \u201cin\ufb01nite\u201d Monte Carlo ensemble, in a model with N degrees of freedom, it requires N(N + 1)/2 + N forecast equations, equivalent to making about (N+ 3)/2 model forecasts. Although this was practical with a three-variable model, it is completely unfeasible for a modern model, with millions of degrees of In 1974, Leith proposed the idea of performing ensemble forecasting with a limited number m of ensemble members instead of the conventional single (deterministic) 6 Atmospheric predictability and ensemble forecasting control forecast. He also proposed performing an \u201coptimal estimation\u201d of the ver- i\ufb01cation using linear regression on the dynamical forecasts, with optimal weights determined from forecast error covariances (cf. Sections 5.3\u20135.5). Since forecasts lose their skill at longer lead times, and individual forecasts eventually are further away from the veri\ufb01cation than the climatology (cf. eqs. (6.4.5) and (6.4.6)), optimal estimation of the veri\ufb01cation is equivalent to tempering (i.e., hedging the forecast He cast his analysis using, instead of model variables, their deviation u with respect to climatology (also known as forecast anomalies). The true state of the atmosphere is denoted u0, and \u02c6u then denotes an unbiased estimate of u0, whose expected value (average over many forecasts, represented by the angle brackets) is equal to zero:"}
{"text": "\u27e8(0 \u2212u0)(0 \u2212u0)T \u27e9= \u27e8u0uT A single (deterministic) forecast \u02c6u, on the other hand, has, on average, an error \u27e8(\u02c6u \u2212u0)(\u02c6u \u2212u0)T \u27e9= \u27e8\u02c6u\u02c6uT + u0uT This limit occurs because the last two terms in the second angle brackets go to zero as the forecasts become decorrelated with the true atmosphere at long lead times, and we assume that the model covariance is also unbiased. This indicates that for long lead times an individual deterministic forecast has twice the error covariance of a climatological forecast. Therefore, a \u201cregressed\u201d forecast, tempered towards climatology, must be better than a single deterministic forecast (in a least square error sense), with an error covariance that asymptotes to U, and not 2U."}
{"text": "A regressed forecast \u02c6u0 = \u02c6uA is obtained by linear regression, minimizing the square of the regressed error \u03b5T \u03b5 = \u27e8(u0 \u2212\u02c6uA)T (u0 \u2212\u02c6uA)\u27e9with respect to the ele- ments of the matrix of constant regression coef\ufb01cients A. As we did in the derivation of the optimal weight matrix for the observational increments in Section 5.4, we make use of the linear regression formulas: if the linear prediction equation is \u02c6y = xA, then the error is given by \u03b5 = y \u2212xA. The matrix of the derivatives of the (scalar) squared error \u03b5T \u03b5 with respect to each element of A is given by \u2202\u03b5T \u03b5/\u2202A = \u22122xT (y \u2212xA) = 0, which gives the normal equation xT y = xT xA, or A = (xT x)\u22121(xT y). Applying this to the regressed forecast we obtain \u27e8\u02c6uT (u0 \u2212\u02c6uA)\u27e9= 0, or Estimating the required forecast statistics in (6.4.7) involves considerable work."}
{"text": "The size of the regression matrix is usually large compared to the size of the sample available to estimate it, and in order to reduce the number of parameters to be 6.4 Ensemble forecasting: early studies estimated additional approximations are needed (e.g., by parameterizing error growth, Hoffman and Kalnay, 1983)."}
{"text": "Now, instead of regression let\u2019s consider an ensemble of m forecasts computed from perturbations ri to the initial best estimate (analysis) \u02c6u. Ideally, the perturbations should be chosen so that their outer product is a good estimate of the initial error co- variance (i.e., the analysis error covariance \u27e8rrT \u27e9= Pa, as suggested in Fig. 6.4.1). In practice, however, the analysis error covariance can only be approximately estimated (e.g., Barkmeijer et al., 1998)."}
{"text": "i=1ui is the average of an ensemble of m forecasts, then its error \u27e8(u \u2212u0)(u \u2212u0)T \u27e9= \u27e8uuT + u0uT since the last two terms in the second angle brackets go to zero at long time leads, and the \ufb01rst one evolves like Equation (6.4.8) shows that averaging a Monte Carlo ensemble of forecasts ap- proximates the tempering of the forecasts towards climatology, without the need to perform regression. It suggests that such tempering may be substantially achieved with a relatively small number of ensemble members (compare (6.4.8) with (6.4.5) and (6.4.6)). Leith (1974) used an analytical turbulence model to test this hypothe- sis, and concluded that a Monte Carlo forecasting procedure represents a practical, computable approximation to the stochastic-dynamic forecasts proposed by Epstein (1969). He suggested that adequate accuracy would be obtained for the best estimate of the forecast (i.e., the ensemble mean) with sample sizes as small as 8, but that the estimation of forecast errors may require a larger number of ensemble mem- bers. Monte Carlo forecasting is thus a feasible approach for ensemble forecasting, requiring only a de\ufb01nition of the initial perturbations and m forecasts."}
{"text": "Lagged average forecasting In 1983, Hoffman and Kalnay proposed lagged average forecasting (LAF) as an al- ternative to Monte Carlo forecasting, in which the forecasts initialized at the current initial time, t = 0, as well as at previous times, t = \u2212\u03c4, \u22122\u03c4, . . . , \u2212(N \u22121)\u03c4 are combined to form an ensemble (see Fig. 6.4.2). In an operational set up, \u03c4 is typically 6, 12 or 24 hours, so that the forecasts are already available, and the perturbations are generated automatically from the forecast errors. Since the ensemble comprises forecasts of different \u201cage\u201d, Hoffman and Kalnay (1983) weighted them according to their expected error, which they estimated by parameterizing the observed error 6 Atmospheric predictability and ensemble forecasting average forecasts (b). The abscissa is forecast time t, and the ordinate is the value of a forecast variable X. The crosses represent analyses obtained at time intervals \u03c4, and the dots, randomly perturbed initial conditions; tf is a particular forecast time. The initial \u201cperturbation\u201d for the lagged average forecast is the previous forecasts\u2019 error at the initial time. (Adapted from Hoffman and Kalnay, 1983.) covariance growth. They compared the lagged average forecasting and Monte Carlo forecasting methods within a simulation system, using a primitive equations model as \u201cnature\u201d, and a quasi-geostrophic model to perform the \u201cforecasts\u201d. In this way they allowed for model errors, unlike the previous \u201cidentical twin\u201d experiments that assumed a perfect model. They \u201cobserved\u201d the required variables and intro- duced random \u201cobservation errors\u201d every 6 hours and performed many ensemble forecast experiments separated by 50 days of integration. They compared the re- sults of single forecasts (ordinary dynamical forecasts), Monte Carlo forecasting, lagged average forecasting and tempered ordinary dynamical forecasts, as well as persistence-climatology forecast (the most skillful baseline forecast)."}
{"text": "Hoffman and Kalnay looked at the error growth of individual forecasts (Fig. 6.4.3). Note in this \ufb01gure that the individual forecast errors grow slowly and then at a certain timethereisarapiderrorgrowthuntilnonlinearsaturationtakesplace(onlytheperiod 6.4 Ensemble forecasting: early studies climatological forecast error, plotted only during the period the forecast error crossed D = 0.5. Also plotted are two measures of average forecast error. (Adapted from Hoffman and Kalnay, 1983.) of rapid growth is plotted). Note also that the forecast errors saturate around the climatological variability, as indicated by (6.4.6)."}
{"text": "In these simulated forecasts, like in real weather forecasts, the forecast skill ex- hibits a lot of day-to-day variability. The rapid growth takes place at a time that varies from a minimum of 5 days to a maximum of 20 days. Hoffman and Kalnay tested the ability of the ensemble to predict the time at which the forecast error crossed 50% of the climatological standard deviation. They used as the predictor the spread of the ensembles (standard deviation with respect to their mean). They found that the lagged average forecasting ensemble average forecast was only slightly better than the Monte Carlo forecasting, but the advantage of lagged average forecasting in predicting forecast skill was much more apparent, with the correlation between predicted and observed time of crossing the 50% level being 0.68 for Monte Carlo forecasting and 0.79 for lagged average forecasting."}
{"text": "The advantages of lagged average forecasting over Monte Carlo forecasting are probably due to the fact that lagged average forecasting perturbations in the initial conditions were not randomly chosen errors like in Monte Carlo forecasting but included dynamical in\ufb02uences and therefore contained \u201cerrors of the day\u201d. This is 6 Atmospheric predictability and ensemble forecasting because the perturbations are generated from actual forecast errors and therefore they are in\ufb02uenced by the evolution of the underlying background large-scale \ufb02ow."}
{"text": "Lagged average forecasting has been frequently used for experimental ensemble forecasting, both for medium-range and climate prediction. However, the statistics required to estimate the weights of the members of the lagged average forecasting ensemble according to their \u201cage\u201d are very hard to obtain, so that except for the study by Dalcher et al. (1988), all the lagged average forecasting members have been generally given equal weight. The advantages of lagged average forecasting are: (a) some of the forecasts are already available in operational centers; (b) it is very simple to perform and does not require special generation of perturbations; and (c) the perturbations contain \u201cerrors of the day\u201d (Lyapunov vectors). Lagged average forecasting has also major disadvantages: (a) a large LAF ensemble would have to include excessively \u201cold\u201d forecasts; (b) without the use of optimal weights, the lagged average forecasting ensemble average may be tainted by the older forecasts."}
{"text": "Ebisuzaki and Kalnay (1991) introduced a variant of lagged average forecasting denoted scaled lagged average forecasting (SLAF) that reduces these two disad- vantages. The perturbations are obtained by computing the forecast error of fore- casts started at t = \u2212j\u03c4, j = 1, . . . , N \u22121, and multiplying these errors by \u00b11/j."}
{"text": "This assumes that the errors grow approximately linearly with time during the \ufb01rst 2\u20133 days, and that the perturbations can be subtracted from and not just added to the analysis. The advantages of scaled lagged average forecasting are: (a) the ini- tial perturbations of the ensemble members are all of approximately the same size (this can be enforced using a more sophisticated rescaling than linear growth), and (b) their number is doubled with respect to lagged average forecasting, so that only shorter-range forecasts are needed to create scaled lagged average forecasting. In practice, it has been observed that pairs of initial perturbations with opposite sign, as used in scaled lagged average forecasting, yield better ensemble forecasts, pre- sumably because the Lyapunov vectors within the analysis errors can have either sign, whereas lagged average forecasting tends to maintain a single sign in the error."}
{"text": "Experiments with the NCEP global model showed that scaled lagged average fore- casting ensembles were better than lagged average forecasting ensembles (Ebisuzaki and Kalnay, 1991). This method is also easier to implement in regional ensemble forecasts, since it generates boundary condition perturbations consistent with the interior perturbations (Hou et al., 2001)."}
{"text": "Operational ensemble forecasting methods control forecast (labeled C) starts from the analysis (denoted by a cross), i.e., from the best estimate of the initial state of the atmosphere; (2) two perturbed ensemble forecasts (labeled P+ and P\u2212) with initial perturbations added and subtracted from 6.5 Operational ensemble forecasting methods forecast (labeled C) which starts from the analysis (denoted by a cross), which is the best estimate of the true initial state of the atmosphere; (2) two perturbed ensemble forecasts (labeled P+ and P\u2212) with initial perturbations added and subtracted from the control; (3) the ensemble average denoted A; and (4) the \u201ctrue\u201d evolution of the atmosphere labeled T. This is a \u201cgood\u201d ensemble since the \u201ctruth\u201d appears as a plausible member of the ensemble. Note that because of nonlinear saturation, the error of the ensemble member initially further away from the truth (in this case P+) tends to grow more slowly than the error of the member initially closer to the truth."}
{"text": "This results in a nonlinear \ufb01ltering of the errors: the average of the ensemble members tends to be closer to the truth than the control forecast (Toth and Kalnay, 1997, also compare with Fig. 1.7.1). (b) Schematic of a \u201cbad\u201d ensemble in which the forecast errors are dominated by system errors (such as model de\ufb01ciencies). In this case, the ensemble is not useful for forecasting, but it helps to identify the fact that forecast errors are probably due to the presence of systematic errors, rather than to the chaotic growth of errors in the initial conditions."}
{"text": "6 Atmospheric predictability and ensemble forecasting the control; (3) the ensemble average labeled A, and (4) the true evolution of the atmosphere (not known in real time), labeled T. This is an example of a \u201cgood\u201d ensemble since the true evolution appears to be a plausible member of the ensemble."}
{"text": "are dominated by problems in the forecasting system (such as model de\ufb01ciencies) rather than the chaotic growth of initial errors. In this case, the true evolution is quite different from the members of the ensemble, but the ensemble is still useful in identifying the presence of a de\ufb01ciency in the forecasting system, which, with a single forecast, could not be distinguished from the growth of errors in the initial conditions."}
{"text": "Ensemble forecasting has essentially three basic goals. The \ufb01rst is to improve the forecast by ensemble averaging. The improvement is a result of the tendency of the ensemble average to \ufb01lter out the components of the forecast that are uncertain (where the members of the ensemble differ from each other) and to retain those components that show agreement among the members of the ensemble. The \ufb01ltering can take place only during the nonlinear evolution of the perturbations: if the perturbations are added to and subtracted from the analysis, the ensemble average forecast is equal to the control while the perturbations remain linear. The improvement of the ensemble av- erage with respect to the control, shown schematically in Fig 6.5.1(a), is noticeable in Fig. 1.7.1 after a few days of forecasts with the NCEP global ensemble. The second goal is to provide an indication of the reliability of the forecast: if the ensemble forecasts are quite different from each other, it is clear that at least some of them are wrong, whereas if there is good agreement among the forecasts, there is more reason to be con\ufb01dent about the forecast (cf. e.g., Fig. 1.7.2(a) and (b)). The quantitative relationship between the ensemble spread and the forecast error (or conversely, be- tween the forecast agreement and the forecast skill) has yet to be \ufb01rmly established, but is now routinely taken into consideration by human forecasters. The third goal of ensemble forecasting is to provide a quantitative basis for probabilistic forecasting."}
{"text": "An ensemble forecasting system requires the de\ufb01nition of the initial amplitude and the horizontal and vertical structure of the perturbations. Typically, the initial amplitude is chosen to be close to the estimated analysis error. The amplitude of the analysis uncertainty depends on the distribution of the observations. Its statistical distribution can be estimated from the analysis error covariance (Chapter 5), which depends on the accuracy of the statistical assumptions, or empirically, from the rms differences between independent analysis cycles (Fig. 6.5.2)."}
{"text": "Ensemble forecasting methods differ mostly in the way the initial perturbations are generated, and can be classi\ufb01ed into essentially two classes: those that have random initial perturbations, and those where the perturbations depend on the dynamics of the underlying \ufb02ow. In the \ufb01rst class, which we can denote Monte Carlo forecasting, the initial perturbations are chosen to be \u201crealistic\u201d, i.e., they have horizontal and vertical structures statistically similar to forecast errors, and amplitudes compatible 6.5 Operational ensemble forecasting methods obtained from running two independent analysis cycles, computing their rms difference, and using a \ufb01lter to retain the planetary scales. The units are arbitrary."}
{"text": "Note the minima over and downstream of rawinsonde-rich land regions and the maxima over the oceans (Courtesy I. Szunyogh, University of Maryland.) with the estimated analysis uncertainty. In the Monte Carlo ensembles, the amplitudes are realistic but the perturbations themselves are chosen randomly, without regard to the \u201cdynamics of the day\u201d. For example, Errico and Baumhefner (1987) and Mullen and Baumhefner (1994) developed a Monte Carlo method that results in realistic perturbations compatible with the average estimated analysis error. However, by construction, this type of Monte Carlo forecast does not include \ufb01nite-size \u201cgrowing errors of the day\u201d which are almost certainly present in the analysis. The experiments of Hollingsworth (1980), Hoffman and Kalnay (1983), and Kalnay and Toth (1996) suggest that random initial perturbations do not grow as fast as the real analysis errors, even if they are in quasi-geostrophic balance. A second class of methods which includes errors of the day has been developed, tested, and implemented at several operational centers. The \ufb01rst two methods of this class implemented operationally are known as \u201cbreeding\u201d and \u201csingular vector\u201d (or optimal perturbations) methods. In contrast to Monte Carlo forecasting, they are characterized by including in the initial perturbations growing errors that depend on the evolving underlying atmospheric \ufb02ow. Two other methods in this class that are also very promising are based on ensembles of data assimilations, and ensembles based on operational systems from different centers, combining different models and data assimilations."}
{"text": "Ensemble experiments performed at NCEP during 1991 showed that initial ensemble perturbations based on lagged average forecasting, scaled lagged average forecasting 6 Atmospheric predictability and ensemble forecasting and on the forecast differences between forecasts verifying at the same initial time, grew much faster than Monte Carlo perturbations with the same overall size and statistical distribution (Kalnay and Toth, 1996). It was apparent that the differences in the growth rate were due to the fact that the \ufb01rst group included perturbations that, by construction, \u201cknew\u201d about the evolving underlying dynamics. Toth and Kalnay (1993, 1996, 1997) created a special operational cycle designed to \u201cbreed\u201d fast growing \u201cerrors of the day\u201d (Fig. 6.5.3(a)). Given an evolving atmospheric \ufb02ow (either a series of atmospheric analyses, or a long model run), a breeding cycle is started by introducing a random initial perturbation (\u201crandom seed\u201d) with a given initial size (measured with any norm, such as the rms of the geopotential height or the kinetic energy). It should be noted that the random seed is introduced only once. The same nonlinear model is integrated from the control and from the perturbed initial conditions.Fromthenon,at\ufb01xedtimeintervals(e.g.,every6hoursorevery24hours), the control forecast is subtracted from the perturbed forecast. The difference is scaled down so that it has the same amplitude (de\ufb01ned using the same arbitrary norm) as the initial perturbation, and then added to the corresponding new analysis or model state. It was found that beyond an initial transient period of 3\u20134 days after random perturbations were introduced, the perturbations generated in the breeding cycle (denoted bred vectors), acquired a large growth rate, faster than the growth rate for Monte Carlo forecasting or even scaled lagged average forecasting and forecast Toth and Kalnay (1993, 1997) also found that (after the transient period of 3\u20134 days) the shape or structure of the perturbation bred vectors did not depend on either the norm used for the rescaling or the length of the scaling period. The bred vectors did depend on the initial random seed in the sense that regional bred vector perturbations would have the same shape but different signs, and that in many areas two or more \u201ccompeting bred vectors\u201d appeared in cycles originated from different random seeds. The breeding method is a nonlinear generalization of the method used to construct Lyapunov vectors (performing two nonlinear integrations and obtaining the approximately linear perturbation from their difference). Since the bred vectors are related to Lyapunov vectors localized in both space and time, it is not surpris- ing that they share their lack of dependence on the norm or on the scaling period."}
{"text": "Toth and Kalnay (1993) have argued that breeding is similar to the analysis cycle. In the analysis cycle (represented schematically in Fig. 6.5.3(b)) errors are evolved in time through the forecast used as background, and they are only partially corrected through the use of noisy data. Therefore, Toth and Kalnay argued that the analysis errors should project strongly on bred vectors. Corazza et al. (2002) compared bred vectors and background errors for a quasi-geostrophic model data assimilation sys- tem developed by Morss et al. (2001) Plate 1 shows a typical comparison, depicting that in fact there is a strong resemblance between the structure of the errors of the forecast used as a \ufb01rst guess and the bred vectors valid at the same time (Corazza et al., 2002). Since the analysis errors are dominated by the background errors, especially 6.5 Operational ensemble forecasting methods Unperturbed control forecast model integration. The initial growth after introducing a random initial perturbation is usually very small, but with time, the perturbation is more dominated by growing errors. The initial transient with slow growth lasts about 3\u20135 days. The difference of the complete perturbed (dashed line) and control (full line) forecasts is scaled back periodically (e.g., every 6 or every 24 hours) to the initial amplitude. The rescaling is done by dividing all the forecast differences by the same observed growth (typically about 1.5/day for mid-latitudes). In operational NWP, the unperturbed model integration is substituted by short-range control forecasts started from consecutive analysis \ufb01elds. The breeding cycle is a nonlinear, \ufb01nite-time, \ufb01nite-amplitude generalization of the method used to obtain the leading Lyapunov vector. (Adapted from Kalnay and Toth, 1996.) (b) Schematic of the 6-h analysis cycle. Indicated on the vertical axis are differences between the true state of the atmosphere (or its observational measurements, burdened with observational errors). The difference between the forecast and the true atmosphere (or the observations) increases with time in the 6-h forecast because of the presence of growing errors in the analysis."}
{"text": "(Adapted from Kalnay and Toth, 1996.) 6 Atmospheric predictability and ensemble forecasting Every day, the 1-day forecast from the negative perturbation is subtracted from the 1-day forecast from the positive perturbation. This difference is divided by 2, and then scaled down (by dividing all variables by the 1-day growth), so that difference is of the same size as the initial perturbation. The scaled difference is then added and subtracted from the new analysis, generating the initial conditions for the new pair of forecasts. This self-breeding is part of the extended ensemble forecast system, and does not require computer resources to generate initial perturbations beyond running the ensemble forecasts. (Adapted from Toth and Kalnay, 1997.) when they are large, this resemblance indicates that the forecast and analysis errors do indeed project strongly on the bred vectors."}
{"text": "the ensemble forecasts. The bred perturbations are de\ufb01ned every day from the dif- ference between the one-day positive and the negative perturbation forecasts divided by 2, they are scaled down by their growth during that day, and added and subtracted to the new analysis valid at the time. This provides the initial positive and negative perturbations for the ensemble forecasts at no additional cost beyond that of com- puting the ensemble forecasts. Separate breeding cycles differ only in the choice of random initial perturbations (performed only once). It has been found empirically that for the atmosphere the \ufb01nite amplitude bred vectors do not converge to a single \u201cleading bred vector\u201d (Kalnay et al., 2002)."}
{"text": "responding to 5 March 2000 at 00UTC. Figure 6.5.5(c) presents an estimate of the effective local dimension of the subspace of the \ufb01ve perturbations using the bred vec- tor dimension de\ufb01ned in Patil et al. (2001).5 Only the areas where the local dimension has collapsed from the original \ufb01ve independent directions (shapes or structure of the 5 The local bred vector dimension is obtained as \u03c8(\u03c31, . . . , \u03c3k) = are the singular values corresponding to the k bred vectors within a region of about 106 m by 106 m, and it de\ufb01nes the effective local dimensionality. For example, if four out of \ufb01ve bred vectors lie along one direction, and one lies along a second direction, the bred vector dimension 4, 1, 0, 0, 0) = 1.8, less than 2 because one direction is more dominant than the other in representing the original data (Patil et al., 2001)."}
{"text": "6.5 Operational ensemble forecasting methods differences, without plotting the zero contour) from the NCEP operational ensemble system valid at 5 March 2000: (a) bred vector 1; (b) bred vector 5. Note that over large parts of the eastern Paci\ufb01c Ocean and western North America, the two perturbations have shapes that are very similar but of opposite signs and/or different amplitudes. In other areas the shape of the perturbations is quite different. (c) The bred-vector-local dimension of the \ufb01ve perturbations subspace (Patil et al., 2001). Only dimensions less than or equal to 3 are contoured with a contour interval 0.25. In these areas the \ufb01ve independent bred vectors have aligned themselves into a locally low-dimensional subspace with an effective dimension less than or equal to 3. (Courtesy of D. J. Patil.) bred vectors) to three or less are contoured. Note that these are the areas where the independently bred vectors aligned themselves into a smaller subspace. The collapse of the perturbations into fewer dimensions is what one could expect if there are locally growing dominant Lyapunov vectors expressing the regional dominant instability of the underlying atmospheric \ufb02ow. These low-dimensional areas are organized into horizontal and vertical structures and have a lifetime of 4\u20137 days, similar to those of baroclinic developments (Patil et al., 2002)."}
